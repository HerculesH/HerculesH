/*! For license information please see 878.ce1c8986.chunk.js.LICENSE.txt */
"use strict";(self.webpackChunkweb_portfolio=self.webpackChunkweb_portfolio||[]).push([[878,326],{9117:(e,t)=>{var s=Symbol.for("react.element"),n=Symbol.for("react.portal"),o=Symbol.for("react.fragment"),i=Symbol.for("react.strict_mode"),r=Symbol.for("react.profiler"),a=Symbol.for("react.provider"),l=Symbol.for("react.context"),c=Symbol.for("react.forward_ref"),d=Symbol.for("react.suspense"),_=Symbol.for("react.memo"),u=Symbol.for("react.lazy"),h=Symbol.iterator;var p={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},m=Object.assign,f={};function g(e,t,s){this.props=e,this.context=t,this.refs=f,this.updater=s||p}function k(){}function w(e,t,s){this.props=e,this.context=t,this.refs=f,this.updater=s||p}g.prototype.isReactComponent={},g.prototype.setState=function(e,t){if("object"!==typeof e&&"function"!==typeof e&&null!=e)throw Error("setState(...): takes an object of state variables to update or a function which returns an object of state variables.");this.updater.enqueueSetState(this,e,t,"setState")},g.prototype.forceUpdate=function(e){this.updater.enqueueForceUpdate(this,e,"forceUpdate")},k.prototype=g.prototype;var x=w.prototype=new k;x.constructor=w,m(x,g.prototype),x.isPureReactComponent=!0;var y=Array.isArray,v=Object.prototype.hasOwnProperty,b={current:null},M={key:!0,ref:!0,__self:!0,__source:!0};function S(e,t,n){var o,i={},r=null,a=null;if(null!=t)for(o in void 0!==t.ref&&(a=t.ref),void 0!==t.key&&(r=""+t.key),t)v.call(t,o)&&!M.hasOwnProperty(o)&&(i[o]=t[o]);var l=arguments.length-2;if(1===l)i.children=n;else if(1<l){for(var c=Array(l),d=0;d<l;d++)c[d]=arguments[d+2];i.children=c}if(e&&e.defaultProps)for(o in l=e.defaultProps)void 0===i[o]&&(i[o]=l[o]);return{$$typeof:s,type:e,key:r,ref:a,props:i,_owner:b.current}}function A(e){return"object"===typeof e&&null!==e&&e.$$typeof===s}var T=/\/+/g;function C(e,t){return"object"===typeof e&&null!==e&&null!=e.key?function(e){var t={"=":"=0",":":"=2"};return"$"+e.replace(/[=:]/g,(function(e){return t[e]}))}(""+e.key):t.toString(36)}function z(e,t,o,i,r){var a=typeof e;"undefined"!==a&&"boolean"!==a||(e=null);var l=!1;if(null===e)l=!0;else switch(a){case"string":case"number":l=!0;break;case"object":switch(e.$$typeof){case s:case n:l=!0}}if(l)return r=r(l=e),e=""===i?"."+C(l,0):i,y(r)?(o="",null!=e&&(o=e.replace(T,"$&/")+"/"),z(r,t,o,"",(function(e){return e}))):null!=r&&(A(r)&&(r=function(e,t){return{$$typeof:s,type:e.type,key:t,ref:e.ref,props:e.props,_owner:e._owner}}(r,o+(!r.key||l&&l.key===r.key?"":(""+r.key).replace(T,"$&/")+"/")+e)),t.push(r)),1;if(l=0,i=""===i?".":i+":",y(e))for(var c=0;c<e.length;c++){var d=i+C(a=e[c],c);l+=z(a,t,o,d,r)}else if(d=function(e){return null===e||"object"!==typeof e?null:"function"===typeof(e=h&&e[h]||e["@@iterator"])?e:null}(e),"function"===typeof d)for(e=d.call(e),c=0;!(a=e.next()).done;)l+=z(a=a.value,t,o,d=i+C(a,c++),r);else if("object"===a)throw t=String(e),Error("Objects are not valid as a React child (found: "+("[object Object]"===t?"object with keys {"+Object.keys(e).join(", ")+"}":t)+"). If you meant to render a collection of children, use an array instead.");return l}function P(e,t,s){if(null==e)return e;var n=[],o=0;return z(e,n,"","",(function(e){return t.call(s,e,o++)})),n}function F(e){if(-1===e._status){var t=e._result;(t=t()).then((function(t){0!==e._status&&-1!==e._status||(e._status=1,e._result=t)}),(function(t){0!==e._status&&-1!==e._status||(e._status=2,e._result=t)})),-1===e._status&&(e._status=0,e._result=t)}if(1===e._status)return e._result.default;throw e._result}var E={current:null},L={transition:null},B={ReactCurrentDispatcher:E,ReactCurrentBatchConfig:L,ReactCurrentOwner:b};t.Component=g},2791:(e,t,s)=>{e.exports=s(9117)},4326:(e,t,s)=>{s.d(t,{t25:()=>ye,Uby:()=>us,WTN:()=>hs,esB:()=>a.es,OBj:()=>Mn.O,qCb:()=>r.qC});var n=s(4942),o=s(2172),i=s(1873),r=s(4323),a=s(8814),l=s(7944),c=s(5357);async function d(e,t){let s=await Promise.all([(0,i.yM)(e,"tokenizer.json",!0,t),(0,i.yM)(e,"tokenizer_config.json",!0,t)]);return null!==t.legacy&&(s[1].legacy=t.legacy),s}function _(e){let t=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];if(void 0!==e.Regex){const t=e.Regex.replace(/\\([#&~])/g,"$1");return new RegExp(t,"gu")}if(void 0!==e.String){const s=(0,o.hr)(e.String);return new RegExp(t?s:"(".concat(s,")"),"gu")}return console.warn("Unknown pattern type:",e),null}function u(e){return new Map(Object.entries(e))}function h(e){const t=e.dims;switch(t.length){case 1:return e.tolist();case 2:if(1!==t[0])throw new Error("Unable to decode tensor with `batch size !== 1`. Use `tokenizer.batch_decode(...)` for batched inputs.");return e.tolist()[0];default:throw new Error("Expected tensor to have 1-2 dimensions, got ".concat(t.length,"."))}}function p(e){return e.replace(/ \./g,".").replace(/ \?/g,"?").replace(/ \!/g,"!").replace(/ ,/g,",").replace(/ \' /g,"'").replace(/ n\'t/g,"n't").replace(/ \'m/g,"'m").replace(/ \'s/g,"'s").replace(/ \'ve/g,"'ve").replace(/ \'re/g,"'re")}function m(e){return e.replace(/[\u0300-\u036f]/g,"")}const f="\\p{P}\\u0021-\\u002F\\u003A-\\u0040\\u005B-\\u0060\\u007B-\\u007E";class g{constructor(e){var t,s,n,o,i;this.content=e.content,this.id=e.id,this.single_word=null!==(t=e.single_word)&&void 0!==t&&t,this.lstrip=null!==(s=e.lstrip)&&void 0!==s&&s,this.rstrip=null!==(n=e.rstrip)&&void 0!==n&&n,this.special=null!==(o=e.special)&&void 0!==o&&o,this.normalized=null!==(i=e.normalized)&&void 0!==i?i:null}}class k extends o.Ag{constructor(e){var t;super(),this.config=e,this.vocab=[],this.tokens_to_ids=new Map,this.unk_token_id=void 0,this.unk_token=void 0,this.end_of_word_suffix=void 0,this.fuse_unk=null!==(t=this.config.fuse_unk)&&void 0!==t&&t}static fromConfig(e){for(var t=arguments.length,s=new Array(t>1?t-1:0),n=1;n<t;n++)s[n-1]=arguments[n];switch(e.type){case"WordPiece":return new w(e);case"Unigram":return new x(e,...s);case"BPE":return new b(e);default:if(e.vocab)return new M(e,...s);throw new Error("Unknown TokenizerModel type: ".concat(e.type))}}_call(e){return this.encode(e)}encode(e){throw Error("encode should be implemented in subclass.")}convert_tokens_to_ids(e){let t=e.map((e=>{var t;return null!==(t=this.tokens_to_ids.get(e))&&void 0!==t?t:this.unk_token_id}));return this.fuse_unk&&(t=function(e,t){let s=[],n=0;for(;n<e.length;)if(s.push(e[n]),e[n]===t)for(;n<e.length&&e[n]===t;)++n;else++n;return s}(t,this.unk_token_id)),t}convert_ids_to_tokens(e){return e.map((e=>{var t;return null!==(t=this.vocab[e])&&void 0!==t?t:this.unk_token}))}}class w extends k{constructor(e){var t;super(e),this.tokens_to_ids=u(e.vocab),this.unk_token_id=this.tokens_to_ids.get(e.unk_token),this.unk_token=e.unk_token,this.max_input_chars_per_word=null!==(t=e.max_input_chars_per_word)&&void 0!==t?t:100,this.vocab=new Array(this.tokens_to_ids.size);for(const[s,n]of this.tokens_to_ids)this.vocab[n]=s}encode(e){let t=[];for(let s of e){let e=[...s];if(e.length>this.max_input_chars_per_word){t.push(this.unk_token);continue}let n=!1,o=0,i=[];for(;o<e.length;){let t=e.length,s=null;for(;o<t;){let n=e.slice(o,t).join("");if(o>0&&(n=this.config.continuing_subword_prefix+n),this.tokens_to_ids.has(n)){s=n;break}--t}if(null===s){n=!0;break}i.push(s),o=t}n?t.push(this.unk_token):t.push(...i)}return t}}class x extends k{constructor(e,t){super(e);const s=e.vocab.length;this.vocab=new Array(s),this.scores=new Array(s);for(let n=0;n<s;++n){const t=e.vocab[n];this.vocab[n]=t[0],this.scores[n]=t[1]}this.unk_token_id=e.unk_id,this.unk_token=this.vocab[e.unk_id],this.tokens_to_ids=new Map(this.vocab.map(((e,t)=>[e,t]))),this.bosToken=" ",this.bosTokenId=this.tokens_to_ids.get(this.bosToken),this.eosToken=t.eos_token,this.eosTokenId=this.tokens_to_ids.get(this.eosToken),this.unkToken=this.vocab[this.unk_token_id],this.minScore=(0,r.VV)(this.scores)[0],this.unkScore=this.minScore-10,this.scores[this.unk_token_id]=this.unkScore,this.trie=new l.GA,this.trie.extend(this.vocab),this.fuse_unk=!0}populateNodes(e){const t=e.sentence,s=t.length;let n=0;for(;n<s;){const s=1;let o=!1;const i=[];for(let r of this.trie.commonPrefixSearch(t.slice(n))){i.push(r);const t=this.tokens_to_ids.get(r),a=this.scores[t],l=r.length;e.insert(n,l,a,t),o||l!==s||(o=!0)}o||e.insert(n,s,this.unkScore,this.unk_token_id),n+=s}}tokenize(e){const t=new l.pQ(e,this.bosTokenId,this.eosTokenId);return this.populateNodes(t),t.tokens()}encode(e){let t=[];for(let s of e){const e=this.tokenize(s);t.push(...e)}return t}}const y=(()=>{const e=[...Array.from({length:"~".charCodeAt(0)-"!".charCodeAt(0)+1},((e,t)=>t+"!".charCodeAt(0))),...Array.from({length:"\xac".charCodeAt(0)-"\xa1".charCodeAt(0)+1},((e,t)=>t+"\xa1".charCodeAt(0))),...Array.from({length:"\xff".charCodeAt(0)-"\xae".charCodeAt(0)+1},((e,t)=>t+"\xae".charCodeAt(0)))];let t=e.slice(),s=0;for(let o=0;o<256;++o)e.includes(o)||(e.push(o),t.push(256+s),s+=1);let n=t.map((e=>String.fromCharCode(e)));return Object.fromEntries(e.map(((e,t)=>[e,n[t]])))})(),v=(0,o.$2)(y);class b extends k{constructor(e){var t,s;super(e),this.BPE_SPLIT_TOKEN=" ",this.tokens_to_ids=u(e.vocab),this.unk_token_id=this.tokens_to_ids.get(e.unk_token),this.unk_token=e.unk_token,this.vocab=new Array(this.tokens_to_ids.size);for(const[n,o]of this.tokens_to_ids)this.vocab[o]=n;this.bpe_ranks=new Map(e.merges.map(((e,t)=>[e,t]))),this.merges=e.merges.map((e=>e.split(this.BPE_SPLIT_TOKEN))),this.end_of_word_suffix=e.end_of_word_suffix,this.continuing_subword_suffix=null!==(t=e.continuing_subword_suffix)&&void 0!==t?t:null,this.byte_fallback=null!==(s=this.config.byte_fallback)&&void 0!==s&&s,this.byte_fallback&&(this.text_encoder=new TextEncoder),this.cache=new Map}bpe(e){if(0===e.length)return[];const t=this.cache.get(e);if(void 0!==t)return t;const s=Array.from(e);this.end_of_word_suffix&&(s[s.length-1]+=this.end_of_word_suffix);let n=[];if(s.length>1){const e=new l.Z3(((e,t)=>e.score<t.score));let t={token:s[0],bias:0,prev:null,next:null},o=t;for(let n=1;n<s.length;++n){const t={bias:n/s.length,token:s[n],prev:o,next:null};o.next=t,this._add_node(e,o),o=t}for(;!e.isEmpty();){const s=e.pop();if(s.deleted||!s.next||s.next.deleted)continue;if(s.deleted=!0,s.next.deleted=!0,s.prev){const e={...s.prev};s.prev.deleted=!0,s.prev=e,e.prev?e.prev.next=e:t=e}const n={token:s.token+s.next.token,bias:s.bias,prev:s.prev,next:s.next.next};n.prev?(n.prev.next=n,this._add_node(e,n.prev)):t=n,n.next&&(n.next.prev=n,this._add_node(e,n))}for(let s=t;null!==s;s=s.next)n.push(s.token)}else n=s;if(this.continuing_subword_suffix)for(let o=0;o<n.length-1;++o)n[o]+=this.continuing_subword_suffix;return this.cache.set(e,n),n}_add_node(e,t){const s=this.bpe_ranks.get(t.token+this.BPE_SPLIT_TOKEN+t.next.token);void 0!==s&&(t.score=s+t.bias,e.push(t))}encode(e){let t=[];for(let s of e){let e=this.bpe(s);for(let s of e)this.tokens_to_ids.has(s)?t.push(s):this.byte_fallback?t.push(...Array.from(this.text_encoder.encode(s)).map((e=>"<0x".concat(e.toString(16).toUpperCase().padStart(2,"0"),">")))):t.push(this.unk_token)}return t}}class M extends k{constructor(e,t){super(e),this.tokens_to_ids=u(t.target_lang?e.vocab[t.target_lang]:e.vocab),this.bos_token=t.bos_token,this.bos_token_id=this.tokens_to_ids.get(this.bos_token),this.eos_token=t.eos_token,this.eos_token_id=this.tokens_to_ids.get(this.eos_token),this.pad_token=t.pad_token,this.pad_token_id=this.tokens_to_ids.get(this.pad_token),this.unk_token=t.unk_token,this.unk_token_id=this.tokens_to_ids.get(this.unk_token),this.vocab=new Array(this.tokens_to_ids.size);for(const[s,n]of this.tokens_to_ids)this.vocab[n]=s}encode(e){return e}}class S extends o.Ag{constructor(e){super(),this.config=e}static fromConfig(e){if(null===e)return null;switch(e.type){case"BertNormalizer":return new N(e);case"Precompiled":return new ie(e);case"Sequence":return new B(e);case"Replace":return new A(e);case"NFC":return new T(e);case"NFKC":return new C(e);case"NFKD":return new z(e);case"Strip":return new P(e);case"StripAccents":return new F(e);case"Lowercase":return new E(e);case"Prepend":return new L(e);default:throw new Error("Unknown Normalizer type: ".concat(e.type))}}normalize(e){throw Error("normalize should be implemented in subclass.")}_call(e){return this.normalize(e)}}class A extends S{normalize(e){let t=_(this.config.pattern);return null===t?e:e=e.replaceAll(t,this.config.content)}}class T extends S{normalize(e){return e=e.normalize("NFC")}}class C extends S{normalize(e){return e=e.normalize("NFKC")}}class z extends S{normalize(e){return e=e.normalize("NFKD")}}class P extends S{normalize(e){return this.config.strip_left&&this.config.strip_right?e=e.trim():(this.config.strip_left&&(e=e.trimStart()),this.config.strip_right&&(e=e.trimEnd())),e}}class F extends S{normalize(e){return e=m(e)}}class E extends S{normalize(e){return e=e.toLowerCase()}}class L extends S{normalize(e){return e=this.config.prepend+e}}class B extends S{constructor(e){super(e),this.normalizers=e.normalizers.map((e=>S.fromConfig(e)))}normalize(e){return this.normalizers.reduce(((e,t)=>t.normalize(e)),e)}}class N extends S{_tokenize_chinese_chars(e){let t=[];for(let s=0;s<e.length;++s){let n=e[s],o=n.charCodeAt(0);this._is_chinese_char(o)?(t.push(" "),t.push(n),t.push(" ")):t.push(n)}return t.join("")}_is_chinese_char(e){return e>=19968&&e<=40959||e>=13312&&e<=19903||e>=131072&&e<=173791||e>=173824&&e<=177983||e>=177984&&e<=178207||e>=178208&&e<=183983||e>=63744&&e<=64255||e>=194560&&e<=195103}stripAccents(e){return e.normalize("NFD").replace(/[\u0300-\u036f]/g,"")}normalize(e){return this.config.handle_chinese_chars&&(e=this._tokenize_chinese_chars(e)),this.config.lowercase?(e=e.toLowerCase(),!1!==this.config.strip_accents&&(e=this.stripAccents(e))):this.config.strip_accents&&(e=this.stripAccents(e)),e}}class I extends o.Ag{static fromConfig(e){if(null===e)return null;switch(e.type){case"BertPreTokenizer":return new j(e);case"Sequence":return new re(e);case"WhitespaceSplit":return new ae(e);case"Metaspace":return new ne(e);case"ByteLevel":return new O(e);case"Split":return new D(e);case"Punctuation":return new q(e);case"Digits":return new G(e);case"Replace":return new le(e);default:throw new Error("Unknown PreTokenizer type: ".concat(e.type))}}pre_tokenize_text(e,t){throw Error("pre_tokenize_text should be implemented in subclass.")}pre_tokenize(e,t){let s=[];return s=Array.isArray(e)?e.map((e=>this.pre_tokenize_text(e,t))):this.pre_tokenize_text(e,t),s.flat()}_call(e,t){return this.pre_tokenize(e,t)}}class j extends I{constructor(e){super(),this.pattern=new RegExp("[^\\s".concat(f,"]+|[").concat(f,"]"),"gu")}pre_tokenize_text(e,t){return e.trim().match(this.pattern)||[]}}class O extends I{constructor(e){var t;super(),this.config=e,this.add_prefix_space=this.config.add_prefix_space,this.trim_offsets=this.config.trim_offsets,this.use_regex=null===(t=this.config.use_regex)||void 0===t||t,this.pattern=/'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+/gu,this.byte_encoder=y,this.text_encoder=new TextEncoder}pre_tokenize_text(e,t){return this.add_prefix_space&&!e.startsWith(" ")&&(e=" "+e),(this.use_regex?e.match(this.pattern)||[]:[e]).map((e=>Array.from(this.text_encoder.encode(e),(e=>this.byte_encoder[e])).join("")))}}class D extends I{constructor(e){super(),this.config=e,this.pattern=_(this.config.pattern,this.config.invert)}pre_tokenize_text(e,t){return null===this.pattern?[]:this.config.invert?e.match(this.pattern)||[]:function(e,t){const s=[];let n=0;for(const o of e.matchAll(t)){const t=o[0];n<o.index&&s.push(e.slice(n,o.index)),t.length>0&&s.push(t),n=o.index+t.length}return n<e.length&&s.push(e.slice(n)),s}(e,this.pattern)}}class q extends I{constructor(e){super(),this.config=e,this.pattern=new RegExp("[^".concat(f,"]+|[").concat(f,"]+"),"gu")}pre_tokenize_text(e,t){return e.match(this.pattern)||[]}}class G extends I{constructor(e){super(),this.config=e;const t="[^\\d]+|\\d".concat(this.config.individual_digits?"":"+");this.pattern=new RegExp(t,"gu")}pre_tokenize_text(e,t){return e.match(this.pattern)||[]}}class R extends o.Ag{constructor(e){super(),this.config=e}static fromConfig(e){if(null===e)return null;switch(e.type){case"TemplateProcessing":return new U(e);case"ByteLevel":return new V(e);case"RobertaProcessing":return new W(e);case"BertProcessing":return new Z(e);default:throw new Error("Unknown PostProcessor type: ".concat(e.type))}}post_process(e){throw Error("post_process should be implemented in subclass.")}_call(e){for(var t=arguments.length,s=new Array(t>1?t-1:0),n=1;n<t;n++)s[n-1]=arguments[n];return this.post_process(e,...s)}}class Z extends R{constructor(e){super(e),this.cls=e.cls[0],this.sep=e.sep[0]}post_process(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null;return e=(0,o.eG)([this.cls],e,[this.sep]),null!==t&&(e=(0,o.eG)(e,[this.sep],t,[this.sep])),e}}class W extends Z{}class U extends R{constructor(e){super(e),this.single=e.single,this.pair=e.pair}post_process(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null,s=null===t?this.single:this.pair,n=[];for(let i of s)"SpecialToken"in i?n.push(i.SpecialToken.id):"Sequence"in i&&("A"===i.Sequence.id?n=(0,o.eG)(n,e):"B"===i.Sequence.id&&(n=(0,o.eG)(n,t)));return n}}class V extends R{post_process(e){return e}}class K extends o.Ag{constructor(e){super(),this.config=e,this.added_tokens=[],this.end_of_word_suffix=null,this.trim_offsets=e.trim_offsets}static fromConfig(e){if(null===e)return null;switch(e.type){case"WordPiece":return new H(e);case"Metaspace":return new oe(e);case"ByteLevel":return new J(e);case"Replace":return new $(e);case"ByteFallback":return new X(e);case"Fuse":return new Q(e);case"Strip":return new Y(e);case"Sequence":return new te(e);case"CTC":return new ee(e);case"BPEDecoder":return new se(e);default:throw new Error("Unknown Decoder type: ".concat(e.type))}}_call(e){return this.decode(e)}decode(e){return this.decode_chain(e).join("")}decode_chain(e){throw Error("`decode_chain` should be implemented in subclass.")}}class $ extends K{decode_chain(e){let t=_(this.config.pattern);return null===t?e:e.map((e=>e.replaceAll(t,this.config.content)))}}class X extends K{constructor(e){super(e),this.text_decoder=new TextDecoder}decode_chain(e){let t=[],s=[];for(let n of e){let e=null;if(6===n.length&&n.startsWith("<0x")&&n.endsWith(">")){let t=parseInt(n.slice(3,5),16);isNaN(t)||(e=t)}if(null!==e)s.push(e);else{if(s.length>0){let e=this.text_decoder.decode(Uint8Array.from(s));t.push(e),s=[]}t.push(n)}}if(s.length>0){let e=this.text_decoder.decode(Uint8Array.from(s));t.push(e),s=[]}return t}}class Q extends K{decode_chain(e){return[e.join("")]}}class Y extends K{constructor(e){super(e),this.content=this.config.content,this.start=this.config.start,this.stop=this.config.stop}decode_chain(e){return e.map((e=>{let t=0;for(let n=0;n<this.start&&e[n]===this.content;++n)t=n+1;let s=e.length;for(let n=0;n<this.stop;++n){const t=e.length-n-1;if(e[t]!==this.content)break;s=t}return e.slice(t,s)}))}}class H extends K{constructor(e){super(e),this.cleanup=e.cleanup}decode_chain(e){return e.map(((e,t)=>(0!==t&&(e=e.startsWith(this.config.prefix)?e.replace(this.config.prefix,""):" "+e),this.cleanup&&(e=p(e)),e)))}}class J extends K{constructor(e){super(e),this.byte_decoder=v,this.text_decoder=new TextDecoder("utf-8",{fatal:!1,ignoreBOM:!0}),this.end_of_word_suffix=null}convert_tokens_to_string(e){let t=e.join(""),s=new Uint8Array([...t].map((e=>this.byte_decoder[e])));return this.text_decoder.decode(s)}decode_chain(e){let t=[],s=[];for(let n of e)void 0!==this.added_tokens.find((e=>e.content===n))?(s.length>0&&(t.push(this.convert_tokens_to_string(s)),s=[]),t.push(n)):s.push(n);return s.length>0&&t.push(this.convert_tokens_to_string(s)),t}}class ee extends K{constructor(e){super(e),this.pad_token=this.config.pad_token,this.word_delimiter_token=this.config.word_delimiter_token,this.cleanup=this.config.cleanup}convert_tokens_to_string(e){if(0===e.length)return"";let t=[e[0]];for(let n=1;n<e.length;++n)e[n]!==t.at(-1)&&t.push(e[n]);let s=t.filter((e=>e!==this.pad_token)).join("");return this.cleanup&&(s=p(s).replaceAll(this.word_delimiter_token," ").trim()),s}decode_chain(e){return[this.convert_tokens_to_string(e)]}}class te extends K{constructor(e){super(e),this.decoders=e.decoders.map((e=>K.fromConfig(e)))}decode_chain(e){return this.decoders.reduce(((e,t)=>t.decode_chain(e)),e)}}class se extends K{constructor(e){super(e),this.suffix=this.config.suffix}decode_chain(e){return e.map(((t,s)=>t.replaceAll(this.suffix,s===e.length-1?"":" ")))}}class ne extends I{constructor(e){var t;super(),this.addPrefixSpace=e.add_prefix_space,this.replacement=e.replacement,this.strRep=e.str_rep||this.replacement,this.prepend_scheme=null!==(t=e.prepend_scheme)&&void 0!==t?t:"always"}pre_tokenize_text(e){let{section_index:t}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},s=e.replaceAll(" ",this.strRep);return this.addPrefixSpace&&!s.startsWith(this.replacement)&&("always"===this.prepend_scheme||"first"===this.prepend_scheme&&0===t)&&(s=this.strRep+s),[s]}}class oe extends K{constructor(e){super(e),this.addPrefixSpace=e.add_prefix_space,this.replacement=e.replacement}decode_chain(e){let t=[];for(let s=0;s<e.length;++s){let n=e[s].replaceAll(this.replacement," ");this.addPrefixSpace&&0==s&&n.startsWith(" ")&&(n=n.substring(1)),t.push(n)}return t}}class ie extends S{constructor(e){super(e),this.charsmap=e.precompiled_charsmap}normalize(e){if((e=(e=e.replace(/[\u0001-\u0008\u000B\u000E-\u001F\u007F\u008F\u009F]/gm,"")).replace(/[\u0009\u000A\u000C\u000D\u1680\u200B\u200C\u200E\u200F\u2028\u2029\u2581\uFEFF\uFFFD]/gm," ")).includes("\uff5e")){const t=e.split("\uff5e");e=t.map((e=>e.normalize("NFKC"))).join("\uff5e")}else e=e.normalize("NFKC");return e}}class re extends I{constructor(e){super(),this.tokenizers=e.pretokenizers.map((e=>I.fromConfig(e)))}pre_tokenize_text(e,t){return this.tokenizers.reduce(((e,s)=>s.pre_tokenize(e,t)),[e])}}class ae extends I{constructor(e){super()}pre_tokenize_text(e,t){return function(e){return e.match(/\S+/g)||[]}(e)}}class le extends I{constructor(e){super(),this.config=e,this.pattern=_(this.config.pattern),this.content=this.config.content}pre_tokenize_text(e,t){return null===this.pattern?[e]:[e.replaceAll(this.pattern,this.config.content)]}}const ce=["bos_token","eos_token","unk_token","sep_token","pad_token","cls_token","mask_token"];class de extends o.Ag{constructor(e,t){var s,i,r,a;super(),(0,n.Z)(this,"_default_chat_template","{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"),this._tokenizer_config=t,this.normalizer=S.fromConfig(e.normalizer),this.pre_tokenizer=I.fromConfig(e.pre_tokenizer),this.model=k.fromConfig(e.model,t),this.post_processor=R.fromConfig(e.post_processor),this.decoder=K.fromConfig(e.decoder),this.special_tokens=[],this.all_special_ids=[],this.added_tokens=[];for(let n of e.added_tokens){const e=new g(n);this.added_tokens.push(e),this.model.tokens_to_ids.set(e.content,e.id),this.model.vocab[e.id]=e.content,e.special&&(this.special_tokens.push(e.content),this.all_special_ids.push(e.id))}this.additional_special_tokens=null!==(s=t.additional_special_tokens)&&void 0!==s?s:[],this.special_tokens.push(...this.additional_special_tokens),this.special_tokens=[...new Set(this.special_tokens)],this.decoder&&(this.decoder.added_tokens=this.added_tokens,this.decoder.end_of_word_suffix=this.model.end_of_word_suffix),this.added_tokens_regex=this.added_tokens.length>0?new RegExp(this.added_tokens.map((e=>"".concat(e.lstrip?"\\s*":"","(").concat((0,o.hr)(e.content),")").concat(e.rstrip?"\\s*":""))).join("|")):null,this.mask_token=this.getToken("mask_token"),this.mask_token_id=this.model.tokens_to_ids.get(this.mask_token),this.pad_token=this.getToken("pad_token","eos_token"),this.pad_token_id=this.model.tokens_to_ids.get(this.pad_token),this.sep_token=this.getToken("sep_token"),this.sep_token_id=this.model.tokens_to_ids.get(this.sep_token),this.unk_token=this.getToken(t,"unk_token"),this.unk_token_id=this.model.tokens_to_ids.get(this.unk_token),this.model_max_length=t.model_max_length,this.remove_space=t.remove_space,this.clean_up_tokenization_spaces=null===(i=t.clean_up_tokenization_spaces)||void 0===i||i,this.do_lowercase_and_remove_accent=null!==(r=t.do_lowercase_and_remove_accent)&&void 0!==r&&r,this.padding_side="right",this.legacy=!1,this.chat_template=null!==(a=t.chat_template)&&void 0!==a?a:null,this._compiled_template_cache=new Map}getToken(){for(var e=arguments.length,t=new Array(e),s=0;s<e;s++)t[s]=arguments[s];for(let n of t){let e=this._tokenizer_config[n];if(e){if("object"===typeof e){if("AddedToken"===e.__type)return e.content;throw Error("Unknown token: ".concat(e))}return e}}return null}static async from_pretrained(e){let{progress_callback:t=null,config:s=null,cache_dir:n=null,local_files_only:o=!1,revision:i="main",legacy:r=null}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return new this(...await d(e,{progress_callback:t,config:s,cache_dir:n,local_files_only:o,revision:i,legacy:r}))}prepare_model_inputs(e){return e}_call(e){let t,{text_pair:s=null,add_special_tokens:n=!0,padding:o=!1,truncation:i=null,max_length:l=null,return_tensor:c=!0}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if(Array.isArray(e)){if(0===e.length)throw Error("text array must be non-empty");if(null!==s){if(!Array.isArray(s))throw Error("text_pair must also be an array");if(e.length!==s.length)throw Error("text and text_pair must have the same length");t=e.map(((e,t)=>this.encode(e,s[t],{add_special_tokens:n})))}else t=e.map((e=>this.encode(e,null,{add_special_tokens:n})))}else{if(null===e)throw Error("text may not be null");if(Array.isArray(s))throw Error("When specifying `text_pair`, since `text` is a string, `text_pair` must also be a string (i.e., not an array).");t=[this.encode(e,s,{add_special_tokens:n})]}let d=(0,r.Fp)(t.map((e=>e.length)))[0];null===l&&(l=d),l=Math.min(l,this.model_max_length);let _=[];if(o||i)for(let r=0;r<t.length;++r)if(t[r].length!==l)if(t[r].length>l)i&&(t[r]=t[r].slice(0,l)),_.push(new Array(t[r].length).fill(1));else if(o){let e=l-t[r].length;"right"===this.padding_side?(_.push(new Array(t[r].length).fill(1).concat(new Array(e).fill(0))),t[r].push(...new Array(e).fill(this.pad_token_id))):(_.push(new Array(e).fill(0).concat(new Array(t[r].length).fill(1))),t[r].unshift(...new Array(e).fill(this.pad_token_id)))}else _.push(new Array(t[r].length).fill(1));else _.push(new Array(t[r].length).fill(1));else _=t.map((e=>new Array(e.length).fill(1)));if(c){if((!o||!i)&&t.some((e=>e.length!==t[0].length)))throw Error("Unable to create tensor, you should probably activate truncation and/or padding with 'padding=true' and 'truncation=true' to have batched tensors with the same length.");let e=[t.length,t[0].length];t=new a.es("int64",BigInt64Array.from(t.flat().map(BigInt)),e),_=new a.es("int64",BigInt64Array.from(_.flat().map(BigInt)),e)}else Array.isArray(e)||(t=t[0],_=_[0]);let u={input_ids:t,attention_mask:_};return u=this.prepare_model_inputs(u),u}_encode_text(e){if(null===e)return null;const t=(this.added_tokens_regex?e.split(this.added_tokens_regex).filter((e=>e)):[e]).map(((e,t)=>{if(void 0!==this.added_tokens.find((t=>t.content===e)))return e;{!0===this.remove_space&&(e=e.trim().split(/\s+/).join(" ")),this.do_lowercase_and_remove_accent&&(e=function(e){return m(e.toLowerCase())}(e)),null!==this.normalizer&&(e=this.normalizer(e));const s=null!==this.pre_tokenizer?this.pre_tokenizer(e,{section_index:t}):[e];return this.model(s)}})).flat();return t}encode(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null,{add_special_tokens:s=!0}=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{},n=this._encode_text(e),i=this._encode_text(t),r=null!==this.post_processor&&s?this.post_processor(n,i):(0,o.eG)(null!==n&&void 0!==n?n:[],null!==i&&void 0!==i?i:[]);return this.model.convert_tokens_to_ids(r)}batch_decode(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return e instanceof a.es&&(e=e.tolist()),e.map((e=>this.decode(e,t)))}decode(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if(e instanceof a.es&&(e=h(e)),!Array.isArray(e)||0===e.length||!(0,o.Wy)(e[0]))throw Error("token_ids must be a non-empty array of integers.");return this.decode_single(e,t)}decode_single(e,t){let{skip_special_tokens:s=!1,clean_up_tokenization_spaces:n=null}=t,o=this.model.convert_ids_to_tokens(e);s&&(o=o.filter((e=>!this.special_tokens.includes(e))));let i=this.decoder?this.decoder(o):o.join(" ");return this.decoder&&this.decoder.end_of_word_suffix&&(i=i.replaceAll(this.decoder.end_of_word_suffix," "),s&&(i=i.trim())),(null!==n&&void 0!==n?n:this.clean_up_tokenization_spaces)&&(i=p(i)),i}get default_chat_template(){return this._warned_about_chat_template||(console.warn("No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information."),this._warned_about_chat_template=!0),this._default_chat_template}apply_chat_template(e){var t,s;let{chat_template:n=null,add_generation_prompt:o=!1,tokenize:i=!0,padding:r=!1,truncation:a=!1,max_length:l=null,return_tensor:d=!0}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};null!==(t=n)&&void 0!==t||(n=null!==(s=this.chat_template)&&void 0!==s?s:this.default_chat_template);let _=this._compiled_template_cache.get(n);void 0===_&&(_=new c.YS(n),this._compiled_template_cache.set(n,_));const u=Object.create(null);for(const c of ce){const e=this.getToken(c);e&&(u[c]=e)}const h=_.render({messages:e,add_generation_prompt:o,...u});return i?this._call(h,{add_special_tokens:!1,padding:r,truncation:a,max_length:l,return_tensor:d}).input_ids:h}}function _e(e){if(e.input_ids instanceof a.es)e.token_type_ids=new a.es("int64",new BigInt64Array(e.input_ids.data.length),e.input_ids.dims);else{if(!Array.isArray(e.input_ids))throw new Error("Input ids must be a Tensor or an Array");Array.isArray(e.input_ids[0])?e.token_type_ids=e.input_ids.map((e=>new Array(e.length).fill(0))):e.token_type_ids=new Array(e.input_ids.length).fill(0)}return e}class ue extends de{constructor(){super(...arguments),(0,n.Z)(this,"_default_chat_template",'{% for message in messages %}" "{{ message.content }}{{ eos_token }}" "{% endfor %}')}}class he extends de{constructor(e,t){super(e,t),this.languageRegex=/^[a-z]{2}_[A-Z]{2}$/,this.language_codes=this.special_tokens.filter((e=>this.languageRegex.test(e))),this.lang_to_token=e=>e}_build_translation_inputs(e,t,s){return fe(this,e,t,s)}}const pe="\u2581";class me extends de{constructor(e,t){var s,o;super(e,t),(0,n.Z)(this,"_default_chat_template","{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif USE_DEFAULT_PROMPT == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'DEFAULT_SYSTEM_MESSAGE' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\n' + system_message + '\n<</SYS>>\n\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\n' + content.strip() + '\n<</SYS>>\n\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"),(0,n.Z)(this,"DEFAULT_SYSTEM_PROMPT","You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information."),this.use_default_system_prompt=null!==(s=t.use_default_system_prompt)&&void 0!==s&&s,this.legacy=null===(o=t.legacy)||void 0===o||o,this.legacy||(this.normalizer=null,this.pre_tokenizer=new ne({replacement:pe,add_prefix_space:!0,prepend_scheme:"first"}))}_encode_text(e){if(null===e)return null;if(this.legacy||0===e.length)return super._encode_text(e);let t=super._encode_text(pe+e.replaceAll(pe," "));return t.length>1&&t[0]===pe&&this.special_tokens.includes(t[1])&&(t=t.slice(1)),t}get default_chat_template(){return super.default_chat_template.replaceAll("USE_DEFAULT_PROMPT",this.use_default_system_prompt?"true":"false").replaceAll("DEFAULT_SYSTEM_MESSAGE",this.DEFAULT_SYSTEM_PROMPT.replaceAll("\n","\\n").replaceAll("'","\\'"))}}function fe(e,t,s,n){if(!("language_codes"in e)||!Array.isArray(e.language_codes))throw new Error("Tokenizer must have `language_codes` attribute set and it should be an array of language ids.");if(!("languageRegex"in e)||!(e.languageRegex instanceof RegExp))throw new Error("Tokenizer must have `languageRegex` attribute set and it should be a regular expression.");if(!("lang_to_token"in e)||"function"!==typeof e.lang_to_token)throw new Error("Tokenizer must have `lang_to_token` attribute set and it should be a function.");const o=n.src_lang,i=n.tgt_lang;if(!e.language_codes.includes(i))throw new Error('Target language code "'.concat(i,'" is not valid. Must be one of: {').concat(e.language_codes.join(", "),"}"));if(void 0!==o){if(!e.language_codes.includes(o))throw new Error('Source language code "'.concat(o,'" is not valid. Must be one of: {').concat(e.language_codes.join(", "),"}"));for(let t of e.post_processor.config.single)if("SpecialToken"in t&&e.languageRegex.test(t.SpecialToken.id)){t.SpecialToken.id=e.lang_to_token(o);break}}return n.forced_bos_token_id=e.model.convert_tokens_to_ids([e.lang_to_token(i)])[0],e._call(t,s)}const ge=[["en","english"],["zh","chinese"],["de","german"],["es","spanish"],["ru","russian"],["ko","korean"],["fr","french"],["ja","japanese"],["pt","portuguese"],["tr","turkish"],["pl","polish"],["ca","catalan"],["nl","dutch"],["ar","arabic"],["sv","swedish"],["it","italian"],["id","indonesian"],["hi","hindi"],["fi","finnish"],["vi","vietnamese"],["he","hebrew"],["uk","ukrainian"],["el","greek"],["ms","malay"],["cs","czech"],["ro","romanian"],["da","danish"],["hu","hungarian"],["ta","tamil"],["no","norwegian"],["th","thai"],["ur","urdu"],["hr","croatian"],["bg","bulgarian"],["lt","lithuanian"],["la","latin"],["mi","maori"],["ml","malayalam"],["cy","welsh"],["sk","slovak"],["te","telugu"],["fa","persian"],["lv","latvian"],["bn","bengali"],["sr","serbian"],["az","azerbaijani"],["sl","slovenian"],["kn","kannada"],["et","estonian"],["mk","macedonian"],["br","breton"],["eu","basque"],["is","icelandic"],["hy","armenian"],["ne","nepali"],["mn","mongolian"],["bs","bosnian"],["kk","kazakh"],["sq","albanian"],["sw","swahili"],["gl","galician"],["mr","marathi"],["pa","punjabi"],["si","sinhala"],["km","khmer"],["sn","shona"],["yo","yoruba"],["so","somali"],["af","afrikaans"],["oc","occitan"],["ka","georgian"],["be","belarusian"],["tg","tajik"],["sd","sindhi"],["gu","gujarati"],["am","amharic"],["yi","yiddish"],["lo","lao"],["uz","uzbek"],["fo","faroese"],["ht","haitian creole"],["ps","pashto"],["tk","turkmen"],["nn","nynorsk"],["mt","maltese"],["sa","sanskrit"],["lb","luxembourgish"],["my","myanmar"],["bo","tibetan"],["tl","tagalog"],["mg","malagasy"],["as","assamese"],["tt","tatar"],["haw","hawaiian"],["ln","lingala"],["ha","hausa"],["ba","bashkir"],["jw","javanese"],["su","sundanese"]],ke=new Map(ge),we=new Map([...ge.map((e=>{let[t,s]=e;return[s,t]})),["burmese","my"],["valencian","ca"],["flemish","nl"],["haitian","ht"],["letzeburgesch","lb"],["pushto","ps"],["panjabi","pa"],["moldavian","ro"],["moldovan","ro"],["sinhalese","si"],["castilian","es"]]);class xe extends de{constructor(){super(...arguments),(0,n.Z)(this,"_default_chat_template","{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}")}}class ye{static async from_pretrained(e){var t,s;let{quantized:n=!0,progress_callback:o=null,config:i=null,cache_dir:r=null,local_files_only:a=!1,revision:l="main",legacy:c=null}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},[_,u]=await d(e,{quantized:n,progress_callback:o,config:i,cache_dir:r,local_files_only:a,revision:l,legacy:c}),h=null!==(t=null===(s=u.tokenizer_class)||void 0===s?void 0:s.replace(/Fast$/,""))&&void 0!==t?t:"PreTrainedTokenizer",p=this.TOKENIZER_CLASS_MAPPING[h];return p||(console.warn('Unknown tokenizer class "'.concat(h,'", attempting to construct from base class.')),p=de),new p(_,u)}}(0,n.Z)(ye,"TOKENIZER_CLASS_MAPPING",{T5Tokenizer:class extends de{},DistilBertTokenizer:class extends de{},CamembertTokenizer:class extends de{},DebertaTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},DebertaV2Tokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},BertTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},HerbertTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},ConvBertTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},XLMTokenizer:class extends de{constructor(e,t){super(e,t),console.warn('WARNING: `XLMTokenizer` is not yet supported by Hugging Face\'s "fast" tokenizers library. Therefore, you may experience slightly inaccurate results.')}prepare_model_inputs(e){return _e(e)}},ElectraTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},MobileBertTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},SqueezeBertTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},AlbertTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},GPT2Tokenizer:ue,BartTokenizer:class extends de{},MBartTokenizer:he,MBart50Tokenizer:class extends he{},RobertaTokenizer:class extends de{},WhisperTokenizer:class extends de{constructor(){super(...arguments),(0,n.Z)(this,"_default_chat_template",'{% for message in messages %}" "{{ message.content }}{{ eos_token }}" "{% endfor %}')}_decode_asr(e){let{return_timestamps:t=!1,return_language:s=!1,time_precision:n=null,force_full_sequences:o=!0}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if(null===n)throw Error("Must specify time_precision");let i=null;const a="word"===t;function l(){return{language:i,timestamp:[null,null],text:""}}const c=[];let d=l(),_=0;const u=this.model.convert_tokens_to_ids(["<|notimestamps|>"])[0]+1;let h=[],p=[],m=!1,f=null;const g=new Set(this.all_special_ids);for(let x of e){const e=x.tokens,s=a?x.token_timestamps:null;let o=null,k=u;if("stride"in x){const[t,s,i]=x.stride;if(_-=s,f=t-i,s&&(k=s/n+u),i)for(let r=e.length-1;r>=0;--r){const t=e[r];if(t>=u){if(null!==o&&(t-u)*n<f)break;o=t}}}let w=[],y=[];for(let f=0;f<e.length;++f){const x=e[f];if(g.has(x)){const e=this.decode([x]),s=ke.get(e.slice(2,-2));if(void 0!==s){if(null!==i&&s!==i&&!t){h.push(w);const e=this.findLongestCommonSequence(h)[0],t=this.decode(e);d.text=t,c.push(d),h=[],w=[],d=l()}i=d.language=s}}else if(x>=u){const e=(x-u)*n+_,t=(0,r.NM)(e,2);if(null!==o&&x>=o)m=!0;else if(m||h.length>0&&x<k)m=!1;else if(null===d.timestamp[0])d.timestamp[0]=t;else if(t===d.timestamp[0]);else{d.timestamp[1]=t,h.push(w),a&&p.push(y);const[e,s]=this.findLongestCommonSequence(h,p),n=this.decode(e);d.text=n,a&&(d.words=this.collateWordTimestamps(e,s,i)),c.push(d),h=[],w=[],p=[],y=[],d=l()}}else if(w.push(x),a){let e,t=(0,r.NM)(s[f]+_,2);e=f+1<s.length?(0,r.NM)(s[f+1]+_,2):null,y.push([t,e])}}if("stride"in x){const[e,t,s]=x.stride;_+=e-s}w.length>0?(h.push(w),a&&p.push(y)):h.every((e=>0===e.length))&&(d=l(),h=[],w=[],p=[],y=[])}if(h.length>0){if(o&&t)throw new Error("Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.");const[e,s]=this.findLongestCommonSequence(h,p),n=this.decode(e);d.text=n,a&&(d.words=this.collateWordTimestamps(e,s,i)),c.push(d)}let k=Object.create(null);const w=c.map((e=>e.text)).join("");if(t||s){for(let e=0;e<c.length;++e){const n=c[e];t||delete n.timestamp,s||delete n.language}if(a){let e=[];for(let t of c)for(let s of t.words)e.push(s);k={chunks:e}}else k={chunks:c}}return[w,k]}findLongestCommonSequence(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null,s=e[0],n=s.length,o=[];const i=Array.isArray(t)&&t.length>0;let r=i?[]:null,a=i?t[0]:null;for(let l=1;l<e.length;++l){const c=e[l];let d=0,_=[n,n,0,0];const u=c.length;for(let e=1;e<n+u;++e){const t=e/1e4,o=Math.max(0,n-e),i=Math.min(n,n+u-e),r=s.slice(o,i),a=Math.max(0,e-n),l=Math.min(u,e),h=c.slice(a,l);if(r.length!==h.length)throw new Error("There is a bug within whisper `decode_asr` function, please report it. Dropping to prevent bad inference.");const p=r.filter(((e,t)=>e===h[t])).length,m=p/e+t;p>1&&m>d&&(d=m,_=[o,i,a,l])}const[h,p,m,f]=_,g=Math.floor((p+h)/2),k=Math.floor((f+m)/2);o.push(...s.slice(0,g)),s=c.slice(k),n=s.length,i&&(r.push(...a.slice(0,g)),a=t[l].slice(k))}return o.push(...s),i?(r.push(...a),[o,r]):[o,[]]}collateWordTimestamps(e,t,s){let[n,o,i]=this.combineTokensIntoWords(e,s),r=[];for(let a=0;a<n.length;++a){const e=i[a];r.push({text:n[a],timestamp:[t[e.at(0)][0],t[e.at(-1)][1]]})}return r}combineTokensIntoWords(e,t){var s;let n,o,i,r=arguments.length>2&&void 0!==arguments[2]?arguments[2]:"\"'\u201c\xa1\xbf([{-",a=arguments.length>3&&void 0!==arguments[3]?arguments[3]:"\"'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001";return t=null!==(s=t)&&void 0!==s?s:"english",["chinese","japanese","thai","lao","myanmar"].includes(t)?[n,o,i]=this.splitTokensOnUnicode(e):[n,o,i]=this.splitTokensOnSpaces(e),this.mergePunctuations(n,o,i,r,a)}decode(e,t){let s;return t&&t.decode_with_timestamps?(e instanceof a.es&&(e=h(e)),s=this.decodeWithTimestamps(e,t)):s=super.decode(e,t),s}decodeWithTimestamps(e,t){var s;const n=null!==(s=null===t||void 0===t?void 0:t.time_precision)&&void 0!==s?s:.02,o=Array.from(this.all_special_ids).at(-1)+1;let i=[[]];for(let a of e)if(a>=o){let e=(a-o)*n;e=(0,r.NM)(e,2),i.push("<|".concat(e,"|>")),i.push([])}else i[i.length-1].push(a);return i=i.map((e=>"string"===typeof e?e:super.decode(e,t))),i.join("")}splitTokensOnUnicode(e){const t=this.decode(e,{decode_with_timestamps:!0});let s=[],n=[],o=[],i=[],r=[],a=0;for(let l=0;l<e.length;++l){const c=e[l];i.push(c),r.push(l);const d=this.decode(i,{decode_with_timestamps:!0});d.includes("\ufffd")&&"\ufffd"!==t[a+d.indexOf("\ufffd")]||(s.push(d),n.push(i),o.push(r),i=[],r=[],a+=d.length)}return[s,n,o]}splitTokensOnSpaces(e){let[t,s,n]=this.splitTokensOnUnicode(e),o=[],i=[],r=[];const a=new RegExp("^[".concat(f,"]$"),"gu");for(let l=0;l<t.length;++l){const e=t[l],c=s[l],d=n[l],_=c[0]>=this.model.tokens_to_ids.get("<|endoftext|>"),u=e.startsWith(" "),h=e.trim(),p=a.test(h);if(_||u||p||0===o.length)o.push(e),i.push(c),r.push(d);else{const t=o.length-1;o[t]+=e,i[t].push(...c),r[t].push(...d)}}return[o,i,r]}mergePunctuations(e,t,s,n,i){let r=structuredClone(e),a=structuredClone(t),l=structuredClone(s),c=r.length-2,d=r.length-1;for(;c>=0;)r[c].startsWith(" ")&&n.includes(r[c].trim())?(r[d]=r[c]+r[d],a[d]=(0,o.eG)(a[c],a[d]),l[d]=(0,o.eG)(l[c],l[d]),r[c]="",a[c]=[],l[c]=[]):d=c,--c;for(c=0,d=1;d<r.length;)!r[c].endsWith(" ")&&i.includes(r[d])?(r[c]+=r[d],a[c]=(0,o.eG)(a[c],a[d]),l[c]=(0,o.eG)(l[c],l[d]),r[d]="",a[d]=[],l[d]=[]):c=d,++d;return[r.filter((e=>e)),a.filter((e=>e.length>0)),l.filter((e=>e.length>0))]}get_decoder_prompt_ids(){let{language:e=null,task:t=null,no_timestamps:s=!0}=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},n=[];if(e){e=e.toLowerCase();let t=we.get(e);if(void 0===t){if(!ke.has(e)){const t=2===e.length?ke.keys():ke.values();throw new Error('Language "'.concat(e,'" is not supported. Must be one of: ').concat(JSON.stringify(t)))}t=e}let s=this.model.tokens_to_ids.get("<|".concat(t,"|>"));if(void 0===s)throw new Error('Unable to find language "'.concat(t,'" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.'));n.push(s)}else n.push(null);if(t){if(t=t.toLowerCase(),"transcribe"!==t&&"translate"!==t)throw new Error('Task "'.concat(t,'" is not supported. Must be one of: ["transcribe", "translate"]'));let e=this.model.tokens_to_ids.get("<|".concat(t,"|>"));if(void 0===e)throw new Error('Unable to find task "'.concat(t,'" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.'));n.push(e)}else n.push(null);if(s){let e=this.model.tokens_to_ids.get("<|notimestamps|>");if(void 0===e)throw new Error('Unable to find "<|notimestamps|>" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.');n.push(e)}return n.map(((e,t)=>[t+1,e])).filter((e=>null!==e[1]))}},CodeGenTokenizer:class extends de{},CLIPTokenizer:class extends de{},MarianTokenizer:class extends de{constructor(e,t){super(e,t),this.languageRegex=/^(>>\w+<<)\s*/g,this.supported_language_codes=this.model.vocab.filter((e=>this.languageRegex.test(e))),console.warn('WARNING: `MarianTokenizer` is not yet supported by Hugging Face\'s "fast" tokenizers library. Therefore, you may experience slightly inaccurate results.')}_encode_text(e){if(null===e)return null;let[t,...s]=e.trim().split(this.languageRegex);if(0===s.length)return super._encode_text(t);if(2===s.length){let[e,t]=s;return this.supported_language_codes.includes(e)||console.warn('Unsupported language code "'.concat(e,'" detected, which may lead to unexpected behavior. Should be one of: ').concat(JSON.stringify(this.supported_language_codes))),(0,o.eG)([e],super._encode_text(t))}}},BloomTokenizer:class extends ue{constructor(e,t){var s;const n=".,!?\u2026\u3002\uff0c\u3001\u0964\u06d4\u060c",o=null===(s=e.pre_tokenizer)||void 0===s||null===(s=s.pretokenizers[0])||void 0===s?void 0:s.pattern;o&&o.Regex===" ?[^(\\s|[".concat(n,"])]+")&&(o.Regex=" ?[^\\s".concat(n,"]+")),super(e,t)}},NllbTokenizer:class extends de{constructor(e,t){super(e,t),this.languageRegex=/^[a-z]{3}_[A-Z][a-z]{3}$/,this.language_codes=this.special_tokens.filter((e=>this.languageRegex.test(e))),this.lang_to_token=e=>e}_build_translation_inputs(e,t,s){return fe(this,e,t,s)}},M2M100Tokenizer:class extends de{constructor(e,t){super(e,t),this.languageRegex=/^__[a-z]{2,3}__$/,this.language_codes=this.special_tokens.filter((e=>this.languageRegex.test(e))).map((e=>e.slice(2,-2))),this.lang_to_token=e=>"__".concat(e,"__")}_build_translation_inputs(e,t,s){return fe(this,e,t,s)}},LlamaTokenizer:me,CodeLlamaTokenizer:class extends me{},XLMRobertaTokenizer:class extends de{},MPNetTokenizer:class extends de{},FalconTokenizer:class extends de{},GPTNeoXTokenizer:class extends de{},EsmTokenizer:class extends de{},Wav2Vec2CTCTokenizer:class extends de{},BlenderbotTokenizer:xe,BlenderbotSmallTokenizer:class extends xe{},SpeechT5Tokenizer:class extends de{},NougatTokenizer:class extends de{},PreTrainedTokenizer:de});var ve=s(2146),be=s(3472),Me=s(5664);const{InferenceSession:Se,Tensor:Ae,env:Te}=Me.ONNX,Ce=0,ze=1,Pe=2,Fe=3,Ee=4,Le=new Map,Be=new Map,Ne=new Map;async function Ie(e,t,s){let n="onnx/".concat(t).concat(s.quantized?"_quantized":"",".onnx"),o=await(0,i.st)(e,n,!0,s);try{return await Se.create(o,{executionProviders:Me.p})}catch(r){if(1===Me.p.length&&"wasm"===Me.p[0])throw r;return console.warn(r),console.warn("Something went wrong during model construction (most likely a missing operation). Using `wasm` as a fallback. "),await Se.create(o,{executionProviders:["wasm"]})}}async function je(e,t){const s=function(e,t){const s=Object.create(null),n=[];for(const r of e.inputNames){const e=t[r];e instanceof a.es?s[r]=Te.wasm.proxy?e.clone():e:n.push(r)}if(n.length>0)throw new Error('An error occurred during model execution: "Missing the following inputs: '.concat(n.join(", "),"."));const o=Object.keys(t).length,i=e.inputNames.length;if(o>i){let s=Object.keys(t).filter((t=>!e.inputNames.includes(t)));console.warn("WARNING: Too many inputs were provided (".concat(o," > ").concat(i,'). The following inputs will be ignored: "').concat(s.join(", "),'".'))}return s}(e,t);try{let t=await e.run(s);return t=Oe(t),t}catch(n){throw console.error('An error occurred during model execution: "'.concat(n,'".')),console.error("Inputs given to model:",s),n}}function Oe(e){for(let t in e)e[t]instanceof Ae?e[t]=new a.es(e[t]):"object"===typeof e[t]&&Oe(e[t]);return e}function De(e){if(e instanceof a.es)return e;if(0===e.length)throw Error("items must be non-empty");if(Array.isArray(e[0])){if(e.some((t=>t.length!==e[0].length)))throw Error("Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' and/or 'truncation=True' to have batched tensors with the same length.");return new a.es("int64",BigInt64Array.from(e.flat().map((e=>BigInt(e)))),[e.length,e[0].length])}return new a.es("int64",BigInt64Array.from(e.map((e=>BigInt(e)))),[1,e.length])}function qe(e,t){var s,n;let i=null!==(s=e.config.pad_token_id)&&void 0!==s?s:null,r=null!==(n=e.config.eos_token_id)&&void 0!==n?n:null;(0,o.Wy)(r)&&(r=[r]);let l=-1!==t.indexOf(i),c=null===r||!r.includes(i);if(l&&c){let e=BigInt64Array.from(t.data.map((e=>e!=i)));return new a.es("int64",e,t.dims)}return(0,a.r6)(t)}function Ge(e,t,s){if(!e.inputNames.includes("position_ids"))return;const n=new BigInt64Array(t.attention_mask.data.length);for(let o=0;o<t.attention_mask.dims[0];++o){let e=o*t.attention_mask.dims[1],s=BigInt(0);for(let o=0;o<t.attention_mask.dims[1];++o){const i=e+o;0n===t.attention_mask.data[i]?n[i]=BigInt(1):(n[i]=s,s+=t.attention_mask.data[i])}}t.position_ids=new a.es("int64",n,t.attention_mask.dims),s&&(t.position_ids=t.position_ids.slice(null,-1).unsqueeze_(-1))}function Re(e){return new a.es("bool",[e],[1])}async function Ze(e,t){let{encoder_outputs:s,past_key_values:n}=t;s||(s=(await Ke(e,t)).last_hidden_state);let o={input_ids:t.decoder_input_ids,encoder_hidden_states:s};const i=!!n;e.decoder_merged_session.inputNames.includes("use_cache_branch")&&(o.use_cache_branch=Re(i)),e.decoder_merged_session.inputNames.includes("encoder_attention_mask")&&(o.encoder_attention_mask=t.attention_mask),Ge(e.decoder_merged_session,o,i),e.addPastKeyValues(o,n);const r=await je(e.decoder_merged_session,o);let a=r.logits;n=e.getPastKeyValues(r,n);const l=e.getAttentions(r);return new dn({logits:a,past_key_values:n,encoder_outputs:s,...l})}function We(e,t,s,n){var o,i,r,l;let c=[],d=0;const _=null===(o=e.requires_attention_mask)||void 0===o||o;let u=null!==(i=null!==(r=null!==(l=s.decoder_input_ids)&&void 0!==l?l:s.decoder_start_token_id)&&void 0!==r?r:s.bos_token_id)&&void 0!==i?i:s.eos_token_id;u instanceof a.es?u=u.tolist().flat():Array.isArray(u)||(u=[u]);for(let a of t){a.dims=[1,...a.dims];let t={inputs:a,encoder_outputs:null,prev_model_outputs:null,output_token_ids:u,done:!1,score:0,id:d++};_&&(t.attention_mask=qe(e,a)),c.push(t)}return c}async function Ue(e,t){var s;const n=e.main_input_name;let o=t.output_token_ids;t.prev_model_outputs&&(o=o.slice(-1));let i={[n]:t.inputs,decoder_input_ids:De(o),encoder_outputs:t.encoder_outputs,past_key_values:null===(s=t.prev_model_outputs)||void 0===s?void 0:s.past_key_values};t.attention_mask&&(i.attention_mask=t.attention_mask);let r=await e.forward(i);return t.prev_model_outputs=r,t.encoder_outputs=r.encoder_outputs,r}function Ve(e,t){e.output_token_ids=[...e.output_token_ids,t]}async function Ke(e,t){const s=Object.create(null);for(const n of e.session.inputNames)s[n]=t[n];return e.session.inputNames.includes("token_type_ids")&&!s.token_type_ids&&_e(s),await je(e.session,s)}async function $e(e,t){let{input_ids:s,past_key_values:n,attention_mask:o}=t,i={input_ids:s,attention_mask:null!==o&&void 0!==o?o:qe(e,s)};const r=!!n;e.session.inputNames.includes("use_cache_branch")&&(i.use_cache_branch=Re(r)),Ge(e.session,i,r),e.addPastKeyValues(i,n);let a=await je(e.session,i),l=a.logits;return n=e.getPastKeyValues(a,n),{logits:l,past_key_values:n}}function Xe(e,t,s,n,o){let i=[],r=0;for(let a of t){let t,s=a.tolist().map(Number);a.dims=[1,...a.dims],o?(t=o[r],t.dims=[1,...t.dims]):t=qe(e,a);let l={input:a,model_input_ids:a,attention_mask:t,prev_model_outputs:null,output_token_ids:s,num_output_tokens:n,done:!1,score:0,id:r++};i.push(l)}return i}async function Qe(e,t){var s;let n=new BigInt64Array(t.output_token_ids.length).fill(1n),o={input_ids:t.model_input_ids,attention_mask:new a.es("int64",n,[1,n.length]),past_key_values:null===(s=t.prev_model_outputs)||void 0===s?void 0:s.past_key_values},i=await e.forward(o);return t.prev_model_outputs=i,i}function Ye(e,t){e.output_token_ids=[...e.output_token_ids,t],e.model_input_ids=new a.es("int64",[BigInt(t)],[1,1])}class He extends o.Ag{constructor(e,t){super(),(0,n.Z)(this,"main_input_name","input_ids"),this.config=e,this.session=t;const s=Ne.get(this.constructor),o=Le.get(s);this.can_generate=!1,this._runBeam=null,this._getStartBeams=null,this._updateBeam=null,this._forward=null,o===Ee?(this.can_generate=!0,this._runBeam=Qe,this._getStartBeams=Xe,this._updateBeam=Ye,this._forward=$e):o===Pe||o===Fe?(this.can_generate=!0,this._runBeam=Ue,this._getStartBeams=We,this._updateBeam=Ve,this._forward=Ze):this._forward=Ke}async dispose(){const e=[];for(let t of Object.keys(this)){const s=this[t];s instanceof Se&&e.push(s.handler.dispose())}return await Promise.all(e)}static async from_pretrained(e){let{quantized:t=!0,progress_callback:s=null,config:n=null,cache_dir:o=null,local_files_only:r=!1,revision:a="main",model_file_name:l=null}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},c={quantized:t,progress_callback:s,config:n,cache_dir:o,local_files_only:r,revision:a,model_file_name:l};const d=Ne.get(this),_=Le.get(d);let u;var h;if(_===Ee)u=await Promise.all([ve.z.from_pretrained(e,c),Ie(e,null!==(h=c.model_file_name)&&void 0!==h?h:"decoder_model_merged",c),(0,i.yM)(e,"generation_config.json",!1,c)]);else if(_===Pe||_===Fe)u=await Promise.all([ve.z.from_pretrained(e,c),Ie(e,"encoder_model",c),Ie(e,"decoder_model_merged",c),(0,i.yM)(e,"generation_config.json",!1,c)]);else if(_===ze)u=await Promise.all([ve.z.from_pretrained(e,c),Ie(e,"encoder_model",c),Ie(e,"decoder_model_merged",c)]);else{var p;_!==Ce&&console.warn("Model type for '".concat(d,"' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose.")),u=await Promise.all([ve.z.from_pretrained(e,c),Ie(e,null!==(p=c.model_file_name)&&void 0!==p?p:"model",c)])}return new this(...u)}async _call(e){return await this.forward(e)}async forward(e){return await this._forward(this,e)}_get_logits_processor(e,t){let s=arguments.length>2&&void 0!==arguments[2]?arguments[2]:null;const n=new be.Jm;if(null!==e.repetition_penalty&&1!==e.repetition_penalty&&n.push(new be.Jj(e.repetition_penalty)),null!==e.no_repeat_ngram_size&&e.no_repeat_ngram_size>0&&n.push(new be.jF(e.no_repeat_ngram_size)),null!==e.bad_words_ids&&n.push(new be.AE(e.bad_words_ids,e.eos_token_id)),null!==e.min_length&&null!==e.eos_token_id&&e.min_length>0&&n.push(new be.ez(e.min_length,e.eos_token_id)),null!==e.min_new_tokens&&null!==e.eos_token_id&&e.min_new_tokens>0&&n.push(new be.CJ(t,e.min_new_tokens,e.eos_token_id)),null!==e.forced_bos_token_id&&n.push(new be.C9(e.forced_bos_token_id)),null!==e.forced_eos_token_id&&n.push(new be.dZ(e.max_length,e.forced_eos_token_id)),null!==e.begin_suppress_tokens){let s=t>1||null===e.forced_bos_token_id?t:t+1;null!==e.forced_decoder_ids&&(s+=e.forced_decoder_ids[e.forced_decoder_ids.length-1][0]),n.push(new be.GU(e.begin_suppress_tokens,s))}return null!==e.forced_decoder_ids&&n.push(new be.E(e.forced_decoder_ids)),null!==s&&n.extend(s),n}_get_generation_config(e){let t=new be.aP(this.config);return"generation_config"in this&&Object.assign(t,this.generation_config),null!==e&&Object.assign(t,e),t}async generate(e){var t,s,n;let i,r=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null,l=arguments.length>2&&void 0!==arguments[2]?arguments[2]:null,{inputs_attention_mask:c=null}=arguments.length>3&&void 0!==arguments[3]?arguments[3]:{};if(!this.can_generate){var d,_,u;const e=Ne.get(this.constructor);let t="The current model class (".concat(e,") is not compatible with `.generate()`, as it doesn't have a language model head.");const s=this.config.model_type,n=null!==(d=null!==(_=null!==(u=Ts.get(s))&&void 0!==u?u:As.get(s))&&void 0!==_?_:vs.get(s))&&void 0!==d?d:Ps.get(s);throw n&&(t+=" Please use the following class instead: '".concat(n[0],"'")),Error(t)}if(!(e instanceof a.es)&&!(0,o.fU)(e)&&!Array.isArray(e))throw Error('`inputs` must be a Tensor, TypedArray, or Array, but is "'.concat(e.constructor.name,'".'));if(this.config.is_encoder_decoder)i=0;else if(i=e instanceof a.es?e.dims.at(-1):e.length,0===i)throw Error("Must supply a non-empty array of input token ids.");r=this._get_generation_config(r),l=null!==(t=l)&&void 0!==t?t:new be.Jm,l=this._get_logits_processor(r,i,l);let h=r.eos_token_id;null===h||Array.isArray(h)||(h=[h]);let p=1;const m=p+(null!==(s=r.max_new_tokens)&&void 0!==s?s:1/0),f=Number.isInteger(r.max_length)&&null===(null!==(n=r.max_new_tokens)&&void 0!==n?n:null);let g=be.Z4.getSampler(r),k=this.getStartBeams(e,r,p,c);for(;k.some((e=>!e.done))&&p<m;){let e=[];for(let t of k){if(t.done){e.push(t);continue}if(f&&t.output_token_ids.length>=r.max_length){t.done=!0,e.push(t);continue}let s=await this.runBeam(t);r.output_attentions&&this.addAttentionsToBeam(t,s),r.output_scores;let n=s.logits.slice(null,-1,null);l(t.output_token_ids,n);let o=g(n);for(let[i,r]of o){let s={...t};this.updateBeam(s,i),s.score+=r,h&&h.includes(i)&&(s.done=!0),e.push(s)}}++p,e=this.groupBeams(e).map((e=>e.sort(((e,t)=>t.score-e.score)).slice(0,r.num_beams))),k=e.flat(),r.callback_function&&r.callback_function(k)}const w=this.groupBeams(k),x=e=>w.map((t=>r.num_return_sequences>1?t.slice(0,r.num_return_sequences).map((t=>t[e])):[t[0][e]])).flat(),y=x("output_token_ids");if(r.return_dict_in_generate){return{sequences:y,decoder_attentions:x("decoder_attentions"),cross_attentions:x("cross_attentions")}}return y}addAttentionsToBeam(e,t){if(this.config.is_encoder_decoder){if(!t.cross_attentions||0===t.cross_attentions.length)throw Error("`output_attentions` is true, but the model did not produce cross-attentions. This is most likely because the model was not exported with `output_attentions=True`.");e.cross_attentions||(e.cross_attentions=[]),e.cross_attentions.push(t.cross_attentions)}if(!t.decoder_attentions||0===t.decoder_attentions.length)throw Error("`output_attentions` is true, but the model did not produce decoder-attentions. This is most likely because the model was not exported with `output_attentions=True`.");e.decoder_attentions||(e.decoder_attentions=[]),e.decoder_attentions.push(t.decoder_attentions)}groupBeams(e){const t=Object.create(null);for(const s of e)void 0===t[s.id]?t[s.id]=[s]:t[s.id].push(s);return Object.values(t)}getPastKeyValues(e,t){const s=Object.create(null);for(const n in e)if(n.startsWith("present")){let o=n.replace("present","past_key_values");t&&n.includes("encoder")?s[o]=t[o]:s[o]=e[n]}return s}getAttentions(e){const t=Object.create(null);for(const s of["cross_attentions","decoder_attentions"]){const n=[];for(const t in e)if(t.startsWith(s)){n[t.split(".").pop()]=e[t]}t[s]=n}return t}addPastKeyValues(e,t){if(t)Object.assign(e,t);else{var s;const t=1;if(this.config.is_encoder_decoder&&(null===(s=this.add_encoder_pkv)||void 0===s||s)){let s=[t,this.num_encoder_heads,0,this.encoder_dim_kv],n=[t,this.num_decoder_heads,0,this.decoder_dim_kv];for(let t=0;t<this.num_decoder_layers;++t)e["past_key_values.".concat(t,".encoder.key")]=new a.es("float32",[],s),e["past_key_values.".concat(t,".encoder.value")]=new a.es("float32",[],s),e["past_key_values.".concat(t,".decoder.key")]=new a.es("float32",[],n),e["past_key_values.".concat(t,".decoder.value")]=new a.es("float32",[],n)}else if("falcon"===this.config.model_type){let s=[t*this.num_heads,0,this.dim_kv];for(let t=0;t<this.num_layers;++t)e["past_key_values.".concat(t,".key")]=new a.es("float32",[],s),e["past_key_values.".concat(t,".value")]=new a.es("float32",[],s)}else if(this.config.multi_query){let s=[t*this.num_heads,0,2*this.dim_kv];for(let t=0;t<this.num_layers;++t)e["past_key_values.".concat(t,".key_value")]=new a.es("float32",[],s)}else if("bloom"===this.config.model_type){let s=[t*this.num_heads,this.dim_kv,0],n=[t*this.num_heads,0,this.dim_kv];for(let t=0;t<this.num_layers;++t)e["past_key_values.".concat(t,".key")]=new a.es("float32",[],s),e["past_key_values.".concat(t,".value")]=new a.es("float32",[],n)}else{let s=[t,this.num_heads,0,this.dim_kv];for(let t=0;t<this.num_layers;++t)e["past_key_values.".concat(t,".key")]=new a.es("float32",[],s),e["past_key_values.".concat(t,".value")]=new a.es("float32",[],s)}}}getStartBeams(e,t,s,n){return this._getStartBeams(this,e,t,s,n)}async runBeam(e){return await this._runBeam(this,e)}updateBeam(e,t){return this._updateBeam(e,t)}}class Je{}class et extends He{}class tt extends He{}class st extends He{}class nt extends He{}class ot extends He{}class it extends He{}class rt extends He{}class at extends He{}class lt extends He{}class ct extends He{}class dt extends He{}class _t extends He{}class ut extends He{}class ht extends He{}class pt extends He{}class mt extends He{}class ft extends He{}class gt extends He{}class kt extends He{}class wt extends He{}class xt extends He{}class yt extends He{}class vt extends He{}class bt extends He{}class Mt extends He{constructor(e,t,s,o){var i;super(e,t),(0,n.Z)(this,"main_input_name","pixel_values"),this.decoder_merged_session=s,this.generation_config=o;const r=this.config.encoder,a=this.config.decoder,l=r.model_type;(null!==(i=ws.get(l))&&void 0!==i?i:xs.get(l))||console.warn("Model type for encoder '".concat(l,"' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose."));const c=Ts.get(a.model_type);if(!c)throw new Error('Unable to construct `VisionEncoderDecoder` due to unsupported decoder: "'.concat(this.config.decoder.model_type,'"'));const d=new(0,c[1])(a,s,o);this.add_encoder_pkv="num_decoder_layers"in d,this.add_encoder_pkv?(this.num_decoder_layers=d.num_decoder_layers,this.num_decoder_heads=d.num_decoder_heads,this.decoder_dim_kv=d.decoder_dim_kv,this.num_encoder_layers=d.num_encoder_layers,this.num_encoder_heads=d.num_encoder_heads,this.encoder_dim_kv=d.encoder_dim_kv):(this.num_layers=d.num_layers,this.num_heads=d.num_heads,this.dim_kv=d.dim_kv)}}class St extends He{}class At extends He{}class Tt extends He{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.n_embd/this.num_heads}}class Ct extends He{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_heads,this.num_layers=this.config.num_layers,this.dim_kv=this.config.hidden_size/this.num_heads}}class zt extends He{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_attention_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.num_heads}}class Pt extends He{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.n_embd/this.num_heads}}class Ft extends He{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.n_embd/this.num_heads}}class Et extends He{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.n_embd/this.num_heads}}class Lt extends He{constructor(e,t,s){var n;super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=null!==(n=this.config.num_key_value_heads)&&void 0!==n?n:this.config.num_attention_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.config.num_attention_heads}}class Bt extends He{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_attention_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.num_heads}}class Nt extends He{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.hidden_size/this.num_heads}}class It extends He{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_heads,this.num_layers=this.config.n_layers,this.dim_kv=this.config.d_model/this.num_heads}}class jt extends He{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_attention_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.num_heads}}class Ot extends He{}class Dt extends He{}class qt extends He{}class Gt extends He{}class Rt extends He{}class Zt extends He{}class Wt extends Je{constructor(e){let{logits:t,pred_boxes:s}=e;super(),this.logits=t,this.pred_boxes=s}}class Ut extends Je{constructor(e){let{logits:t,pred_boxes:s,pred_masks:n}=e;super(),this.logits=t,this.pred_boxes=s,this.pred_masks=n}}class Vt extends He{}class Kt extends He{}class $t extends He{}class Xt extends He{}class Qt extends He{}class Yt extends He{}class Ht extends He{}class Jt extends He{}class es extends He{}class ts extends He{}class ss extends He{}class ns extends Je{constructor(e){let{logits:t,pred_boxes:s}=e;super(),this.logits=t,this.pred_boxes=s}}class os extends He{}class is extends os{async _call(e){return new rs(await super._call(e))}}class rs extends Je{constructor(e){let{iou_scores:t,pred_masks:s}=e;super(),this.iou_scores=t,this.pred_masks=s}}class as extends He{}class ls extends He{}class cs extends He{}class ds extends He{}class _s extends He{}class us extends _s{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.hidden_size/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.hidden_size/this.num_encoder_heads}async generate_speech(e,t){let{threshold:s=.5,minlenratio:n=0,maxlenratio:o=20,vocoder:i=null}=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};const r={input_ids:e},{encoder_outputs:l,encoder_attention_mask:c}=await Ke(this,r),d=l.dims[1]/this.config.reduction_factor,_=Math.floor(d*o),u=Math.floor(d*n),h=this.config.num_mel_bins;let p=[],m=null,f=null,g=0;for(;;){++g;const e=Re(!!f);let n;n=f?f.output_sequence_out:new a.es("float32",new Float32Array(h),[1,1,h]);let o={use_cache_branch:e,output_sequence:n,encoder_attention_mask:c,speaker_embeddings:t,encoder_hidden_states:l};this.addPastKeyValues(o,m),f=await je(this.decoder_merged_session,o),m=this.getPastKeyValues(f,m);const{prob:i,spectrum:r}=f;if(p.push(r),g>=u&&(Array.from(i.data).filter((e=>e>=s)).length>0||g>=_))break}const k=(0,a.d3)(p),{waveform:w}=await je(i.session,{spectrogram:k});return{spectrogram:k,waveform:w}}}class hs extends He{constructor(){super(...arguments),(0,n.Z)(this,"main_input_name","spectrogram")}}class ps extends He{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_encoder_layers=this.num_decoder_layers=this.config.decoder_layers,this.num_encoder_heads=this.num_decoder_heads=this.config.decoder_attention_heads,this.encoder_dim_kv=this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads}}class ms extends He{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_key_value_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.config.num_attention_heads}}class fs extends He{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_attention_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.config.num_attention_heads}}class gs extends He{}class ks{static async from_pretrained(e){let{quantized:t=!0,progress_callback:s=null,config:n=null,cache_dir:o=null,local_files_only:i=!1,revision:r="main",model_file_name:a=null}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},l={quantized:t,progress_callback:s,config:n,cache_dir:o,local_files_only:i,revision:r,model_file_name:a};if(n=await ve.z.from_pretrained(e,l),l.config||(l.config=n),!this.MODEL_CLASS_MAPPINGS)throw new Error("`MODEL_CLASS_MAPPINGS` not implemented for this type of `AutoClass`: "+this.name);for(let c of this.MODEL_CLASS_MAPPINGS){const t=c.get(n.model_type);if(t)return await t[1].from_pretrained(e,l)}if(this.BASE_IF_FAIL)return console.warn('Unknown model class "'.concat(n.model_type,'", attempting to construct from base class.')),await He.from_pretrained(e,l);throw Error("Unsupported model type: ".concat(n.model_type))}}(0,n.Z)(ks,"MODEL_CLASS_MAPPINGS",null),(0,n.Z)(ks,"BASE_IF_FAIL",!1);const ws=new Map([["bert",["BertModel",class extends et{}]],["electra",["ElectraModel",class extends st{}]],["esm",["EsmModel",class extends at{}]],["convbert",["ConvBertModel",class extends tt{}]],["camembert",["CamembertModel",class extends nt{}]],["deberta",["DebertaModel",class extends ot{}]],["deberta-v2",["DebertaV2Model",class extends it{}]],["mpnet",["MPNetModel",class extends ct{}]],["albert",["AlbertModel",class extends _t{}]],["distilbert",["DistilBertModel",class extends rt{}]],["roberta",["RobertaModel",class extends wt{}]],["xlm",["XLMModel",class extends xt{}]],["xlm-roberta",["XLMRobertaModel",class extends yt{}]],["clap",["ClapModel",class extends gs{}]],["clip",["CLIPModel",class extends St{}]],["chinese_clip",["ChineseCLIPModel",class extends At{}]],["mobilebert",["MobileBertModel",class extends lt{}]],["squeezebert",["SqueezeBertModel",class extends dt{}]],["wav2vec2",["Wav2Vec2Model",class extends cs{}]],["hubert",["HubertModel",class extends cs{}]],["wavlm",["WavLMModel",class extends ds{}]],["audio-spectrogram-transformer",["ASTModel",class extends vt{}]],["detr",["DetrModel",class extends Zt{}]],["vit",["ViTModel",class extends Ot{}]],["mobilevit",["MobileViTModel",class extends qt{}]],["owlvit",["OwlViTModel",class extends Gt{}]],["beit",["BeitModel",class extends Rt{}]],["deit",["DeiTModel",class extends Vt{}]],["convnext",["ConvNextModel",class extends Jt{}]],["convnextv2",["ConvNextV2Model",class extends es{}]],["dinov2",["Dinov2Model",class extends ts{}]],["resnet",["ResNetModel",class extends Kt{}]],["swin",["SwinModel",class extends $t{}]],["swin2sr",["Swin2SRModel",class extends Xt{}]],["donut-swin",["DonutSwinModel",class extends Ht{}]],["yolos",["YolosModel",class extends ss{}]],["dpt",["DPTModel",class extends Qt{}]],["glpn",["GLPNModel",class extends Yt{}]],["hifigan",["SpeechT5HifiGan",hs]],["sam",["SamModel",is]]]),xs=new Map([["t5",["T5Model",class extends ut{}]],["longt5",["LongT5Model",class extends ht{}]],["mt5",["MT5Model",class extends pt{}]],["bart",["BartModel",class extends mt{}]],["mbart",["MBartModel",class extends ft{}]],["marian",["MarianModel",class extends as{}]],["whisper",["WhisperModel",class extends bt{}]],["m2m_100",["M2M100Model",class extends ls{}]],["blenderbot",["BlenderbotModel",class extends gt{}]],["blenderbot-small",["BlenderbotSmallModel",class extends kt{}]]]),ys=new Map([["bloom",["BloomModel",class extends Nt{}]],["gpt2",["GPT2Model",class extends Tt{}]],["gptj",["GPTJModel",class extends Pt{}]],["gpt_bigcode",["GPTBigCodeModel",class extends Ft{}]],["gpt_neo",["GPTNeoModel",class extends Ct{}]],["gpt_neox",["GPTNeoXModel",class extends zt{}]],["codegen",["CodeGenModel",class extends Et{}]],["llama",["LlamaModel",class extends Lt{}]],["phi",["PhiModel",class extends Bt{}]],["mpt",["MptModel",class extends It{}]],["opt",["OPTModel",class extends jt{}]],["mistral",["MistralModel",class extends ms{}]],["falcon",["FalconModel",class extends fs{}]]]),vs=new Map([["speecht5",["SpeechT5ForSpeechToText",class extends _s{}]],["whisper",["WhisperForConditionalGeneration",class extends bt{constructor(e,t,s,o){super(e,t),(0,n.Z)(this,"requires_attention_mask",!1),(0,n.Z)(this,"main_input_name","input_features"),this.decoder_merged_session=s,this.generation_config=o,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}async generate(e){var t,s;let n=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null,o=arguments.length>2&&void 0!==arguments[2]?arguments[2]:null;if(n=this._get_generation_config(n),null!==(s=(t=n).return_timestamps)&&void 0!==s||(t.return_timestamps=!1),n.return_timestamps&&(o=[new be.Pg(n)]),n.return_token_timestamps&&(n.output_attentions=!0,n.return_dict_in_generate=!0,"translate"===n.task&&console.warn("Token-level timestamps may not be reliable for task 'translate'."),!n.alignment_heads))throw new Error("Model generation config has no `alignment_heads`, token-level timestamps not available. See https://gist.github.com/hollance/42e32852f24243b748ae6bc1f985b13a on how to add this property to the generation config.");const i=await super.generate(e,n,o);return n.return_token_timestamps&&n.alignment_heads&&(i.token_timestamps=this._extract_token_timestamps(i,n.alignment_heads,n.num_frames)),i}_extract_token_timestamps(e,t){let s=arguments.length>2&&void 0!==arguments[2]?arguments[2]:null,n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:.02;if(!e.cross_attentions)throw new Error("Model outputs must contain cross attentions to extract timestamps. This is most likely because the model was not exported with `output_attentions=True`.");let i=this.config.median_filter_width;void 0===i&&(console.warn("Model config has no `median_filter_width`, using default value of 7."),i=7);const l=e.cross_attentions.map((e=>{let n=Array.from({length:this.config.decoder_layers},((t,s)=>(0,a.d3)(e.map((e=>e[s])),2))),o=(0,a.kn)(t.map((e=>{let[t,o]=e;return s?n[t].slice(null,o,null,[0,s]):n[t].slice(null,o)})));o=o.transpose(1,0,2,3);let[l,c]=(0,a.f3)(o,-2,0,!0),d=o.clone();for(let t=0;t<d.dims[0];++t){let e=d[t];for(let s=0;s<e.dims[0];++s){let n=e[s];const o=l[t][s][0],a=c[t][s][0];for(let e=0;e<n.dims[0];++e){let t=n[e];for(let e=0;e<t.data.length;++e)t.data[e]=(t.data[e]-a.data[e])/o.data[e];t.data.set((0,r.qC)(t.data,i))}}}return(0,a.J6)(d,1)})),c=[e.sequences.length,e.sequences[0].length],d=new a.es("float32",new Float32Array(c[0]*c[1]),c);for(let r=0;r<c[0];++r){const e=l[r].neg().squeeze_(0);let[t,s]=(0,a.Ks)(e),i=Array.from({length:t.length-1},((e,s)=>t[s+1]-t[s])),c=(0,o.eG)([1],i).map((e=>!!e)),_=[];for(let o=0;o<c.length;++o)c[o]&&_.push(s[o]*n);d[r].data.set(_,1)}return d}}]]]),bs=new Map([["speecht5",["SpeechT5ForTextToSpeech",us]]]),Ms=new Map([["bert",["BertForSequenceClassification",class extends et{async _call(e){return new _n(await super._call(e))}}]],["electra",["ElectraForSequenceClassification",class extends st{async _call(e){return new _n(await super._call(e))}}]],["esm",["EsmForSequenceClassification",class extends at{async _call(e){return new _n(await super._call(e))}}]],["convbert",["ConvBertForSequenceClassification",class extends tt{async _call(e){return new _n(await super._call(e))}}]],["camembert",["CamembertForSequenceClassification",class extends nt{async _call(e){return new _n(await super._call(e))}}]],["deberta",["DebertaForSequenceClassification",class extends ot{async _call(e){return new _n(await super._call(e))}}]],["deberta-v2",["DebertaV2ForSequenceClassification",class extends it{async _call(e){return new _n(await super._call(e))}}]],["mpnet",["MPNetForSequenceClassification",class extends ct{async _call(e){return new _n(await super._call(e))}}]],["albert",["AlbertForSequenceClassification",class extends _t{async _call(e){return new _n(await super._call(e))}}]],["distilbert",["DistilBertForSequenceClassification",class extends rt{async _call(e){return new _n(await super._call(e))}}]],["roberta",["RobertaForSequenceClassification",class extends wt{async _call(e){return new _n(await super._call(e))}}]],["xlm",["XLMForSequenceClassification",class extends xt{async _call(e){return new _n(await super._call(e))}}]],["xlm-roberta",["XLMRobertaForSequenceClassification",class extends yt{async _call(e){return new _n(await super._call(e))}}]],["bart",["BartForSequenceClassification",class extends mt{async _call(e){return new _n(await super._call(e))}}]],["mbart",["MBartForSequenceClassification",class extends ft{async _call(e){return new _n(await super._call(e))}}]],["mobilebert",["MobileBertForSequenceClassification",class extends lt{async _call(e){return new _n(await super._call(e))}}]],["squeezebert",["SqueezeBertForSequenceClassification",class extends dt{async _call(e){return new _n(await super._call(e))}}]]]),Ss=new Map([["bert",["BertForTokenClassification",class extends et{async _call(e){return new un(await super._call(e))}}]],["electra",["ElectraForTokenClassification",class extends st{async _call(e){return new un(await super._call(e))}}]],["esm",["EsmForTokenClassification",class extends at{async _call(e){return new un(await super._call(e))}}]],["convbert",["ConvBertForTokenClassification",class extends tt{async _call(e){return new un(await super._call(e))}}]],["camembert",["CamembertForTokenClassification",class extends nt{async _call(e){return new un(await super._call(e))}}]],["deberta",["DebertaForTokenClassification",class extends ot{async _call(e){return new un(await super._call(e))}}]],["deberta-v2",["DebertaV2ForTokenClassification",class extends it{async _call(e){return new un(await super._call(e))}}]],["mpnet",["MPNetForTokenClassification",class extends ct{async _call(e){return new un(await super._call(e))}}]],["distilbert",["DistilBertForTokenClassification",class extends rt{async _call(e){return new un(await super._call(e))}}]],["roberta",["RobertaForTokenClassification",class extends wt{async _call(e){return new un(await super._call(e))}}]],["xlm",["XLMForTokenClassification",class extends xt{async _call(e){return new un(await super._call(e))}}]],["xlm-roberta",["XLMRobertaForTokenClassification",class extends yt{async _call(e){return new un(await super._call(e))}}]]]),As=new Map([["t5",["T5ForConditionalGeneration",class extends ut{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.num_decoder_layers,this.num_decoder_heads=this.config.num_heads,this.decoder_dim_kv=this.config.d_kv,this.num_encoder_layers=this.config.num_layers,this.num_encoder_heads=this.config.num_heads,this.encoder_dim_kv=this.config.d_kv}}]],["longt5",["LongT5ForConditionalGeneration",class extends ht{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.num_decoder_layers,this.num_decoder_heads=this.config.num_heads,this.decoder_dim_kv=this.config.d_kv,this.num_encoder_layers=this.config.num_layers,this.num_encoder_heads=this.config.num_heads,this.encoder_dim_kv=this.config.d_kv}}]],["mt5",["MT5ForConditionalGeneration",class extends pt{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.num_decoder_layers,this.num_decoder_heads=this.config.num_heads,this.decoder_dim_kv=this.config.d_kv,this.num_encoder_layers=this.config.num_layers,this.num_encoder_heads=this.config.num_heads,this.encoder_dim_kv=this.config.d_kv}}]],["bart",["BartForConditionalGeneration",class extends mt{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["mbart",["MBartForConditionalGeneration",class extends ft{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["marian",["MarianMTModel",class extends as{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["m2m_100",["M2M100ForConditionalGeneration",class extends ls{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["blenderbot",["BlenderbotForConditionalGeneration",class extends gt{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["blenderbot-small",["BlenderbotSmallForConditionalGeneration",class extends kt{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]]]),Ts=new Map([["bloom",["BloomForCausalLM",class extends Nt{}]],["gpt2",["GPT2LMHeadModel",class extends Tt{}]],["gptj",["GPTJForCausalLM",class extends Pt{}]],["gpt_bigcode",["GPTBigCodeForCausalLM",class extends Ft{}]],["gpt_neo",["GPTNeoForCausalLM",class extends Ct{}]],["gpt_neox",["GPTNeoXForCausalLM",class extends zt{}]],["codegen",["CodeGenForCausalLM",class extends Et{}]],["llama",["LlamaForCausalLM",class extends Lt{}]],["phi",["PhiForCausalLM",class extends Bt{}]],["mpt",["MptForCausalLM",class extends It{}]],["opt",["OPTForCausalLM",class extends jt{}]],["mbart",["MBartForCausalLM",class extends ft{constructor(e,t,s){super(e,t),this.generation_config=s,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["mistral",["MistralForCausalLM",class extends ms{}]],["falcon",["FalconForCausalLM",class extends fs{}]],["trocr",["TrOCRForCausalLM",class extends ps{}]]]),Cs=new Map([["bert",["BertForMaskedLM",class extends et{async _call(e){return new hn(await super._call(e))}}]],["electra",["ElectraForMaskedLM",class extends st{async _call(e){return new hn(await super._call(e))}}]],["esm",["EsmForMaskedLM",class extends at{async _call(e){return new hn(await super._call(e))}}]],["convbert",["ConvBertForMaskedLM",class extends tt{async _call(e){return new hn(await super._call(e))}}]],["camembert",["CamembertForMaskedLM",class extends nt{async _call(e){return new hn(await super._call(e))}}]],["deberta",["DebertaForMaskedLM",class extends ot{async _call(e){return new hn(await super._call(e))}}]],["deberta-v2",["DebertaV2ForMaskedLM",class extends it{async _call(e){return new hn(await super._call(e))}}]],["mpnet",["MPNetForMaskedLM",class extends ct{async _call(e){return new hn(await super._call(e))}}]],["albert",["AlbertForMaskedLM",class extends _t{async _call(e){return new hn(await super._call(e))}}]],["distilbert",["DistilBertForMaskedLM",class extends rt{async _call(e){return new hn(await super._call(e))}}]],["roberta",["RobertaForMaskedLM",class extends wt{async _call(e){return new hn(await super._call(e))}}]],["xlm",["XLMWithLMHeadModel",class extends xt{async _call(e){return new hn(await super._call(e))}}]],["xlm-roberta",["XLMRobertaForMaskedLM",class extends yt{async _call(e){return new hn(await super._call(e))}}]],["mobilebert",["MobileBertForMaskedLM",class extends lt{async _call(e){return new hn(await super._call(e))}}]],["squeezebert",["SqueezeBertForMaskedLM",class extends dt{async _call(e){return new hn(await super._call(e))}}]]]),zs=new Map([["bert",["BertForQuestionAnswering",class extends et{async _call(e){return new pn(await super._call(e))}}]],["electra",["ElectraForQuestionAnswering",class extends st{async _call(e){return new pn(await super._call(e))}}]],["convbert",["ConvBertForQuestionAnswering",class extends tt{async _call(e){return new pn(await super._call(e))}}]],["camembert",["CamembertForQuestionAnswering",class extends nt{async _call(e){return new pn(await super._call(e))}}]],["deberta",["DebertaForQuestionAnswering",class extends ot{async _call(e){return new pn(await super._call(e))}}]],["deberta-v2",["DebertaV2ForQuestionAnswering",class extends it{async _call(e){return new pn(await super._call(e))}}]],["mpnet",["MPNetForQuestionAnswering",class extends ct{async _call(e){return new pn(await super._call(e))}}]],["albert",["AlbertForQuestionAnswering",class extends _t{async _call(e){return new pn(await super._call(e))}}]],["distilbert",["DistilBertForQuestionAnswering",class extends rt{async _call(e){return new pn(await super._call(e))}}]],["roberta",["RobertaForQuestionAnswering",class extends wt{async _call(e){return new pn(await super._call(e))}}]],["xlm",["XLMForQuestionAnswering",class extends xt{async _call(e){return new pn(await super._call(e))}}]],["xlm-roberta",["XLMRobertaForQuestionAnswering",class extends yt{async _call(e){return new pn(await super._call(e))}}]],["mobilebert",["MobileBertForQuestionAnswering",class extends lt{async _call(e){return new pn(await super._call(e))}}]],["squeezebert",["SqueezeBertForQuestionAnswering",class extends dt{async _call(e){return new pn(await super._call(e))}}]]]),Ps=new Map([["vision-encoder-decoder",["VisionEncoderDecoderModel",Mt]]]),Fs=new Map([["vision-encoder-decoder",["VisionEncoderDecoderModel",Mt]]]),Es=new Map([["vit",["ViTForImageClassification",class extends Ot{async _call(e){return new _n(await super._call(e))}}]],["mobilevit",["MobileViTForImageClassification",class extends qt{async _call(e){return new _n(await super._call(e))}}]],["beit",["BeitForImageClassification",class extends Rt{async _call(e){return new _n(await super._call(e))}}]],["deit",["DeiTForImageClassification",class extends Vt{async _call(e){return new _n(await super._call(e))}}]],["convnext",["ConvNextForImageClassification",class extends Jt{async _call(e){return new _n(await super._call(e))}}]],["convnextv2",["ConvNextV2ForImageClassification",class extends es{async _call(e){return new _n(await super._call(e))}}]],["dinov2",["Dinov2ForImageClassification",class extends ts{async _call(e){return new _n(await super._call(e))}}]],["resnet",["ResNetForImageClassification",class extends Kt{async _call(e){return new _n(await super._call(e))}}]],["swin",["SwinForImageClassification",class extends $t{async _call(e){return new _n(await super._call(e))}}]]]),Ls=new Map([["detr",["DetrForObjectDetection",class extends Zt{async _call(e){return new Wt(await super._call(e))}}]],["yolos",["YolosForObjectDetection",class extends ss{async _call(e){return new ns(await super._call(e))}}]]]),Bs=new Map([["owlvit",["OwlViTForObjectDetection",class extends Gt{}]]]),Ns=new Map([["detr",["DetrForSegmentation",class extends Zt{async _call(e){return new Ut(await super._call(e))}}]]]),Is=new Map([["sam",["SamModel",is]]]),js=new Map([["wav2vec2",["Wav2Vec2ForCTC",class extends cs{async _call(e){return new mn(await super._call(e))}}]],["wavlm",["WavLMForCTC",class extends ds{async _call(e){return new mn(await super._call(e))}}]],["hubert",["HubertForCTC",class extends cs{async _call(e){return new mn(await super._call(e))}}]]]),Os=new Map([["wav2vec2",["Wav2Vec2ForSequenceClassification",class extends cs{async _call(e){return new _n(await super._call(e))}}]],["wavlm",["WavLMForSequenceClassification",class extends ds{async _call(e){return new _n(await super._call(e))}}]],["hubert",["HubertForSequenceClassification",class extends cs{async _call(e){return new _n(await super._call(e))}}]],["audio-spectrogram-transformer",["ASTForAudioClassification",class extends vt{}]]]),Ds=new Map([["vitmatte",["VitMatteForImageMatting",class extends Dt{async _call(e){return new fn(await super._call(e))}}]]]),qs=new Map([["swin2sr",["Swin2SRForImageSuperResolution",class extends Xt{}]]]),Gs=new Map([["dpt",["DPTForDepthEstimation",class extends Qt{}]],["glpn",["GLPNForDepthEstimation",class extends Yt{}]]]),Rs=[[ws,Ce],[xs,ze],[ys,Ee],[Ms,Ce],[Ss,Ce],[As,Pe],[vs,Pe],[Ts,Ee],[Cs,Ce],[zs,Ce],[Ps,Fe],[Es,Ce],[Ns,Ce],[Ds,Ce],[qs,Ce],[Gs,Ce],[Ls,Ce],[Bs,Ce],[Is,Ce],[js,Ce],[Os,Ce],[bs,Pe]];for(const[Sn,An]of Rs)for(const[e,t]of Sn.values())Le.set(e,An),Ne.set(t,e),Be.set(e,t);const Zs=[["CLIPTextModelWithProjection",class extends St{static async from_pretrained(e){var t;let s=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return null!==(t=s.model_file_name)&&void 0!==t||(s.model_file_name="text_model"),super.from_pretrained(e,s)}},Ce],["CLIPVisionModelWithProjection",class extends St{static async from_pretrained(e){var t;let s=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return null!==(t=s.model_file_name)&&void 0!==t||(s.model_file_name="vision_model"),super.from_pretrained(e,s)}},Ce],["ClapTextModelWithProjection",class extends gs{static async from_pretrained(e){var t;let s=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return null!==(t=s.model_file_name)&&void 0!==t||(s.model_file_name="text_model"),super.from_pretrained(e,s)}},Ce],["ClapAudioModelWithProjection",class extends gs{static async from_pretrained(e){var t;let s=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return null!==(t=s.model_file_name)&&void 0!==t||(s.model_file_name="audio_model"),super.from_pretrained(e,s)}},Ce]];for(const[Sn,An,Tn]of Zs)Le.set(Sn,Tn),Ne.set(An,Sn),Be.set(Sn,An);class Ws extends ks{}(0,n.Z)(Ws,"MODEL_CLASS_MAPPINGS",[ws,xs,ys]),(0,n.Z)(Ws,"BASE_IF_FAIL",!0);class Us extends ks{}(0,n.Z)(Us,"MODEL_CLASS_MAPPINGS",[Ms]);class Vs extends ks{}(0,n.Z)(Vs,"MODEL_CLASS_MAPPINGS",[Ss]);class Ks extends ks{}(0,n.Z)(Ks,"MODEL_CLASS_MAPPINGS",[As]);class $s extends ks{}(0,n.Z)($s,"MODEL_CLASS_MAPPINGS",[vs]);class Xs extends ks{}(0,n.Z)(Xs,"MODEL_CLASS_MAPPINGS",[bs]);class Qs extends ks{}(0,n.Z)(Qs,"MODEL_CLASS_MAPPINGS",[Ts]);class Ys extends ks{}(0,n.Z)(Ys,"MODEL_CLASS_MAPPINGS",[Cs]);class Hs extends ks{}(0,n.Z)(Hs,"MODEL_CLASS_MAPPINGS",[zs]);class Js extends ks{}(0,n.Z)(Js,"MODEL_CLASS_MAPPINGS",[Ps]);class en extends ks{}(0,n.Z)(en,"MODEL_CLASS_MAPPINGS",[Es]);class tn extends ks{}(0,n.Z)(tn,"MODEL_CLASS_MAPPINGS",[Ns]);class sn extends ks{}(0,n.Z)(sn,"MODEL_CLASS_MAPPINGS",[Ls]);class nn extends ks{}(0,n.Z)(nn,"MODEL_CLASS_MAPPINGS",[Bs]);(0,n.Z)(class extends ks{},"MODEL_CLASS_MAPPINGS",[Is]);class on extends ks{}(0,n.Z)(on,"MODEL_CLASS_MAPPINGS",[js]);class rn extends ks{}(0,n.Z)(rn,"MODEL_CLASS_MAPPINGS",[Os]);class an extends ks{}(0,n.Z)(an,"MODEL_CLASS_MAPPINGS",[Fs]);(0,n.Z)(class extends ks{},"MODEL_CLASS_MAPPINGS",[Ds]);class ln extends ks{}(0,n.Z)(ln,"MODEL_CLASS_MAPPINGS",[qs]);class cn extends ks{}(0,n.Z)(cn,"MODEL_CLASS_MAPPINGS",[Gs]);class dn extends Je{constructor(e){let{logits:t,past_key_values:s,encoder_outputs:n,decoder_attentions:o=null,cross_attentions:i=null}=e;super(),this.logits=t,this.past_key_values=s,this.encoder_outputs=n,this.decoder_attentions=o,this.cross_attentions=i}}class _n extends Je{constructor(e){let{logits:t}=e;super(),this.logits=t}}class un extends Je{constructor(e){let{logits:t}=e;super(),this.logits=t}}class hn extends Je{constructor(e){let{logits:t}=e;super(),this.logits=t}}class pn extends Je{constructor(e){let{start_logits:t,end_logits:s}=e;super(),this.start_logits=t,this.end_logits=s}}class mn extends Je{constructor(e){let{logits:t}=e;super(),this.logits=t}}class fn extends Je{constructor(e){let{alphas:t}=e;super(),this.alphas=t}}var gn=s(3620),kn=s(6264),wn=s(5456);async function xn(e){return Array.isArray(e)||(e=[e]),e=await Promise.all(e.map((e=>wn.O.read(e))))}class yn extends o.Ag{constructor(e){let{task:t,model:s,tokenizer:n=null,processor:o=null}=e;super(),this.task=t,this.model=s,this.tokenizer=n,this.processor=o}async dispose(){await this.model.dispose()}async _call(e){let t=this.tokenizer(e,{padding:!0,truncation:!0});return[t,await this.model(t)]}}class vn extends yn{constructor(){super(...arguments),(0,n.Z)(this,"_key","generated_text")}async _call(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};Array.isArray(e)||(e=[e]),this.model.config.prefix&&(e=e.map((e=>this.model.config.prefix+e)));let s=this.model.config.task_specific_params;s&&s[this.task]&&s[this.task].prefix&&(e=e.map((e=>s[this.task].prefix+e)));let n,o={padding:!0,truncation:!0};n=this instanceof bn&&"_build_translation_inputs"in this.tokenizer?this.tokenizer._build_translation_inputs(e,o,t).input_ids:this.tokenizer(e,o).input_ids;let i=await this.model.generate(n,t),r=this.tokenizer.batch_decode(i,{skip_special_tokens:!0});return null!==this._key&&(r=r.map((e=>null===this._key?e:{[this._key]:e}))),r}}class bn extends vn{constructor(){super(...arguments),(0,n.Z)(this,"_key","translation_text")}}gn.Z,gn.Z,gn.Z,gn.Z,gn.Z,gn.Z,gn.Z,gn.Z,gn.Z,gn.Z,gn.Z,gn.Z,gn.Z;var Mn=s(9970)}}]);
//# sourceMappingURL=878.ce1c8986.chunk.js.map
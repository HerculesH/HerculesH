{"version":3,"file":"static/js/878.ce1c8986.chunk.js","mappings":";mHASiBA,EAAEC,OAAOC,IAAI,iBAAiBC,EAAEF,OAAOC,IAAI,gBAAgBE,EAAEH,OAAOC,IAAI,kBAAkBG,EAAEJ,OAAOC,IAAI,qBAAqBI,EAAEL,OAAOC,IAAI,kBAAkBK,EAAEN,OAAOC,IAAI,kBAAkBM,EAAEP,OAAOC,IAAI,iBAAiBO,EAAER,OAAOC,IAAI,qBAAqBQ,EAAET,OAAOC,IAAI,kBAAkBS,EAAEV,OAAOC,IAAI,cAAcU,EAAEX,OAAOC,IAAI,cAAcW,EAAEZ,OAAOa,SACzW,IAAIC,EAAE,CAACC,UAAU,WAAW,OAAM,CAAE,EAAEC,mBAAmB,WAAW,EAAEC,oBAAoB,WAAW,EAAEC,gBAAgB,WAAW,GAAGC,EAAEC,OAAOC,OAAOC,EAAE,CAAC,EAAE,SAASC,EAAEC,EAAEC,EAAEC,GAAGC,KAAKC,MAAMJ,EAAEG,KAAKE,QAAQJ,EAAEE,KAAKG,KAAKR,EAAEK,KAAKI,QAAQL,GAAGZ,CAAC,CACwI,SAASkB,IAAI,CAAyB,SAASC,EAAET,EAAEC,EAAEC,GAAGC,KAAKC,MAAMJ,EAAEG,KAAKE,QAAQJ,EAAEE,KAAKG,KAAKR,EAAEK,KAAKI,QAAQL,GAAGZ,CAAC,CADxPS,EAAEW,UAAUC,iBAAiB,CAAC,EACpQZ,EAAEW,UAAUE,SAAS,SAASZ,EAAEC,GAAG,GAAG,kBAAkBD,GAAG,oBAAoBA,GAAG,MAAMA,EAAE,MAAMa,MAAM,yHAAyHV,KAAKI,QAAQb,gBAAgBS,KAAKH,EAAEC,EAAE,WAAW,EAAEF,EAAEW,UAAUI,YAAY,SAASd,GAAGG,KAAKI,QAAQf,mBAAmBW,KAAKH,EAAE,cAAc,EAAgBQ,EAAEE,UAAUX,EAAEW,UAAsF,IAAIK,EAAEN,EAAEC,UAAU,IAAIF,EACrfO,EAAEC,YAAYP,EAAEd,EAAEoB,EAAEhB,EAAEW,WAAWK,EAAEE,sBAAqB,EAAG,IAAIC,EAAEC,MAAMC,QAAQC,EAAEzB,OAAOc,UAAUY,eAAeC,EAAE,CAACC,QAAQ,MAAMC,EAAE,CAACC,KAAI,EAAGC,KAAI,EAAGC,QAAO,EAAGC,UAAS,GACtK,SAASC,EAAE9B,EAAEC,EAAEC,GAAG,IAAI6B,EAAEC,EAAE,CAAC,EAAEC,EAAE,KAAKC,EAAE,KAAK,GAAG,MAAMjC,EAAE,IAAI8B,UAAK,IAAS9B,EAAE0B,MAAMO,EAAEjC,EAAE0B,UAAK,IAAS1B,EAAEyB,MAAMO,EAAE,GAAGhC,EAAEyB,KAAKzB,EAAEoB,EAAEc,KAAKlC,EAAE8B,KAAKN,EAAEH,eAAeS,KAAKC,EAAED,GAAG9B,EAAE8B,IAAI,IAAIK,EAAEC,UAAUC,OAAO,EAAE,GAAG,IAAIF,EAAEJ,EAAEO,SAASrC,OAAO,GAAG,EAAEkC,EAAE,CAAC,IAAI,IAAII,EAAErB,MAAMiB,GAAGK,EAAE,EAAEA,EAAEL,EAAEK,IAAID,EAAEC,GAAGJ,UAAUI,EAAE,GAAGT,EAAEO,SAASC,CAAC,CAAC,GAAGxC,GAAGA,EAAE0C,aAAa,IAAIX,KAAKK,EAAEpC,EAAE0C,kBAAe,IAASV,EAAED,KAAKC,EAAED,GAAGK,EAAEL,IAAI,MAAM,CAACY,SAASpE,EAAEqE,KAAK5C,EAAE0B,IAAIO,EAAEN,IAAIO,EAAE9B,MAAM4B,EAAEa,OAAOtB,EAAEC,QAAQ,CAChV,SAASsB,EAAE9C,GAAG,MAAM,kBAAkBA,GAAG,OAAOA,GAAGA,EAAE2C,WAAWpE,CAAC,CAAoG,IAAIwE,EAAE,OAAO,SAASC,EAAEhD,EAAEC,GAAG,MAAM,kBAAkBD,GAAG,OAAOA,GAAG,MAAMA,EAAE0B,IAA7K,SAAgB1B,GAAG,IAAIC,EAAE,CAAC,IAAI,KAAK,IAAI,MAAM,MAAM,IAAID,EAAEiD,QAAQ,SAAQ,SAASjD,GAAG,OAAOC,EAAED,EAAE,GAAE,CAA+EkD,CAAO,GAAGlD,EAAE0B,KAAKzB,EAAEkD,SAAS,GAAG,CAC/W,SAASC,EAAEpD,EAAEC,EAAEC,EAAE6B,EAAEC,GAAG,IAAIC,SAASjC,EAAK,cAAciC,GAAG,YAAYA,IAAEjC,EAAE,MAAK,IAAIkC,GAAE,EAAG,GAAG,OAAOlC,EAAEkC,GAAE,OAAQ,OAAOD,GAAG,IAAK,SAAS,IAAK,SAASC,GAAE,EAAG,MAAM,IAAK,SAAS,OAAOlC,EAAE2C,UAAU,KAAKpE,EAAE,KAAKG,EAAEwD,GAAE,GAAI,GAAGA,EAAE,OAAWF,EAAEA,EAANE,EAAElC,GAASA,EAAE,KAAK+B,EAAE,IAAIiB,EAAEd,EAAE,GAAGH,EAAEb,EAAEc,IAAI9B,EAAE,GAAG,MAAMF,IAAIE,EAAEF,EAAEiD,QAAQF,EAAE,OAAO,KAAKK,EAAEpB,EAAE/B,EAAEC,EAAE,IAAG,SAASF,GAAG,OAAOA,CAAC,KAAI,MAAMgC,IAAIc,EAAEd,KAAKA,EADnW,SAAWhC,EAAEC,GAAG,MAAM,CAAC0C,SAASpE,EAAEqE,KAAK5C,EAAE4C,KAAKlB,IAAIzB,EAAE0B,IAAI3B,EAAE2B,IAAIvB,MAAMJ,EAAEI,MAAMyC,OAAO7C,EAAE6C,OAAO,CACyQQ,CAAErB,EAAE9B,IAAI8B,EAAEN,KAAKQ,GAAGA,EAAER,MAAMM,EAAEN,IAAI,IAAI,GAAGM,EAAEN,KAAKuB,QAAQF,EAAE,OAAO,KAAK/C,IAAIC,EAAEqD,KAAKtB,IAAI,EAAyB,GAAvBE,EAAE,EAAEH,EAAE,KAAKA,EAAE,IAAIA,EAAE,IAAOb,EAAElB,GAAG,IAAI,IAAIoC,EAAE,EAAEA,EAAEpC,EAAEsC,OAAOF,IAAI,CAC/e,IAAII,EAAET,EAAEiB,EADwef,EACrfjC,EAAEoC,GAAeA,GAAGF,GAAGkB,EAAEnB,EAAEhC,EAAEC,EAAEsC,EAAER,EAAE,MAAM,GAAGQ,EAPsU,SAAWxC,GAAG,OAAG,OAAOA,GAAG,kBAAkBA,EAAS,KAAsC,oBAAjCA,EAAEZ,GAAGY,EAAEZ,IAAIY,EAAE,eAA0CA,EAAE,IAAI,CAO5buD,CAAEvD,GAAG,oBAAoBwC,EAAE,IAAIxC,EAAEwC,EAAEL,KAAKnC,GAAGoC,EAAE,IAAIH,EAAEjC,EAAEwD,QAAQC,MAA6BvB,GAAGkB,EAA1BnB,EAAEA,EAAEyB,MAA0BzD,EAAEC,EAAtBsC,EAAET,EAAEiB,EAAEf,EAAEG,KAAkBJ,QAAQ,GAAG,WAAWC,EAAE,MAAMhC,EAAE0D,OAAO3D,GAAGa,MAAM,mDAAmD,oBAAoBZ,EAAE,qBAAqBL,OAAOgE,KAAK5D,GAAG6D,KAAK,MAAM,IAAI5D,GAAG,6EAA6E,OAAOiC,CAAC,CACzZ,SAAS4B,EAAE9D,EAAEC,EAAEC,GAAG,GAAG,MAAMF,EAAE,OAAOA,EAAE,IAAI+B,EAAE,GAAGC,EAAE,EAAmD,OAAjDoB,EAAEpD,EAAE+B,EAAE,GAAG,IAAG,SAAS/B,GAAG,OAAOC,EAAEkC,KAAKjC,EAAEF,EAAEgC,IAAI,IAAUD,CAAC,CAAC,SAASgC,EAAE/D,GAAG,IAAI,IAAIA,EAAEgE,QAAQ,CAAC,IAAI/D,EAAED,EAAEiE,SAAQhE,EAAEA,KAAMiE,MAAK,SAASjE,GAAM,IAAID,EAAEgE,UAAU,IAAIhE,EAAEgE,UAAQhE,EAAEgE,QAAQ,EAAEhE,EAAEiE,QAAQhE,EAAC,IAAE,SAASA,GAAM,IAAID,EAAEgE,UAAU,IAAIhE,EAAEgE,UAAQhE,EAAEgE,QAAQ,EAAEhE,EAAEiE,QAAQhE,EAAC,KAAI,IAAID,EAAEgE,UAAUhE,EAAEgE,QAAQ,EAAEhE,EAAEiE,QAAQhE,EAAE,CAAC,GAAG,IAAID,EAAEgE,QAAQ,OAAOhE,EAAEiE,QAAQE,QAAQ,MAAMnE,EAAEiE,OAAQ,CAC5Z,IAAIG,EAAE,CAAC5C,QAAQ,MAAM6C,EAAE,CAACC,WAAW,MAAMC,EAAE,CAACC,uBAAuBJ,EAAEK,wBAAwBJ,EAAEK,kBAAkBnD,GAA6UoD,EAAQC,UAAU7E,kBCf9c8E,EAAOF,QAAU,EAAjBE,iLCuDFC,eAAeC,EAAcC,EAA+BC,GAExD,IAAIC,QAAaC,QAAQC,IAAI,EACzBC,EAAAA,EAAAA,IAAaL,EAA+B,kBAAkB,EAAMC,IACpEI,EAAAA,EAAAA,IAAaL,EAA+B,yBAAyB,EAAMC,KAO/E,OAHuB,OAAnBA,EAAQK,SACRJ,EAAK,GAAGI,OAASL,EAAQK,QAEtBJ,CACX,CAqCA,SAASK,EAAcC,GAAwB,IAAfC,IAAMpD,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,KAAAA,UAAA,GAElC,QAAsBqD,IAAlBF,EAAQG,MAAqB,CAM7B,MAAMC,EAAQJ,EAAQG,MAAM1C,QAAQ,aAAc,MAClD,OAAO,IAAI4C,OAAOD,EAAO,KAE7B,CAAO,QAAuBF,IAAnBF,EAAQ7B,OAAsB,CACrC,MAAMmC,GAAUC,EAAAA,EAAAA,IAAaP,EAAQ7B,QAErC,OAAO,IAAIkC,OAAOJ,EAASK,EAAU,IAAHE,OAAOF,EAAO,KAAK,KAEzD,CAEI,OADAG,QAAQC,KAAK,wBAAyBV,GAC/B,IAEf,CAOA,SAASW,EAAYC,GACjB,OAAO,IAAIC,IAAIzG,OAAO0G,QAAQF,GAClC,CAOA,SAASG,EAAuBC,GAC5B,MAAMC,EAAOD,EAAOC,KACpB,OAAQA,EAAKnE,QACT,KAAK,EACD,OAAOkE,EAAOE,SAClB,KAAK,EACD,GAAgB,IAAZD,EAAK,GACL,MAAM,IAAI5F,MAAM,0GAEpB,OAAO2F,EAAOE,SAAS,GAC3B,QACI,MAAM,IAAI7F,MAAM,+CAADmF,OAAgDS,EAAKnE,OAAM,MAEtF,CAOA,SAASqE,EAAsBC,GAG3B,OAAOA,EAAK3D,QAAQ,OAAQ,KACvBA,QAAQ,OAAQ,KAChBA,QAAQ,OAAQ,KAChBA,QAAQ,MAAO,KACfA,QAAQ,QAAS,KACjBA,QAAQ,SAAU,OAClBA,QAAQ,QAAS,MACjBA,QAAQ,QAAS,MACjBA,QAAQ,SAAU,OAClBA,QAAQ,SAAU,MAC3B,CAOA,SAAS4D,EAAeD,GACpB,OAAOA,EAAK3D,QAAQ,mBAAoB,GAC5C,CA2CA,MAAM6D,EAAoB,qEAQ1B,MAAMC,EAYF/F,WAAAA,CAAYgG,GAAQ,IAAAC,EAAAC,EAAAC,EAAAC,EAAAC,EAChBlH,KAAKmH,QAAUN,EAAOM,QACtBnH,KAAKoH,GAAKP,EAAOO,GACjBpH,KAAKqH,YAAgC,QAArBP,EAAGD,EAAOQ,mBAAW,IAAAP,GAAAA,EACrC9G,KAAKsH,OAAsB,QAAhBP,EAAGF,EAAOS,cAAM,IAAAP,GAAAA,EAC3B/G,KAAKuH,OAAsB,QAAhBP,EAAGH,EAAOU,cAAM,IAAAP,GAAAA,EAC3BhH,KAAKwH,QAAwB,QAAjBP,EAAGJ,EAAOW,eAAO,IAAAP,GAAAA,EAC7BjH,KAAKyH,WAA8B,QAApBP,EAAGL,EAAOY,kBAAU,IAAAP,EAAAA,EAAI,IAC3C,EAQG,MAAMQ,UAAuBC,EAAAA,GAKhC9G,WAAAA,CAAYgG,GAAQ,IAAAe,EAChBC,QACA7H,KAAK6G,OAASA,EAGd7G,KAAK8H,MAAQ,GAMb9H,KAAK+H,cAAgB,IAAI7B,IAEzBlG,KAAKgI,kBAAezC,EACpBvF,KAAKiI,eAAY1C,EACjBvF,KAAKkI,wBAAqB3C,EAG1BvF,KAAKmI,SAA+B,QAAvBP,EAAG5H,KAAK6G,OAAOsB,gBAAQ,IAAAP,GAAAA,CACxC,CASA,iBAAOQ,CAAWvB,GAAiB,QAAAwB,EAAAnG,UAAAC,OAANmG,EAAI,IAAAtH,MAAAqH,EAAA,EAAAA,EAAA,KAAAE,EAAA,EAAAA,EAAAF,EAAAE,IAAJD,EAAIC,EAAA,GAAArG,UAAAqG,GAC7B,OAAQ1B,EAAOpE,MACX,IAAK,YACD,OAAO,IAAI+F,EAAmB3B,GAClC,IAAK,UAED,OAAO,IAAI4B,EAAQ5B,KAAWyB,GAElC,IAAK,MACD,OAAO,IAAII,EAAI7B,GAEnB,QACI,GAAIA,EAAOiB,MAEP,OAAO,IAAIa,EAAqB9B,KAAWyB,GAE/C,MAAM,IAAI5H,MAAM,gCAADmF,OAAiCgB,EAAOpE,OAEnE,CAOAmG,KAAAA,CAAMC,GACF,OAAO7I,KAAK8I,OAAOD,EACvB,CAQAC,MAAAA,CAAOD,GACH,MAAMnI,MAAM,4CAChB,CAOAqI,qBAAAA,CAAsBF,GAClB,IAAIG,EAAMH,EAAOI,KAAItK,IAAC,IAAAuK,EAAA,OAA6B,QAA7BA,EAAIlJ,KAAK+H,cAAcoB,IAAIxK,UAAE,IAAAuK,EAAAA,EAAIlJ,KAAKgI,YAAY,IAMxE,OAJIhI,KAAKmI,WAELa,EAjJZ,SAAcI,EAAK7F,GACf,IAAI8F,EAAQ,GACRC,EAAI,EACR,KAAOA,EAAIF,EAAIjH,QAEX,GADAkH,EAAMlG,KAAKiG,EAAIE,IACXF,EAAIE,KAAO/F,EAKf,KAAO+F,EAAIF,EAAIjH,QAAUiH,EAAIE,KAAO/F,KAC9B+F,QALAA,EASV,OAAOD,CACX,CAiIkBE,CAAKP,EAAKhJ,KAAKgI,eAElBgB,CACX,CAOAQ,qBAAAA,CAAsBR,GAClB,OAAOA,EAAIC,KAAIK,IAAC,IAAAG,EAAA,OAAiB,QAAjBA,EAAIzJ,KAAK8H,MAAMwB,UAAE,IAAAG,EAAAA,EAAIzJ,KAAKiI,SAAS,GACvD,EAOJ,MAAMO,UAA2Bd,EAQ7B7G,WAAAA,CAAYgG,GAAQ,IAAA6C,EAChB7B,MAAMhB,GAKN7G,KAAK+H,cAAgB/B,EAAYa,EAAOiB,OAMxC9H,KAAKgI,aAAehI,KAAK+H,cAAcoB,IAAItC,EAAOoB,WAMlDjI,KAAKiI,UAAYpB,EAAOoB,UAMxBjI,KAAK2J,yBAA0D,QAAlCD,EAAG7C,EAAO8C,gCAAwB,IAAAD,EAAAA,EAAI,IAMnE1J,KAAK8H,MAAQ,IAAI9G,MAAMhB,KAAK+H,cAAc6B,MAC1C,IAAK,MAAOrI,EAAKgC,KAAUvD,KAAK+H,cAC5B/H,KAAK8H,MAAMvE,GAAShC,CAE5B,CAOAuH,MAAAA,CAAOD,GACH,IAAIgB,EAAe,GACnB,IAAK,IAAIC,KAASjB,EAAQ,CACtB,IAAIkB,EAAQ,IAAID,GAChB,GAAIC,EAAM5H,OAASnC,KAAK2J,yBAA0B,CAC9CE,EAAa1G,KAAKnD,KAAKiI,WACvB,QACJ,CAEA,IAAI+B,GAAY,EACZC,EAAQ,EACRC,EAAY,GAEhB,KAAOD,EAAQF,EAAM5H,QAAQ,CACzB,IAAIgI,EAAMJ,EAAM5H,OACZiI,EAAmB,KACvB,KAAOH,EAAQE,GAAK,CAChB,IAAIE,EAASN,EAAMO,MAAML,EAAOE,GAAKzG,KAAK,IAK1C,GAHIuG,EAAQ,IACRI,EAASrK,KAAK6G,OAAO0D,0BAA4BF,GAEjDrK,KAAK+H,cAAcyC,IAAIH,GAAS,CAChCD,EAAmBC,EACnB,KACJ,GAEEF,CACN,CACA,GAAyB,OAArBC,EAA2B,CAC3BJ,GAAY,EACZ,KACJ,CACAE,EAAU/G,KAAKiH,GACfH,EAAQE,CACZ,CACIH,EACAH,EAAa1G,KAAKnD,KAAKiI,WAEvB4B,EAAa1G,QAAQ+G,EAE7B,CAEA,OAAOL,CACX,EAQJ,MAAMpB,UAAgBf,EAQlB7G,WAAAA,CAAYgG,EAAQ4D,GAChB5C,MAAMhB,GAEN,MAAM6D,EAAY7D,EAAOiB,MAAM3F,OAC/BnC,KAAK8H,MAAQ,IAAI9G,MAAM0J,GACvB1K,KAAK2K,OAAS,IAAI3J,MAAM0J,GACxB,IAAK,IAAIpB,EAAI,EAAGA,EAAIoB,IAAapB,EAAG,CAChC,MAAMsB,EAAQ/D,EAAOiB,MAAMwB,GAC3BtJ,KAAK8H,MAAMwB,GAAKsB,EAAM,GACtB5K,KAAK2K,OAAOrB,GAAKsB,EAAM,EAC3B,CAEA5K,KAAKgI,aAAenB,EAAOgE,OAC3B7K,KAAKiI,UAAYjI,KAAK8H,MAAMjB,EAAOgE,QAEnC7K,KAAK+H,cAAgB,IAAI7B,IAAIlG,KAAK8H,MAAMmB,KAAI,CAAClK,EAAGuK,IAAM,CAACvK,EAAGuK,MAC1DtJ,KAAK8K,SAAW,IAEhB9K,KAAK+K,WAAa/K,KAAK+H,cAAcoB,IAAInJ,KAAK8K,UAC9C9K,KAAKgL,SAAWP,EAAWQ,UAE3BjL,KAAKkL,WAAalL,KAAK+H,cAAcoB,IAAInJ,KAAKgL,UAC9ChL,KAAKmL,SAAWnL,KAAK8H,MAAM9H,KAAKgI,cAEhChI,KAAKoL,UAAWC,EAAAA,EAAAA,IAAIrL,KAAK2K,QAAQ,GAEjC3K,KAAKsL,SAAWtL,KAAKoL,SAAW,GAChCpL,KAAK2K,OAAO3K,KAAKgI,cAAgBhI,KAAKsL,SAEtCtL,KAAKuL,KAAO,IAAIC,EAAAA,GAChBxL,KAAKuL,KAAKE,OAAOzL,KAAK8H,OAItB9H,KAAKmI,UAAW,CACpB,CAMAuD,aAAAA,CAAcC,GACV,MAAMC,EAAWD,EAAQC,SACnBC,EAAMD,EAASzJ,OACrB,IAAI2J,EAAW,EACf,KAAOA,EAAWD,GAAK,CACnB,MAAME,EAAQ,EACd,IAAIC,GAAgB,EACpB,MAAMnD,EAAS,GAEf,IAAK,IAAIiB,KAAS9J,KAAKuL,KAAKU,mBAAmBL,EAAStB,MAAMwB,IAAY,CACtEjD,EAAO1F,KAAK2G,GACZ,MAAMoC,EAAUlM,KAAK+H,cAAcoB,IAAIW,GACjCqC,EAAanM,KAAK2K,OAAOuB,GACzB3N,EAAIuL,EAAM3H,OAChBwJ,EAAQS,OAAON,EAAUvN,EAAG4N,EAAYD,GACnCF,GAAiBzN,IAAMwN,IACxBC,GAAgB,EAExB,CACKA,GACDL,EAAQS,OAAON,EAAUC,EAAO/L,KAAKsL,SAAUtL,KAAKgI,cAExD8D,GAAYC,CAChB,CACJ,CAQAM,QAAAA,CAAS5E,GACL,MAAMkE,EAAU,IAAIW,EAAAA,GAAa7E,EAAYzH,KAAK+K,WAAY/K,KAAKkL,YAEnE,OADAlL,KAAK0L,cAAcC,GACZA,EAAQ9C,QACnB,CAOAC,MAAAA,CAAOD,GACH,IAAI0D,EAAW,GACf,IAAK,IAAIzC,KAASjB,EAAQ,CACtB,MAAM2D,EAAYxM,KAAKqM,SAASvC,GAChCyC,EAASpJ,QAAQqJ,EACrB,CACA,OAAOD,CACX,EASJ,MAAME,EAAmB,MAKrB,MAAMC,EAAK,IACJ1L,MAAM2L,KAAK,CAAExK,OAAQ,IAAIyK,WAAW,GAAK,IAAIA,WAAW,GAAK,IAAK,CAACC,EAAGvD,IAAMA,EAAI,IAAIsD,WAAW,QAC/F5L,MAAM2L,KAAK,CAAExK,OAAQ,OAAIyK,WAAW,GAAK,OAAIA,WAAW,GAAK,IAAK,CAACC,EAAGvD,IAAMA,EAAI,OAAIsD,WAAW,QAC/F5L,MAAM2L,KAAK,CAAExK,OAAQ,OAAIyK,WAAW,GAAK,OAAIA,WAAW,GAAK,IAAK,CAACC,EAAGvD,IAAMA,EAAI,OAAIsD,WAAW,MAEtG,IAAIE,EAAKJ,EAAGpC,QACR/L,EAAI,EACR,IAAK,IAAIuB,EAAI,EAAGA,EAAI,MAAOA,EAClB4M,EAAGK,SAASjN,KACb4M,EAAGvJ,KAAKrD,GACRgN,EAAG3J,KAAK,IAAM5E,GACdA,GAAK,GAGb,IAAIyO,EAAMF,EAAG7D,KAAI1K,GAAKiF,OAAOyJ,aAAa1O,KAC1C,OAAOkB,OAAOyN,YAAYR,EAAGzD,KAAI,CAACnJ,EAAGwJ,IAAM,CAACxJ,EAAGkN,EAAI1D,MACtD,EArBwB,GAuBnB6D,GAAmBC,EAAAA,EAAAA,IAAkBX,GAgB3C,MAAM/D,UAAYhB,EAUd7G,WAAAA,CAAYgG,GAAQ,IAAAwG,EAAAC,EAChBzF,MAAMhB,GAEN7G,KAAKuN,gBAAkB,IAGvBvN,KAAK+H,cAAgB/B,EAAYa,EAAOiB,OAExC9H,KAAKgI,aAAehI,KAAK+H,cAAcoB,IAAItC,EAAOoB,WAClDjI,KAAKiI,UAAYpB,EAAOoB,UAExBjI,KAAK8H,MAAQ,IAAI9G,MAAMhB,KAAK+H,cAAc6B,MAC1C,IAAK,MAAOrI,EAAKgC,KAAUvD,KAAK+H,cAC5B/H,KAAK8H,MAAMvE,GAAShC,EAGxBvB,KAAKwN,UAAY,IAAItH,IAAIW,EAAO4G,OAAOxE,KAAI,CAAClK,EAAGuK,IAAM,CAACvK,EAAGuK,MACzDtJ,KAAKyN,OAAS5G,EAAO4G,OAAOxE,KAAIlK,GAAKA,EAAE2O,MAAM1N,KAAKuN,mBAElDvN,KAAKkI,mBAAqBrB,EAAOqB,mBAGjClI,KAAK2N,0BAA4D,QAAnCN,EAAGxG,EAAO8G,iCAAyB,IAAAN,EAAAA,EAAI,KAErErN,KAAK4N,cAAyC,QAA5BN,EAAGtN,KAAK6G,OAAO+G,qBAAa,IAAAN,GAAAA,EAE1CtN,KAAK4N,gBACL5N,KAAK6N,aAAe,IAAIC,aAI5B9N,KAAK+N,MAAQ,IAAI7H,GACrB,CAQA8H,GAAAA,CAAIlE,GACA,GAAqB,IAAjBA,EAAM3H,OACN,MAAO,GAGX,MAAM8L,EAASjO,KAAK+N,MAAM5E,IAAIW,GAC9B,QAAevE,IAAX0I,EACA,OAAOA,EAGX,MAAMC,EAAOlN,MAAM2L,KAAK7C,GACpB9J,KAAKkI,qBACLgG,EAAKA,EAAK/L,OAAS,IAAMnC,KAAKkI,oBAGlC,IAAIiG,EAAS,GACb,GAAID,EAAK/L,OAAS,EAAG,CAGjB,MAAMiM,EAAQ,IAAIC,EAAAA,IAAc,CAACxO,EAAGC,IAAMD,EAAEyO,MAAQxO,EAAEwO,QAKtD,IAAIC,EAAe,CACfzE,MAAOoE,EAAK,GACZM,KAAM,EACNC,KAAM,KACNpL,KAAM,MAGNqL,EAAeH,EACnB,IAAK,IAAIjF,EAAI,EAAGA,EAAI4E,EAAK/L,SAAUmH,EAAG,CAClC,MAAMqF,EAAc,CAChBH,KAAMlF,EAAI4E,EAAK/L,OACf2H,MAAOoE,EAAK5E,GACZmF,KAAMC,EACNrL,KAAM,MAEVqL,EAAarL,KAAOsL,EACpB3O,KAAK4O,UAAUR,EAAOM,GACtBA,EAAeC,CACnB,CAEA,MAAQP,EAAMS,WAAW,CAErB,MAAMC,EAAOV,EAAMW,MAGnB,GAAID,EAAKE,UAAYF,EAAKzL,MAAQyL,EAAKzL,KAAK2L,QAAS,SAQrD,GAJAF,EAAKE,SAAU,EACfF,EAAKzL,KAAK2L,SAAU,EAGhBF,EAAKL,KAAM,CAGX,MAAMQ,EAAkB,IAAKH,EAAKL,MAIlCK,EAAKL,KAAKO,SAAU,EACpBF,EAAKL,KAAOQ,EAGRA,EAAgBR,KAChBQ,EAAgBR,KAAKpL,KAAO4L,EAI5BV,EAAeU,CAEvB,CAGA,MAAMC,EAAS,CACXpF,MAAOgF,EAAKhF,MAAQgF,EAAKzL,KAAKyG,MAC9B0E,KAAMM,EAAKN,KACXC,KAAMK,EAAKL,KACXpL,KAAMyL,EAAKzL,KAAKA,MAKhB6L,EAAOT,MACPS,EAAOT,KAAKpL,KAAO6L,EACnBlP,KAAK4O,UAAUR,EAAOc,EAAOT,OAG7BF,EAAeW,EAIfA,EAAO7L,OACP6L,EAAO7L,KAAKoL,KAAOS,EACnBlP,KAAK4O,UAAUR,EAAOc,GAE9B,CAGA,IAAK,IAAIP,EAAcJ,EAA8B,OAAhBI,EAAsBA,EAAcA,EAAYtL,KACjF8K,EAAOhL,KAAKwL,EAAY7E,MAEhC,MACIqE,EAASD,EAIb,GAAIlO,KAAK2N,0BAEL,IAAK,IAAIrE,EAAI,EAAGA,EAAI6E,EAAOhM,OAAS,IAAKmH,EACrC6E,EAAO7E,IAAMtJ,KAAK2N,0BAO1B,OAFA3N,KAAK+N,MAAMoB,IAAIrF,EAAOqE,GAEfA,CACX,CASAS,SAAAA,CAAUR,EAAOU,GAIb,MAAMM,EAAOpP,KAAKwN,UAAUrE,IAAI2F,EAAKhF,MAAQ9J,KAAKuN,gBAAkBuB,EAAKzL,KAAKyG,YACjEvE,IAAT6J,IACAN,EAAKR,MAAQc,EAAON,EAAKN,KACzBJ,EAAMjL,KAAK2L,GAEnB,CAOAhG,MAAAA,CAAOD,GACH,IAAIgB,EAAe,GAEnB,IAAK,IAAIC,KAASjB,EAAQ,CACtB,IAAIwG,EAAiBrP,KAAKgO,IAAIlE,GAE9B,IAAK,IAAInL,KAAK0Q,EACNrP,KAAK+H,cAAcyC,IAAI7L,GACvBkL,EAAa1G,KAAKxE,GAEdqB,KAAK4N,cACL/D,EAAa1G,QACNnC,MAAM2L,KAAK3M,KAAK6N,aAAa/E,OAAOnK,IAClCsK,KAAIlK,GAAK,MAAJ8G,OAAU9G,EAAEiE,SAAS,IAAIsM,cAAcC,SAAS,EAAG,KAAI,QAGrE1F,EAAa1G,KAAKnD,KAAKiI,UAIvC,CAEA,OAAO4B,CACX,EAOJ,MAAMlB,UAA6BjB,EAO/B7G,WAAAA,CAAYgG,EAAQ4D,GAChB5C,MAAMhB,GAGN7G,KAAK+H,cAAgB/B,EACjByE,EAAW+E,YACL3I,EAAOiB,MAAM2C,EAAW+E,aACxB3I,EAAOiB,OAGjB9H,KAAKyP,UAAYhF,EAAWgF,UAC5BzP,KAAK0P,aAAe1P,KAAK+H,cAAcoB,IAAInJ,KAAKyP,WAEhDzP,KAAKiL,UAAYR,EAAWQ,UAC5BjL,KAAK2P,aAAe3P,KAAK+H,cAAcoB,IAAInJ,KAAKiL,WAEhDjL,KAAK4P,UAAYnF,EAAWmF,UAC5B5P,KAAK6P,aAAe7P,KAAK+H,cAAcoB,IAAInJ,KAAK4P,WAEhD5P,KAAKiI,UAAYwC,EAAWxC,UAC5BjI,KAAKgI,aAAehI,KAAK+H,cAAcoB,IAAInJ,KAAKiI,WAEhDjI,KAAK8H,MAAQ,IAAI9G,MAAMhB,KAAK+H,cAAc6B,MAC1C,IAAK,MAAOrI,EAAKgC,KAAUvD,KAAK+H,cAC5B/H,KAAK8H,MAAMvE,GAAShC,CAE5B,CAEAuH,MAAAA,CAAOD,GACH,OAAOA,CACX,EAQJ,MAAMiH,UAAmBnI,EAAAA,GAIrB9G,WAAAA,CAAYgG,GACRgB,QACA7H,KAAK6G,OAASA,CAClB,CASA,iBAAOuB,CAAWvB,GACd,GAAe,OAAXA,EAAiB,OAAO,KAC5B,OAAQA,EAAOpE,MACX,IAAK,iBACD,OAAO,IAAIsN,EAAelJ,GAC9B,IAAK,cACD,OAAO,IAAImJ,GAAYnJ,GAC3B,IAAK,WACD,OAAO,IAAIoJ,EAAmBpJ,GAClC,IAAK,UACD,OAAO,IAAIqJ,EAAQrJ,GACvB,IAAK,MACD,OAAO,IAAIsJ,EAAItJ,GACnB,IAAK,OACD,OAAO,IAAIuJ,EAAKvJ,GACpB,IAAK,OACD,OAAO,IAAIwJ,EAAKxJ,GACpB,IAAK,QACD,OAAO,IAAIyJ,EAAgBzJ,GAC/B,IAAK,eACD,OAAO,IAAI0J,EAAa1J,GAC5B,IAAK,YACD,OAAO,IAAI2J,EAAU3J,GACzB,IAAK,UACD,OAAO,IAAI4J,EAAQ5J,GACvB,QACI,MAAM,IAAInG,MAAM,4BAADmF,OAA6BgB,EAAOpE,OAE/D,CASAiO,SAAAA,CAAUjK,GACN,MAAM/F,MAAM,+CAChB,CAOAkI,KAAAA,CAAMnC,GACF,OAAOzG,KAAK0Q,UAAUjK,EAC1B,EAQJ,MAAMyJ,UAAgBJ,EAMlBY,SAAAA,CAAUjK,GACN,IAAIpB,EAAUD,EAAcpF,KAAK6G,OAAOxB,SACxC,OAAgB,OAAZA,EACOoB,EAGXA,EAAOA,EAAKkK,WAAWtL,EAASrF,KAAK6G,OAAOM,QAGhD,EAOJ,MAAMgJ,UAAYL,EAMdY,SAAAA,CAAUjK,GAEN,OADAA,EAAOA,EAAKiK,UAAU,MAE1B,EAOJ,MAAMN,UAAaN,EAMfY,SAAAA,CAAUjK,GAEN,OADAA,EAAOA,EAAKiK,UAAU,OAE1B,EAMJ,MAAML,UAAaP,EAMfY,SAAAA,CAAUjK,GAEN,OADAA,EAAOA,EAAKiK,UAAU,OAE1B,EAMJ,MAAMJ,UAAwBR,EAM1BY,SAAAA,CAAUjK,GAYN,OAXIzG,KAAK6G,OAAO+J,YAAc5Q,KAAK6G,OAAOgK,YAEtCpK,EAAOA,EAAKqK,QAER9Q,KAAK6G,OAAO+J,aACZnK,EAAOA,EAAKsK,aAEZ/Q,KAAK6G,OAAOgK,cACZpK,EAAOA,EAAKuK,YAGbvK,CACX,EAOJ,MAAM8J,UAAqBT,EAMvBY,SAAAA,CAAUjK,GAEN,OADAA,EAAOC,EAAeD,EAE1B,EAOJ,MAAM+J,UAAkBV,EAMpBY,SAAAA,CAAUjK,GAEN,OADAA,EAAOA,EAAKwK,aAEhB,EAOJ,MAAMR,UAAgBX,EAMlBY,SAAAA,CAAUjK,GAEN,OADAA,EAAOzG,KAAK6G,OAAOqK,QAAUzK,CAEjC,EAOJ,MAAMwJ,UAA2BH,EAM7BjP,WAAAA,CAAYgG,GACRgB,MAAMhB,GACN7G,KAAKmR,YAActK,EAAOsK,YAAYlI,KAAIlK,GAAK+Q,EAAW1H,WAAWrJ,IACzE,CAMA2R,SAAAA,CAAUjK,GACN,OAAOzG,KAAKmR,YAAYC,QAAO,CAACzS,EAAG0S,IACxBA,EAAWX,UAAU/R,IAC7B8H,EACP,EAOJ,MAAMsJ,UAAuBD,EAOzBwB,uBAAAA,CAAwB7K,GAEpB,IAAI8K,EAAS,GACb,IAAK,IAAIjI,EAAI,EAAGA,EAAI7C,EAAKtE,SAAUmH,EAAG,CAClC,IAAIkI,EAAO/K,EAAK6C,GACZmI,EAAKD,EAAK5E,WAAW,GACrB5M,KAAK0R,iBAAiBD,IACtBF,EAAOpO,KAAK,KACZoO,EAAOpO,KAAKqO,GACZD,EAAOpO,KAAK,MAEZoO,EAAOpO,KAAKqO,EAEpB,CACA,OAAOD,EAAO7N,KAAK,GACvB,CAgBAgO,gBAAAA,CAAiBD,GACb,OACKA,GAAM,OAAUA,GAAM,OACnBA,GAAM,OAAUA,GAAM,OACtBA,GAAM,QAAWA,GAAM,QACvBA,GAAM,QAAWA,GAAM,QACvBA,GAAM,QAAWA,GAAM,QACvBA,GAAM,QAAWA,GAAM,QACvBA,GAAM,OAAUA,GAAM,OACtBA,GAAM,QAAWA,GAAM,MAEnC,CAMAE,YAAAA,CAAalL,GACT,OAAOA,EAAKiK,UAAU,OAAO5N,QAAQ,mBAAoB,GAC7D,CAOA4N,SAAAA,CAAUjK,GAqBN,OAdIzG,KAAK6G,OAAO+K,uBACZnL,EAAOzG,KAAKsR,wBAAwB7K,IAGpCzG,KAAK6G,OAAOgL,WACZpL,EAAOA,EAAKwK,eAEsB,IAA9BjR,KAAK6G,OAAOiL,gBACZrL,EAAOzG,KAAK2R,aAAalL,KAEtBzG,KAAK6G,OAAOiL,gBACnBrL,EAAOzG,KAAK2R,aAAalL,IAGtBA,CACX,EAQJ,MAAMsL,UAAqBpK,EAAAA,GASvB,iBAAOS,CAAWvB,GACd,GAAe,OAAXA,EAAiB,OAAO,KAE5B,OAAQA,EAAOpE,MACX,IAAK,mBACD,OAAO,IAAIuP,EAAiBnL,GAChC,IAAK,WACD,OAAO,IAAIoL,GAAqBpL,GACpC,IAAK,kBACD,OAAO,IAAIqL,GAAgBrL,GAC/B,IAAK,YACD,OAAO,IAAIsL,GAAsBtL,GAErC,IAAK,YACD,OAAO,IAAIuL,EAAsBvL,GACrC,IAAK,QACD,OAAO,IAAIwL,EAAkBxL,GACjC,IAAK,cACD,OAAO,IAAIyL,EAAwBzL,GACvC,IAAK,SACD,OAAO,IAAI0L,EAAmB1L,GAClC,IAAK,UACD,OAAO,IAAI2L,GAAoB3L,GACnC,QACI,MAAM,IAAInG,MAAM,8BAADmF,OAA+BgB,EAAOpE,OAEjE,CAWAgQ,iBAAAA,CAAkBhM,EAAM3B,GACpB,MAAMpE,MAAM,uDAChB,CAQAgS,YAAAA,CAAajM,EAAM3B,GACf,IAAIqJ,EAAS,GAMb,OAJIA,EADAnN,MAAMC,QAAQwF,GACLA,EAAKwC,KAAIlK,GAAKiB,KAAKyS,kBAAkB1T,EAAG+F,KAExC9E,KAAKyS,kBAAkBhM,EAAM3B,GAEnCqJ,EAAOwE,MAClB,CAQA/J,KAAAA,CAAMnC,EAAM3B,GACR,OAAO9E,KAAK0S,aAAajM,EAAM3B,EACnC,EAMJ,MAAMkN,UAAyBD,EAO3BlR,WAAAA,CAAYgG,GACRgB,QAIA7H,KAAKqF,QAAU,IAAIK,OAAO,QAADG,OAASc,EAAiB,QAAAd,OAAOc,EAAiB,KAAK,KACpF,CAQA8L,iBAAAA,CAAkBhM,EAAM3B,GACpB,OAAO2B,EAAKqK,OAAO8B,MAAM5S,KAAKqF,UAAY,EAC9C,EAOJ,MAAM+M,UAA8BL,EAKhClR,WAAAA,CAAYgG,GAAQ,IAAAgM,EAChBhL,QACA7H,KAAK6G,OAASA,EAMd7G,KAAK8S,iBAAmB9S,KAAK6G,OAAOiM,iBAOpC9S,KAAK+S,aAAe/S,KAAK6G,OAAOkM,aAMhC/S,KAAKgT,UAAiC,QAAxBH,EAAG7S,KAAK6G,OAAOmM,iBAAS,IAAAH,GAAAA,EACtC7S,KAAKqF,QAAU,+EAEfrF,KAAKiT,aAAexG,EACpBzM,KAAK6N,aAAe,IAAIC,WAC5B,CAQA2E,iBAAAA,CAAkBhM,EAAM3B,GAUpB,OARI9E,KAAK8S,mBAAqBrM,EAAKyM,WAAW,OAC1CzM,EAAO,IAAMA,IAIJzG,KAAKgT,UAAavM,EAAKmM,MAAM5S,KAAKqF,UAAY,GAAM,CAACoB,IAGpDwC,KACVa,GAAS9I,MAAM2L,KAAK3M,KAAK6N,aAAa/E,OAAOgB,IAAQqJ,GAAQnT,KAAKiT,aAAaE,KAAOzP,KAAK,KAEnG,EAWJ,MAAM2O,UAA0BN,EAS5BlR,WAAAA,CAAYgG,GACRgB,QACA7H,KAAK6G,OAASA,EAGd7G,KAAKqF,QAAUD,EAAcpF,KAAK6G,OAAOxB,QAASrF,KAAK6G,OAAOvB,OAClE,CAQAmN,iBAAAA,CAAkBhM,EAAM3B,GACpB,OAAqB,OAAjB9E,KAAKqF,QACE,GAGPrF,KAAK6G,OAAOvB,OACLmB,EAAKmM,MAAM5S,KAAKqF,UAAY,GA5zC/C,SAAoBoB,EAAMhB,GACtB,MAAM0I,EAAS,GACf,IAAIM,EAAO,EACX,IAAK,MAAMmE,KAASnM,EAAK2M,SAAS3N,GAAQ,CACtC,MAAM4N,EAAYT,EAAM,GACpBnE,EAAOmE,EAAMU,OACbnF,EAAOhL,KAAKsD,EAAK6D,MAAMmE,EAAMmE,EAAMU,QAEnCD,EAAUlR,OAAS,GACnBgM,EAAOhL,KAAKkQ,GAEhB5E,EAAOmE,EAAMU,MAAQD,EAAUlR,MACnC,CAIA,OAHIsM,EAAOhI,EAAKtE,QACZgM,EAAOhL,KAAKsD,EAAK6D,MAAMmE,IAEpBN,CACX,CA6yCmBoF,CAAW9M,EAAMzG,KAAKqF,QAErC,EAOJ,MAAMiN,UAAgCP,EAKlClR,WAAAA,CAAYgG,GACRgB,QACA7H,KAAK6G,OAASA,EACd7G,KAAKqF,QAAU,IAAIK,OAAO,KAADG,OAAMc,EAAiB,QAAAd,OAAOc,EAAiB,MAAM,KAClF,CAQA8L,iBAAAA,CAAkBhM,EAAM3B,GACpB,OAAO2B,EAAKmM,MAAM5S,KAAKqF,UAAY,EACvC,EAQJ,MAAMkN,UAA2BR,EAK7BlR,WAAAA,CAAYgG,GACRgB,QACA7H,KAAK6G,OAASA,EAGd,MAAM2M,EAAgB,cAAH3N,OAAiB7F,KAAK6G,OAAO4M,kBAAoB,GAAK,KACzEzT,KAAKqF,QAAU,IAAIK,OAAO8N,EAAe,KAC7C,CAQAf,iBAAAA,CAAkBhM,EAAM3B,GACpB,OAAO2B,EAAKmM,MAAM5S,KAAKqF,UAAY,EACvC,EAMJ,MAAMqO,UAAsB/L,EAAAA,GAKxB9G,WAAAA,CAAYgG,GACRgB,QACA7H,KAAK6G,OAASA,CAClB,CASA,iBAAOuB,CAAWvB,GACd,GAAe,OAAXA,EAAiB,OAAO,KAC5B,OAAQA,EAAOpE,MACX,IAAK,qBACD,OAAO,IAAIkR,EAAmB9M,GAElC,IAAK,YACD,OAAO,IAAI+M,EAAuB/M,GAEtC,IAAK,oBACD,OAAO,IAAIgN,EAAkBhN,GACjC,IAAK,iBACD,OAAO,IAAIiN,EAAejN,GAE9B,QACI,MAAM,IAAInG,MAAM,+BAADmF,OAAgCgB,EAAOpE,OAElE,CAUAsR,YAAAA,CAAalL,GACT,MAAMnI,MAAM,kDAChB,CAQAkI,KAAAA,CAAMC,GAAiB,QAAAmL,EAAA9R,UAAAC,OAANmG,EAAI,IAAAtH,MAAAgT,EAAA,EAAAA,EAAA,KAAAC,EAAA,EAAAA,EAAAD,EAAAC,IAAJ3L,EAAI2L,EAAA,GAAA/R,UAAA+R,GACjB,OAAOjU,KAAK+T,aAAalL,KAAWP,EACxC,EAMJ,MAAMwL,UAAuBJ,EAMzB7S,WAAAA,CAAYgG,GACRgB,MAAMhB,GAGN7G,KAAKkU,IAAMrN,EAAOqN,IAAI,GACtBlU,KAAKmU,IAAMtN,EAAOsN,IAAI,EAC1B,CAQAJ,YAAAA,CAAalL,GAA4B,IAApBuL,EAAWlS,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,KAQ/B,OAPA2G,GAASwL,EAAAA,EAAAA,IAAY,CAACrU,KAAKkU,KAAMrL,EAAQ,CAAC7I,KAAKmU,MAI3B,OAAhBC,IACAvL,GAASwL,EAAAA,EAAAA,IAAYxL,EAAQ,CAAC7I,KAAKmU,KAAMC,EAAa,CAACpU,KAAKmU,OAEzDtL,CACX,EAEJ,MAAMgL,UAA0BC,GAMhC,MAAMH,UAA2BD,EAO7B7S,WAAAA,CAAYgG,GACRgB,MAAMhB,GAEN7G,KAAKsU,OAASzN,EAAOyN,OACrBtU,KAAKuU,KAAO1N,EAAO0N,IACvB,CAQAR,YAAAA,CAAalL,GAA4B,IAApBuL,EAAWlS,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,KAC3BO,EAAuB,OAAhB2R,EAAuBpU,KAAKsU,OAAStU,KAAKuU,KAEjDhI,EAAW,GACf,IAAK,IAAIiI,KAAQ/R,EACT,iBAAkB+R,EAClBjI,EAASpJ,KAAKqR,EAAKC,aAAarN,IAEzB,aAAcoN,IACI,MAArBA,EAAKE,SAAStN,GACdmF,GAAW8H,EAAAA,EAAAA,IAAY9H,EAAU1D,GAEL,MAArB2L,EAAKE,SAAStN,KACrBmF,GAAW8H,EAAAA,EAAAA,IAAY9H,EAAU6H,KAI7C,OAAO7H,CACX,EAOJ,MAAMqH,UAA+BF,EAMjCK,YAAAA,CAAalL,GACT,OAAOA,CACX,EAOJ,MAAM8L,UAAgBhN,EAAAA,GAOlB9G,WAAAA,CAAYgG,GACRgB,QACA7H,KAAK6G,OAASA,EAGd7G,KAAK4U,aAAe,GACpB5U,KAAKkI,mBAAqB,KAC1BlI,KAAK+S,aAAelM,EAAOkM,YAC/B,CASA,iBAAO3K,CAAWvB,GACd,GAAe,OAAXA,EAAiB,OAAO,KAC5B,OAAQA,EAAOpE,MACX,IAAK,YACD,OAAO,IAAIoS,EAAiBhO,GAChC,IAAK,YACD,OAAO,IAAIiO,GAAiBjO,GAChC,IAAK,YACD,OAAO,IAAIkO,EAAiBlO,GAEhC,IAAK,UACD,OAAO,IAAImO,EAAenO,GAC9B,IAAK,eACD,OAAO,IAAIoO,EAAapO,GAC5B,IAAK,OACD,OAAO,IAAIqO,EAAYrO,GAC3B,IAAK,QACD,OAAO,IAAIsO,EAAatO,GAE5B,IAAK,WACD,OAAO,IAAIuO,GAAgBvO,GAE/B,IAAK,MACD,OAAO,IAAIwO,GAAWxO,GAC1B,IAAK,aACD,OAAO,IAAIyO,GAAWzO,GAC1B,QACI,MAAM,IAAInG,MAAM,yBAADmF,OAA0BgB,EAAOpE,OAE5D,CAQAmG,KAAAA,CAAMC,GACF,OAAO7I,KAAKuV,OAAO1M,EACvB,CAOA0M,MAAAA,CAAO1M,GACH,OAAO7I,KAAKwV,aAAa3M,GAAQnF,KAAK,GAC1C,CASA8R,YAAAA,CAAa3M,GACT,MAAMnI,MAAM,oDAChB,EAIJ,MAAMsU,UAAuBL,EAGzBa,YAAAA,CAAa3M,GACT,IAAIxD,EAAUD,EAAcpF,KAAK6G,OAAOxB,SACxC,OAAgB,OAAZA,EACOwD,EAGJA,EAAOI,KAAIa,GAASA,EAAM6G,WAAWtL,EAASrF,KAAK6G,OAAOM,UACrE,EAIJ,MAAM8N,UAAqBN,EACvB9T,WAAAA,CAAYgG,GACRgB,MAAMhB,GAEN7G,KAAKyV,aAAe,IAAIC,WAC5B,CAGAF,YAAAA,CAAa3M,GAET,IAAI8M,EAAa,GACbC,EAAuB,GAE3B,IAAK,IAAI9L,KAASjB,EAAQ,CACtB,IAAIgN,EAAQ,KACZ,GAAqB,IAAjB/L,EAAM3H,QAAgB2H,EAAMoJ,WAAW,QAAUpJ,EAAMgM,SAAS,KAAM,CACtE,IAAI3C,EAAO4C,SAASjM,EAAMQ,MAAM,EAAG,GAAI,IAClC0L,MAAM7C,KACP0C,EAAQ1C,EAEhB,CACA,GAAc,OAAV0C,EACAD,EAAqBzS,KAAK0S,OACvB,CACH,GAAID,EAAqBzT,OAAS,EAAG,CACjC,IAAI8T,EAASjW,KAAKyV,aAAaF,OAAOW,WAAWvJ,KAAKiJ,IACtDD,EAAWxS,KAAK8S,GAChBL,EAAuB,EAC3B,CACAD,EAAWxS,KAAK2G,EACpB,CACJ,CACA,GAAI8L,EAAqBzT,OAAS,EAAG,CACjC,IAAI8T,EAASjW,KAAKyV,aAAaF,OAAOW,WAAWvJ,KAAKiJ,IACtDD,EAAWxS,KAAK8S,GAChBL,EAAuB,EAC3B,CAEA,OAAOD,CACX,EAQJ,MAAMT,UAAoBP,EAGtBa,YAAAA,CAAa3M,GACT,MAAO,CAACA,EAAOnF,KAAK,IACxB,EAIJ,MAAMyR,UAAqBR,EACvB9T,WAAAA,CAAYgG,GACRgB,MAAMhB,GAEN7G,KAAKmH,QAAUnH,KAAK6G,OAAOM,QAC3BnH,KAAKiK,MAAQjK,KAAK6G,OAAOoD,MACzBjK,KAAKmW,KAAOnW,KAAK6G,OAAOsP,IAC5B,CAGAX,YAAAA,CAAa3M,GACT,OAAOA,EAAOI,KAAIa,IACd,IAAIsM,EAAY,EAChB,IAAK,IAAI9M,EAAI,EAAGA,EAAItJ,KAAKiK,OACjBH,EAAMR,KAAOtJ,KAAKmH,UADQmC,EAE1B8M,EAAY9M,EAAI,EAOxB,IAAI+M,EAAWvM,EAAM3H,OACrB,IAAK,IAAImH,EAAI,EAAGA,EAAItJ,KAAKmW,OAAQ7M,EAAG,CAChC,MAAMgK,EAAQxJ,EAAM3H,OAASmH,EAAI,EACjC,GAAIQ,EAAMwJ,KAAWtT,KAAKmH,QAItB,MAHAkP,EAAW/C,CAKnB,CAEA,OAAOxJ,EAAMQ,MAAM8L,EAAWC,EAAS,GAE/C,EAOJ,MAAMxB,UAAyBF,EAQ3B9T,WAAAA,CAAYgG,GACRgB,MAAMhB,GACN7G,KAAKsW,QAAUzP,EAAOyP,OAC1B,CAGAd,YAAAA,CAAa3M,GACT,OAAOA,EAAOI,KAAI,CAACa,EAAOR,KACZ,IAANA,IAGIQ,EAFAA,EAAMoJ,WAAWlT,KAAK6G,OAAO0P,QAErBzM,EAAMhH,QAAQ9C,KAAK6G,OAAO0P,OAAQ,IAElC,IAAMzM,GAGlB9J,KAAKsW,UACLxM,EAAQtD,EAAsBsD,IAG3BA,IAEf,EAOJ,MAAMiL,UAAyBJ,EAM3B9T,WAAAA,CAAYgG,GACRgB,MAAMhB,GAEN7G,KAAKwW,aAAerJ,EACpBnN,KAAKyV,aAAe,IAAIC,YAAY,QAAS,CACzCe,OAAO,EACPC,WAAW,IAGf1W,KAAKkI,mBAAqB,IAC9B,CAOAyO,wBAAAA,CAAyB9N,GACrB,IAAIpC,EAAOoC,EAAOnF,KAAK,IAEnBkT,EAAY,IAAIV,WAAW,IAAIzP,GAAMwC,KAAIpH,GAAK7B,KAAKwW,aAAa3U,MAEpE,OADmB7B,KAAKyV,aAAaF,OAAOqB,EAEhD,CAGApB,YAAAA,CAAa3M,GAOT,IAAIgO,EAAY,GACZC,EAAmB,GACvB,IAAK,IAAIhN,KAASjB,OAM2CtD,IAArDvF,KAAK4U,aAAamC,MAAKhY,GAAKA,EAAEoI,UAAY2C,KACtCgN,EAAiB3U,OAAS,IAC1B0U,EAAU1T,KAAKnD,KAAK2W,yBAAyBG,IAC7CA,EAAmB,IAEvBD,EAAU1T,KAAK2G,IAEfgN,EAAiB3T,KAAK2G,GAS9B,OANIgN,EAAiB3U,OAAS,GAC1B0U,EAAU1T,KAAKnD,KAAK2W,yBAAyBG,IAK1CD,CACX,EAOJ,MAAMxB,WAAmBV,EAErB9T,WAAAA,CAAYgG,GACRgB,MAAMhB,GAEN7G,KAAK4P,UAAY5P,KAAK6G,OAAO+I,UAC7B5P,KAAKgX,qBAAuBhX,KAAK6G,OAAOmQ,qBACxChX,KAAKsW,QAAUtW,KAAK6G,OAAOyP,OAC/B,CAMAK,wBAAAA,CAAyB9N,GACrB,GAAsB,IAAlBA,EAAO1G,OAAc,MAAO,GAGhC,IAAI8U,EAAiB,CAACpO,EAAO,IAC7B,IAAK,IAAIS,EAAI,EAAGA,EAAIT,EAAO1G,SAAUmH,EAC7BT,EAAOS,KAAO2N,EAAeC,IAAI,IACjCD,EAAe9T,KAAK0F,EAAOS,IAKnC,IAEI7C,EAFkBwQ,EAAeE,QAAOrN,GAASA,IAAU9J,KAAK4P,YAEzClM,KAAK,IAOhC,OANI1D,KAAKsW,UAEL7P,EAAOD,EAAsBC,GACxBkK,WAAW3Q,KAAKgX,qBAAsB,KACtClG,QAEFrK,CACX,CAIA+O,YAAAA,CAAa3M,GACT,MAAO,CAAC7I,KAAK2W,yBAAyB9N,GAC1C,EAOJ,MAAMuM,WAAwBT,EAO1B9T,WAAAA,CAAYgG,GACRgB,MAAMhB,GACN7G,KAAKoX,SAAWvQ,EAAOuQ,SAASnO,KAAIlK,GAAK4V,EAAQvM,WAAWrJ,IAChE,CAGAyW,YAAAA,CAAa3M,GAET,OAAO7I,KAAKoX,SAAShG,QAAO,CAACiG,EAAMC,IACxBA,EAAQ9B,aAAa6B,IAC7BxO,EACP,EAIJ,MAAMyM,WAAmBX,EACrB9T,WAAAA,CAAYgG,GACRgB,MAAMhB,GAEN7G,KAAKuX,OAASvX,KAAK6G,OAAO0Q,MAC9B,CAEA/B,YAAAA,CAAa3M,GACT,OAAOA,EAAOI,KAAI,CAACa,EAAOR,IACfQ,EAAM6G,WAAW3Q,KAAKuX,OAASjO,IAAMT,EAAO1G,OAAS,EAAK,GAAK,MAE9E,EASJ,MAAMgQ,WAA8BJ,EAQhClR,WAAAA,CAAYgG,GAAQ,IAAA2Q,EAChB3P,QAEA7H,KAAKyX,eAAiB5Q,EAAOiM,iBAC7B9S,KAAK0X,YAAc7Q,EAAO6Q,YAC1B1X,KAAK2X,OAAS9Q,EAAO+Q,SAAW5X,KAAK0X,YACrC1X,KAAK6X,eAAsC,QAAxBL,EAAG3Q,EAAOgR,sBAAc,IAAAL,EAAAA,EAAI,QACnD,CAUA/E,iBAAAA,CAAkBhM,GAEV,IAFgB,cACpBqR,GACH5V,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAEGuF,EAAahB,EAAKkK,WAAW,IAAK3Q,KAAK2X,QAkB3C,OAZK3X,KAAKyX,iBAAmBhQ,EAAWyL,WAAWlT,KAAK0X,eAMxB,WAAxB1X,KAAK6X,gBACoB,UAAxB7X,KAAK6X,gBAAgD,IAAlBC,KAGxCrQ,EAAazH,KAAK2X,OAASlQ,GAExB,CAACA,EACZ,EAOJ,MAAMqN,WAAyBH,EAO3B9T,WAAAA,CAAYgG,GACRgB,MAAMhB,GAEN7G,KAAKyX,eAAiB5Q,EAAOiM,iBAC7B9S,KAAK0X,YAAc7Q,EAAO6Q,WAC9B,CAGAlC,YAAAA,CAAa3M,GACT,IAAIsF,EAAS,GACb,IAAK,IAAI7E,EAAI,EAAGA,EAAIT,EAAO1G,SAAUmH,EAAG,CACpC,IAAI7B,EAAaoB,EAAOS,GAAGqH,WAAW3Q,KAAK0X,YAAa,KACpD1X,KAAKyX,gBAAuB,GAALnO,GAAU7B,EAAWyL,WAAW,OACvDzL,EAAaA,EAAWsQ,UAAU,IAEtC5J,EAAOhL,KAAKsE,EAChB,CACA,OAAO0G,CACX,EAUJ,MAAM6B,WAAoBF,EAMtBjP,WAAAA,CAAYgG,GACRgB,MAAMhB,GACN7G,KAAKgY,SAAWnR,EAAOoR,oBAC3B,CAOAvH,SAAAA,CAAUjK,GAgBN,IAFAA,GADAA,EAAOA,EAAK3D,QAAQ,yDAA0D,KAClEA,QAAQ,2FAA4F,MAEvGiK,SAAS,UAAW,CAKzB,MAAMmL,EAAQzR,EAAKiH,MAAM,UACzBjH,EAAOyR,EAAMjP,KAAIkP,GAAQA,EAAKzH,UAAU,UAAShN,KAAK,SAC1D,MACI+C,EAAOA,EAAKiK,UAAU,QAG1B,OAAOjK,CACX,EAOJ,MAAMwL,WAA6BF,EAM/BlR,WAAAA,CAAYgG,GACRgB,QACA7H,KAAKoY,WAAavR,EAAOwR,cAAcpP,KAAIlK,GAAKgT,EAAa3J,WAAWrJ,IAC5E,CAQA0T,iBAAAA,CAAkBhM,EAAM3B,GAEpB,OAAO9E,KAAKoY,WAAWhH,QAAO,CAACkH,EAAkBC,IACtCA,EAAU7F,aAAa4F,EAAkBxT,IACjD,CAAC2B,GACR,EAOJ,MAAMyL,WAAwBH,EAK1BlR,WAAAA,CAAYgG,GACRgB,OACJ,CAOA4K,iBAAAA,CAAkBhM,EAAM3B,GACpB,OAr9DR,SAA0B2B,GACtB,OAAOA,EAAKmM,MAAM,SAAW,EACjC,CAm9De4F,CAAiB/R,EAC5B,EAIJ,MAAM+L,WAA4BT,EAM9BlR,WAAAA,CAAYgG,GACRgB,QACA7H,KAAK6G,OAASA,EACd7G,KAAKqF,QAAUD,EAAcpF,KAAK6G,OAAOxB,SACzCrF,KAAKmH,QAAUnH,KAAK6G,OAAOM,OAC/B,CAQAsL,iBAAAA,CAAkBhM,EAAM3B,GACpB,OAAqB,OAAjB9E,KAAKqF,QACE,CAACoB,GAEL,CAACA,EAAKkK,WAAW3Q,KAAKqF,QAASrF,KAAK6G,OAAOM,SACtD,EAGJ,MAAMsR,GAA2B,CAC7B,YACA,YACA,YACA,YACA,YACA,YACA,cAIG,MAAMC,WAA4B/Q,EAAAA,GAQrC9G,WAAAA,CAAY8X,EAAeC,GAAiB,IAAAC,EAAAC,EAAAC,EAAAC,EACxCnR,SAAQoR,EAAAA,EAAAA,GAAA,yOAERjZ,KAAKkZ,kBAAoBN,EAGzB5Y,KAAKqR,WAAavB,EAAW1H,WAAWuQ,EAActH,YACtDrR,KAAKmZ,cAAgBpH,EAAa3J,WAAWuQ,EAAcQ,eAE3DnZ,KAAKoZ,MAAQ1R,EAAeU,WAAWuQ,EAAcS,MAAOR,GAC5D5Y,KAAKqZ,eAAiB3F,EAActL,WAAWuQ,EAAcU,gBAG7DrZ,KAAKsX,QAAU3C,EAAQvM,WAAWuQ,EAAcrB,SAGhDtX,KAAKsZ,eAAiB,GACtBtZ,KAAKuZ,gBAAkB,GAGvBvZ,KAAK4U,aAAe,GACpB,IAAK,IAAI4E,KAAcb,EAAc/D,aAAc,CAC/C,MAAM9K,EAAQ,IAAIlD,EAAW4S,GAC7BxZ,KAAK4U,aAAazR,KAAK2G,GAEvB9J,KAAKoZ,MAAMrR,cAAcoH,IAAIrF,EAAM3C,QAAS2C,EAAM1C,IAClDpH,KAAKoZ,MAAMtR,MAAMgC,EAAM1C,IAAM0C,EAAM3C,QAE/B2C,EAAMtC,UACNxH,KAAKsZ,eAAenW,KAAK2G,EAAM3C,SAC/BnH,KAAKuZ,gBAAgBpW,KAAK2G,EAAM1C,IAExC,CAGApH,KAAKyZ,0BAAqE,QAA5CZ,EAAGD,EAAgBa,iCAAyB,IAAAZ,EAAAA,EAAI,GAC9E7Y,KAAKsZ,eAAenW,QAAQnD,KAAKyZ,2BACjCzZ,KAAKsZ,eAAiB,IAAI,IAAII,IAAI1Z,KAAKsZ,iBAEnCtZ,KAAKsX,UAELtX,KAAKsX,QAAQ1C,aAAe5U,KAAK4U,aAMjC5U,KAAKsX,QAAQpP,mBAAqBlI,KAAKoZ,MAAMlR,oBAIjDlI,KAAK2Z,mBAAqB3Z,KAAK4U,aAAazS,OAAS,EAAI,IAAIuD,OACzD1F,KAAK4U,aAAa3L,KAAIlK,GAAK,GAAJ8G,OAAO9G,EAAEuI,OAAS,OAAS,GAAE,KAAAzB,QAAID,EAAAA,EAAAA,IAAa7G,EAAEoI,SAAQ,KAAAtB,OAAI9G,EAAEwI,OAAS,OAAS,MAAM7D,KAAK,MAClH,KAGJ1D,KAAK4Z,WAAa5Z,KAAK6Z,SAAS,cAChC7Z,KAAK8Z,cAAgB9Z,KAAKoZ,MAAMrR,cAAcoB,IAAInJ,KAAK4Z,YAEvD5Z,KAAK4P,UAAY5P,KAAK6Z,SAAS,YAAa,aAC5C7Z,KAAK6P,aAAe7P,KAAKoZ,MAAMrR,cAAcoB,IAAInJ,KAAK4P,WAEtD5P,KAAK+Z,UAAY/Z,KAAK6Z,SAAS,aAC/B7Z,KAAKga,aAAeha,KAAKoZ,MAAMrR,cAAcoB,IAAInJ,KAAK+Z,WAEtD/Z,KAAKiI,UAAYjI,KAAK6Z,SAASjB,EAAiB,aAChD5Y,KAAKgI,aAAehI,KAAKoZ,MAAMrR,cAAcoB,IAAInJ,KAAKiI,WAEtDjI,KAAKia,iBAAmBrB,EAAgBqB,iBAGxCja,KAAKka,aAAetB,EAAgBsB,aAEpCla,KAAKma,6BAA2E,QAA/CrB,EAAGF,EAAgBuB,oCAA4B,IAAArB,GAAAA,EAChF9Y,KAAKoa,+BAA+E,QAAjDrB,EAAGH,EAAgBwB,sCAA8B,IAAArB,GAAAA,EAGpF/Y,KAAKqa,aAAe,QAEpBra,KAAKmF,QAAS,EAEdnF,KAAKsa,cAA6C,QAAhCtB,EAAGJ,EAAgB0B,qBAAa,IAAAtB,EAAAA,EAAI,KACtDhZ,KAAKua,yBAA2B,IAAIrU,GACxC,CAQA2T,QAAAA,GAAkB,QAAAW,EAAAtY,UAAAC,OAANsB,EAAI,IAAAzC,MAAAwZ,GAAAC,EAAA,EAAAA,EAAAD,EAAAC,IAAJhX,EAAIgX,GAAAvY,UAAAuY,GACZ,IAAK,IAAIlZ,KAAOkC,EAAM,CAClB,IAAI+Q,EAAOxU,KAAKkZ,kBAAkB3X,GAElC,GAAKiT,EAAL,CAEA,GAAoB,kBAATA,EAAmB,CAC1B,GAAoB,eAAhBA,EAAKkG,OACL,OAAOlG,EAAKrN,QAEZ,MAAMzG,MAAM,kBAADmF,OAAmB2O,GAEtC,CACI,OAAOA,CATQ,CAWvB,CACA,OAAO,IACX,CAWA,4BAAamG,CAAgB9V,GAOrB,IAPoD,kBACxD+V,EAAoB,KAAI,OACxB/T,EAAS,KAAI,UACbgU,EAAY,KAAI,iBAChBC,GAAmB,EAAK,SACxBC,EAAW,OAAM,OACjB5V,EAAS,MACZjD,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAYD,OAAO,IAAIlC,cAVM4E,EAAcC,EAA+B,CAC1D+V,oBACA/T,SACAgU,YACAC,mBACAC,WACA5V,WAKR,CAQA6V,oBAAAA,CAAqBC,GACjB,OAAOA,CACX,CAcArS,KAAAA,CAEInC,GAWF,IAGMoC,GAXJ,UACIqS,EAAY,KAAI,mBAChBC,GAAqB,EAAI,QACzBC,GAAU,EAAK,WACfC,EAAa,KAAI,WACjBC,EAAa,KAAI,cACjBC,GAAgB,GACnBrZ,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAML,GAAIlB,MAAMC,QAAQwF,GAAO,CACrB,GAAoB,IAAhBA,EAAKtE,OACL,MAAMzB,MAAM,gCAGhB,GAAkB,OAAdwa,EAAoB,CACpB,IAAKla,MAAMC,QAAQia,GACf,MAAMxa,MAAM,mCAET,GAAI+F,EAAKtE,SAAW+Y,EAAU/Y,OACjC,MAAMzB,MAAM,gDAGhBmI,EAASpC,EAAKwC,KACV,CAACtK,EAAG2K,IAAMtJ,KAAK8I,OAAOnK,EAAGuc,EAAU5R,GAAI,CAAE6R,wBAGjD,MACItS,EAASpC,EAAKwC,KAAIlK,GAAKiB,KAAK8I,OAAO/J,EAAG,KAAM,CAAEoc,wBAGtD,KAAO,CACH,GAAa,OAAT1U,EACA,MAAM/F,MAAM,wBAGhB,GAAIM,MAAMC,QAAQia,GACd,MAAMxa,MAAM,kHAIhBmI,EAAS,CAAC7I,KAAK8I,OAAOrC,EAAMyU,EAAW,CAAEC,uBAC7C,CAIA,IAAIK,GAAmBC,EAAAA,EAAAA,IAAI5S,EAAOI,KAAIlK,GAAKA,EAAEoD,UAAS,GAGnC,OAAfmZ,IACAA,EAAaE,GAIjBF,EAAaI,KAAKrQ,IAAIiQ,EAAYtb,KAAKia,kBAGvC,IAAI0B,EAAiB,GACrB,GAAIP,GAAWC,EAEX,IAAK,IAAI/R,EAAI,EAAGA,EAAIT,EAAO1G,SAAUmH,EACjC,GAAIT,EAAOS,GAAGnH,SAAWmZ,EAIlB,GAAIzS,EAAOS,GAAGnH,OAASmZ,EAEtBD,IACAxS,EAAOS,GAAKT,EAAOS,GAAGgB,MAAM,EAAGgR,IAEnCK,EAAexY,KAAK,IAAInC,MAAM6H,EAAOS,GAAGnH,QAAQyZ,KAAK,SAGrD,GAAIR,EAAS,CACT,IAAIS,EAAOP,EAAazS,EAAOS,GAAGnH,OAER,UAAtBnC,KAAKqa,cACLsB,EAAexY,KACV,IAAInC,MAAM6H,EAAOS,GAAGnH,QAAQyZ,KAAK,GAAI/V,OAAO,IAAI7E,MAAM6a,GAAMD,KAAK,KAEtE/S,EAAOS,GAAGnG,QAAQ,IAAInC,MAAM6a,GAAMD,KAAK5b,KAAK6P,iBAE5C8L,EAAexY,KACV,IAAInC,MAAM6a,GAAMD,KAAK,GAAI/V,OAAO,IAAI7E,MAAM6H,EAAOS,GAAGnH,QAAQyZ,KAAK,KAEtE/S,EAAOS,GAAGwS,WAAW,IAAI9a,MAAM6a,GAAMD,KAAK5b,KAAK6P,eAGvD,MACI8L,EAAexY,KAAK,IAAInC,MAAM6H,EAAOS,GAAGnH,QAAQyZ,KAAK,SA3BzDD,EAAexY,KAAK,IAAInC,MAAM6H,EAAOS,GAAGnH,QAAQyZ,KAAK,SAgC7DD,EAAiB9S,EAAOI,KAAIlK,GAAK,IAAIiC,MAAMjC,EAAEoD,QAAQyZ,KAAK,KAG9D,GAAIL,EAAe,CACf,KAAMH,IAAWC,IAITxS,EAAOkT,MAAKhd,GAAKA,EAAEoD,SAAW0G,EAAO,GAAG1G,SACxC,MAAMzB,MACF,2KASZ,IAAI4F,EAAO,CAACuC,EAAO1G,OAAQ0G,EAAO,GAAG1G,QAErC0G,EAAS,IAAImT,EAAAA,GAAO,QAChBC,cAActP,KAAK9D,EAAO8J,OAAO1J,IAAIiT,SACrC5V,GAGJqV,EAAiB,IAAIK,EAAAA,GACjB,QACAC,cAActP,KAAKgP,EAAehJ,OAAO1J,IAAIiT,SAC7C5V,EAER,MAEStF,MAAMC,QAAQwF,KAEfoC,EAASA,EAAO,GAChB8S,EAAiBA,EAAe,IAMxC,IAAIQ,EAAc,CACdC,UAAWvT,EACX8S,eAAgBA,GAMpB,OAFAQ,EAAcnc,KAAKgb,qBAAqBmB,GAEjCA,CACX,CAQAE,YAAAA,CAAa5V,GACT,GAAa,OAATA,EAAe,OAAO,KAK1B,MAEMoC,GAFW7I,KAAK2Z,mBAAqBlT,EAAKiH,MAAM1N,KAAK2Z,oBAAoBxC,QAAOpY,GAAKA,IAAK,CAAC0H,IAEzEwC,KAAI,CAAClK,EAAG+Y,KAE5B,QAAmBvS,IADAvF,KAAK4U,aAAamC,MAAKpY,GAAKA,EAAEwI,UAAYpI,IAGzD,OAAOA,EACJ,EACuB,IAAtBiB,KAAKka,eACLnb,EAAIA,EAAE+R,OAAOpD,MAAM,OAAOhK,KAAK,MAE/B1D,KAAKoa,iCACLrb,EA93EpB,SAAqC0H,GACjC,OAAOC,EAAeD,EAAKwK,cAC/B,CA43EwBqL,CAA4Bvd,IAGZ,OAApBiB,KAAKqR,aACLtS,EAAIiB,KAAKqR,WAAWtS,IAGxB,MAAMwd,EAAwC,OAAvBvc,KAAKmZ,cAA0BnZ,KAAKmZ,cAAcpa,EAAG,CACxE+Y,kBACC,CAAC/Y,GAIN,OAFeiB,KAAKoZ,MAAMmD,EAG9B,KACD5J,OAEH,OAAO9J,CACX,CAWAC,MAAAA,CAAOrC,GAEC,IAFKyU,EAAShZ,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,MAAM,mBAC3BiZ,GAAqB,GACxBjZ,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAEG2G,EAAS7I,KAAKqc,aAAa5V,GAC3B+V,EAAUxc,KAAKqc,aAAanB,GAG5BuB,EAA0C,OAAxBzc,KAAKqZ,gBAA2B8B,EAChDnb,KAAKqZ,eAAexQ,EAAQ2T,IAC5BnI,EAAAA,EAAAA,IAAkB,OAANxL,QAAM,IAANA,EAAAA,EAAU,GAAW,OAAP2T,QAAO,IAAPA,EAAAA,EAAW,IAG3C,OADUxc,KAAKoZ,MAAMrQ,sBAAsB0T,EAE/C,CAQAC,YAAAA,CAAaC,GAAyB,IAAlBC,EAAW1a,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAI/B,OAHIya,aAAiBX,EAAAA,KACjBW,EAAQA,EAAMpW,UAEXoW,EAAM1T,KAAIlK,GAAKiB,KAAKuV,OAAOxW,EAAG6d,IACzC,CAaArH,MAAAA,CACIsH,GAEF,IADED,EAAW1a,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAMf,GAJI2a,aAAqBb,EAAAA,KACrBa,EAAYzW,EAAuByW,KAGlC7b,MAAMC,QAAQ4b,IAAmC,IAArBA,EAAU1a,UAAiB2a,EAAAA,EAAAA,IAAiBD,EAAU,IACnF,MAAMnc,MAAM,oDAGhB,OAAOV,KAAK+c,cAAcF,EAAWD,EACzC,CAWAG,aAAAA,CACIF,EAASG,GAKX,IAJE,oBACIC,GAAsB,EAAK,6BAC3B9C,EAA+B,MAClC6C,EAEGnU,EAAS7I,KAAKoZ,MAAM5P,sBAAsBqT,GAC1CI,IACApU,EAASA,EAAOsO,QAAOpY,IAAMiB,KAAKsZ,eAAevM,SAAShO,MAM9D,IAAIme,EAAUld,KAAKsX,QAAUtX,KAAKsX,QAAQzO,GAAUA,EAAOnF,KAAK,KAehE,OAXI1D,KAAKsX,SAAWtX,KAAKsX,QAAQpP,qBAC7BgV,EAAUA,EAAQvM,WAAW3Q,KAAKsX,QAAQpP,mBAAoB,KAC1D+U,IACAC,EAAUA,EAAQpM,UAIM,OAA5BqJ,QAA4B,IAA5BA,EAAAA,EAAgCna,KAAKma,gCACrC+C,EAAU1W,EAAsB0W,IAG7BA,CACX,CAEA,yBAAIC,GAWA,OAVKnd,KAAKod,8BACNtX,QAAQC,KACJ,gUAKJ/F,KAAKod,6BAA8B,GAGhCpd,KAAKqd,sBAChB,CAoDAC,mBAAAA,CAAoBC,GAQZ,IAAAC,EAAAC,EAAA,IAR0B,cAC9BnD,EAAgB,KAAI,sBACpBoD,GAAwB,EAAK,SAC7BrR,GAAW,EAAI,QACf+O,GAAU,EAAK,WACfC,GAAa,EAAK,WAClBC,EAAa,KAAI,cACjBC,GAAgB,GACnBrZ,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAEY,QAAbsb,EAAAlD,SAAa,IAAAkD,IAAblD,EAAoC,QAAvBmD,EAAKzd,KAAKsa,qBAAa,IAAAmD,EAAAA,EAAIzd,KAAKmd,uBAG7C,IAAIQ,EAAmB3d,KAAKua,yBAAyBpR,IAAImR,QAChC/U,IAArBoY,IACAA,EAAmB,IAAIC,EAAAA,GAAStD,GAChCta,KAAKua,yBAAyBpL,IAAImL,EAAeqD,IAGrD,MAAME,EAAqBpe,OAAOqe,OAAO,MACzC,IAAK,MAAMvc,KAAOkX,GAA0B,CACxC,MAAMlV,EAAQvD,KAAK6Z,SAAStY,GACxBgC,IACAsa,EAAmBtc,GAAOgC,EAElC,CAEA,MAAMwa,EAAWJ,EAAiBK,OAAO,CACrCC,SAAUV,EACVG,sBAAuBA,KAEpBG,IAGP,OAAIxR,EACOrM,KAAK4I,MAAMmV,EAAU,CACxB5C,oBAAoB,EACpBC,UACAC,aACAC,aACAC,kBACDa,UAGA2B,CACX,EAQG,SAASG,GAAgBjD,GAE5B,GAAIA,EAAOmB,qBAAqBJ,EAAAA,GAC5Bf,EAAOkD,eAAiB,IAAInC,EAAAA,GACxB,QACA,IAAIC,cAAchB,EAAOmB,UAAUgC,KAAKjc,QACxC8Y,EAAOmB,UAAU9V,UAElB,KAAItF,MAAMC,QAAQga,EAAOmB,WAW5B,MAAM,IAAI1b,MAAM,0CATZM,MAAMC,QAAQga,EAAOmB,UAAU,IAE/BnB,EAAOkD,eAAiBlD,EAAOmB,UAAUnT,KACrClK,GAAK,IAAIiC,MAAMjC,EAAEoD,QAAQyZ,KAAK,KAGlCX,EAAOkD,eAAiB,IAAInd,MAAMia,EAAOmB,UAAUja,QAAQyZ,KAAK,EAIxE,CAEA,OAAOX,CACX,CA+EO,MAAMoD,WAAsB3F,GAAoB7X,WAAAA,GAAA,SAAAqB,YAAA+W,EAAAA,EAAAA,GAAA,sHAIhD,MAAMqF,WAAuB5F,GAChC7X,WAAAA,CAAY8X,EAAeC,GACvB/Q,MAAM8Q,EAAeC,GAErB5Y,KAAKue,cAAgB,sBACrBve,KAAKwe,eAAiBxe,KAAKsZ,eAAenC,QAAOpY,GAAKiB,KAAKue,cAAcE,KAAK1f,KAC9EiB,KAAK0e,cAAgB3f,GAAKA,CAC9B,CASA4f,yBAAAA,CAA0BC,EAAYC,EAAmBC,GACrD,OAAOH,GAA0B3e,KAAM4e,EAAYC,EAAmBC,EAC1E,EAoBJ,MAAMC,GAAmB,SAElB,MAAMC,WAAuBtG,GAUhC7X,WAAAA,CAAY8X,EAAeC,GAAiB,IAAAqG,EAAAC,EACxCrX,MAAM8Q,EAAeC,IAAiBK,EAAAA,EAAAA,GAAA,olCAAAA,EAAAA,EAAAA,GAAA,6BAPtC,igBAQAjZ,KAAKmf,0BAAqE,QAA5CF,EAAGrG,EAAgBuG,iCAAyB,IAAAF,GAAAA,EAE1Ejf,KAAKmF,OAA+B,QAAzB+Z,EAAGtG,EAAgBzT,cAAM,IAAA+Z,GAAAA,EAC/Blf,KAAKmF,SAENnF,KAAKqR,WAAa,KAClBrR,KAAKmZ,cAAgB,IAAIhH,GAAsB,CAC3CuF,YAAaqH,GACbjM,kBAAkB,EAClB+E,eAAgB,UAG5B,CAQAwE,YAAAA,CAAa5V,GACT,GAAa,OAATA,EAAe,OAAO,KAE1B,GAAIzG,KAAKmF,QAA0B,IAAhBsB,EAAKtE,OACpB,OAAO0F,MAAMwU,aAAa5V,GAG9B,IAAIoC,EAAShB,MAAMwU,aAAa0C,GAAmBtY,EAAKkK,WAAWoO,GAAkB,MAIrF,OAHIlW,EAAO1G,OAAS,GAAK0G,EAAO,KAAOkW,IAAoB/e,KAAKsZ,eAAevM,SAASlE,EAAO,MAC3FA,EAASA,EAAOyB,MAAM,IAEnBzB,CACX,CAEA,yBAAIsU,GACA,OAAOtV,MAAMsV,sBACRxM,WAAW,qBAAsB3Q,KAAKmf,0BAA4B,OAAS,SAC3ExO,WAAW,yBAA0B3Q,KAAKof,sBAAsBzO,WAAW,KAAM,OAAOA,WAAW,IAAK,OACjH,EAsBJ,SAASgO,GAA0BU,EAAMT,EAAYC,EAAmBC,GACpE,KAAM,mBAAoBO,KAAUre,MAAMC,QAAQoe,EAAKb,gBACnD,MAAM,IAAI9d,MAAM,iGAEpB,KAAM,kBAAmB2e,MAAWA,EAAKd,yBAAyB7Y,QAC9D,MAAM,IAAIhF,MAAM,4FAEpB,KAAM,kBAAmB2e,IAAuC,oBAAvBA,EAAKX,cAC1C,MAAM,IAAIhe,MAAM,kFAEpB,MAAM4e,EAAiBR,EAAgBS,SACjCC,EAAiBV,EAAgBW,SAGvC,IAAKJ,EAAKb,eAAezR,SAASyS,GAC9B,MAAM,IAAI9e,MAAM,yBAADmF,OAA0B2Z,EAAc,qCAAA3Z,OAAoCwZ,EAAKb,eAAe9a,KAAK,MAAK,MAI7H,QAAuB6B,IAAnB+Z,EAA8B,CAE9B,IAAKD,EAAKb,eAAezR,SAASuS,GAC9B,MAAM,IAAI5e,MAAM,yBAADmF,OAA0ByZ,EAAc,qCAAAzZ,OAAoCwZ,EAAKb,eAAe9a,KAAK,MAAK,MAK7H,IAAK,IAAI8Q,KAAQ6K,EAAKhG,eAAexS,OAAOyN,OACxC,GAAI,iBAAkBE,GAAQ6K,EAAKd,cAAcE,KAAKjK,EAAKC,aAAarN,IAAK,CACzEoN,EAAKC,aAAarN,GAAKiY,EAAKX,cAAcY,GAC1C,KACJ,CAGR,CAKA,OAFAR,EAAgBY,oBAAsBL,EAAKjG,MAAMrQ,sBAAsB,CAACsW,EAAKX,cAAcc,KAAkB,GAEtGH,EAAKzW,MAAMgW,EAAYC,EAClC,CAuEA,MAAMc,GAAoB,CACtB,CAAC,KAAM,WACP,CAAC,KAAM,WACP,CAAC,KAAM,UACP,CAAC,KAAM,WACP,CAAC,KAAM,WACP,CAAC,KAAM,UACP,CAAC,KAAM,UACP,CAAC,KAAM,YACP,CAAC,KAAM,cACP,CAAC,KAAM,WACP,CAAC,KAAM,UACP,CAAC,KAAM,WACP,CAAC,KAAM,SACP,CAAC,KAAM,UACP,CAAC,KAAM,WACP,CAAC,KAAM,WACP,CAAC,KAAM,cACP,CAAC,KAAM,SACP,CAAC,KAAM,WACP,CAAC,KAAM,cACP,CAAC,KAAM,UACP,CAAC,KAAM,aACP,CAAC,KAAM,SACP,CAAC,KAAM,SACP,CAAC,KAAM,SACP,CAAC,KAAM,YACP,CAAC,KAAM,UACP,CAAC,KAAM,aACP,CAAC,KAAM,SACP,CAAC,KAAM,aACP,CAAC,KAAM,QACP,CAAC,KAAM,QACP,CAAC,KAAM,YACP,CAAC,KAAM,aACP,CAAC,KAAM,cACP,CAAC,KAAM,SACP,CAAC,KAAM,SACP,CAAC,KAAM,aACP,CAAC,KAAM,SACP,CAAC,KAAM,UACP,CAAC,KAAM,UACP,CAAC,KAAM,WACP,CAAC,KAAM,WACP,CAAC,KAAM,WACP,CAAC,KAAM,WACP,CAAC,KAAM,eACP,CAAC,KAAM,aACP,CAAC,KAAM,WACP,CAAC,KAAM,YACP,CAAC,KAAM,cACP,CAAC,KAAM,UACP,CAAC,KAAM,UACP,CAAC,KAAM,aACP,CAAC,KAAM,YACP,CAAC,KAAM,UACP,CAAC,KAAM,aACP,CAAC,KAAM,WACP,CAAC,KAAM,UACP,CAAC,KAAM,YACP,CAAC,KAAM,WACP,CAAC,KAAM,YACP,CAAC,KAAM,WACP,CAAC,KAAM,WACP,CAAC,KAAM,WACP,CAAC,KAAM,SACP,CAAC,KAAM,SACP,CAAC,KAAM,UACP,CAAC,KAAM,UACP,CAAC,KAAM,aACP,CAAC,KAAM,WACP,CAAC,KAAM,YACP,CAAC,KAAM,cACP,CAAC,KAAM,SACP,CAAC,KAAM,UACP,CAAC,KAAM,YACP,CAAC,KAAM,WACP,CAAC,KAAM,WACP,CAAC,KAAM,OACP,CAAC,KAAM,SACP,CAAC,KAAM,WACP,CAAC,KAAM,kBACP,CAAC,KAAM,UACP,CAAC,KAAM,WACP,CAAC,KAAM,WACP,CAAC,KAAM,WACP,CAAC,KAAM,YACP,CAAC,KAAM,iBACP,CAAC,KAAM,WACP,CAAC,KAAM,WACP,CAAC,KAAM,WACP,CAAC,KAAM,YACP,CAAC,KAAM,YACP,CAAC,KAAM,SACP,CAAC,MAAO,YACR,CAAC,KAAM,WACP,CAAC,KAAM,SACP,CAAC,KAAM,WACP,CAAC,KAAM,YACP,CAAC,KAAM,cAILC,GAA2B,IAAI1Z,IAAIyZ,IAEnCE,GAAmC,IAAI3Z,IAAI,IAC1CyZ,GAAkB1W,KAAI6W,IAAA,IAAEhe,EAAGjD,GAAEihB,EAAA,MAAK,CAACjhB,EAAGiD,EAAE,IAEvC,CAAC,UAAW,MACZ,CAAC,YAAa,MACd,CAAC,UAAW,MACZ,CAAC,UAAW,MACZ,CAAC,gBAAiB,MAClB,CAAC,SAAU,MACX,CAAC,UAAW,MACZ,CAAC,YAAa,MACd,CAAC,WAAY,MACb,CAAC,YAAa,MACd,CAAC,YAAa,QAgyBf,MAAMie,WAA4BrH,GAAoB7X,WAAAA,GAAA,SAAAqB,YAAA+W,EAAAA,EAAAA,GAAA,oNAgBtD,MAAM+G,GA4DT,4BAAarF,CAAgB9V,GAQrB,IAAAob,EAAAC,EAAA,IARoD,UACxDC,GAAY,EAAI,kBAChBvF,EAAoB,KAAI,OACxB/T,EAAS,KAAI,UACbgU,EAAY,KAAI,iBAChBC,GAAmB,EAAK,SACxBC,EAAW,OAAM,OACjB5V,EAAS,MACZjD,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,GAEIyW,EAAeC,SAAyBhU,EAAcC,EAA+B,CACtFsb,YACAvF,oBACA/T,SACAgU,YACAC,mBACAC,WACA5V,WAIAib,EAAqE,QAAxDH,EAAkC,QAAlCC,EAAGtH,EAAgByH,uBAAe,IAAAH,OAAA,EAA/BA,EAAiCpd,QAAQ,QAAS,WAAG,IAAAmd,EAAAA,EAAI,sBAEzE/L,EAAMlU,KAAKsgB,wBAAwBF,GAKvC,OAJKlM,IACDpO,QAAQC,KAAK,4BAADF,OAA6Bua,EAAa,gDACtDlM,EAAMwE,IAEH,IAAIxE,EAAIyE,EAAeC,EAClC,GACHK,EAAAA,EAAAA,GA1FY+G,GAAa,0BACW,CAC7BO,YA5oCD,cAA0B7H,KA6oCzB8H,oBAjqCD,cAAkC9H,KAkqCjC+H,mBAjqCD,cAAiC/H,KAkqChCgI,iBA3rCD,cAA+BhI,GAElCsC,oBAAAA,CAAqBC,GACjB,OAAOiD,GAAgBjD,EAC3B,GAwrCI0F,mBAtrCD,cAAiCjI,GAEpCsC,oBAAAA,CAAqBC,GACjB,OAAOiD,GAAgBjD,EAC3B,GAmrCI2F,cAztCD,cAA4BlI,GAE/BsC,oBAAAA,CAAqBC,GACjB,OAAOiD,GAAgBjD,EAC3B,GAstCI4F,iBAlrCD,cAA+BnI,GAElCsC,oBAAAA,CAAqBC,GACjB,OAAOiD,GAAgBjD,EAC3B,GA+qCI6F,kBA7qCD,cAAgCpI,GAEnCsC,oBAAAA,CAAqBC,GACjB,OAAOiD,GAAgBjD,EAC3B,GA0qCI8F,aAtqCD,cAA2BrI,GAC9B7X,WAAAA,CAAY8X,EAAeC,GACvB/Q,MAAM8Q,EAAeC,GACrB9S,QAAQC,KAAK,wJACjB,CAGAiV,oBAAAA,CAAqBC,GACjB,OAAOiD,GAAgBjD,EAC3B,GA8pCI+F,iBA5pCD,cAA+BtI,GAElCsC,oBAAAA,CAAqBC,GACjB,OAAOiD,GAAgBjD,EAC3B,GAypCIgG,oBA9sCD,cAAkCvI,GAErCsC,oBAAAA,CAAqBC,GACjB,OAAOiD,GAAgBjD,EAC3B,GA2sCIiG,qBAzsCD,cAAmCxI,GAEtCsC,oBAAAA,CAAqBC,GACjB,OAAOiD,GAAgBjD,EAC3B,GAssCIkG,gBAttCD,cAA8BzI,GAEjCsC,oBAAAA,CAAqBC,GACjB,OAAOiD,GAAgBjD,EAC3B,GAmtCIoD,iBACA+C,cAtpCD,cAA4B1I,KAupC3B4F,kBACA+C,iBAnoCD,cAA+B/C,KAooC9BgD,iBAloCD,cAA+B5I,KAmoC9B6I,iBA5zBD,cAA+B7I,GAAoB7X,WAAAA,GAAA,SAAAqB,YAAA+W,EAAAA,EAAAA,GAAA,qHAStDuI,WAAAA,CAAYC,GAKJ,IALe,kBACnBC,GAAoB,EAAK,gBACzBC,GAAkB,EAAK,eACvBC,EAAiB,KAAI,qBACrBC,GAAuB,GAC1B3f,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAoBD,GAAuB,OAAnB0f,EACA,MAAMlhB,MAAM,+BAEhB,IAAIohB,EAAgB,KAEpB,MAAMC,EAA6C,SAAtBL,EAE7B,SAASM,IACL,MAAO,CAAE,SAAYF,EAAe,UAAa,CAAC,KAAM,MAAO,KAAQ,GAC3E,CAGA,MAAMG,EAAS,GACf,IAAIC,EAAQF,IACRG,EAAc,EAClB,MAAMC,EAAkBpiB,KAAKoZ,MAAMrQ,sBAAsB,CAAC,qBAAqB,GAAK,EAEpF,IAAIsZ,EAAkB,GAClBC,EAA4B,GAE5BC,GAAO,EACPC,EAAqB,KAGzB,MAAMjJ,EAAkB,IAAIG,IAAI1Z,KAAKuZ,iBAErC,IAAK,IAAIhI,KAAUkQ,EAAW,CAE1B,MAAM5E,EAAYtL,EAAO1I,OACnB4Z,EAAmBV,EAAuBxQ,EAAOkR,iBAAmB,KAI1E,IAAIC,EAAiB,KACjBC,EAAkBP,EAEtB,GAAI,WAAY7Q,EAAQ,CACpB,MAAOqR,EAAWC,EAAaC,GAAgBvR,EAAOwR,OActD,GAXAZ,GAAeU,EACfL,EAAqBI,EAAYE,EAM7BD,IACAF,EAAkBE,EAAcjB,EAAiBQ,GAGjDU,EACA,IAAK,IAAIxZ,EAAIuT,EAAU1a,OAAS,EAAGmH,GAAK,IAAKA,EAAG,CAC5C,MAAMQ,EAAQ+S,EAAUvT,GACxB,GAAIQ,GAASsY,EAAiB,CAG1B,GAAuB,OAAnBM,IAA4B5Y,EAAQsY,GAAmBR,EAAiBY,EACxE,MAEJE,EAAiB5Y,CACrB,CACJ,CAER,CAEA,IAAIkZ,EAAiB,GACjBC,EAA2B,GAG/B,IAAK,IAAI3Z,EAAI,EAAGA,EAAIuT,EAAU1a,SAAUmH,EAAG,CACvC,MAAMQ,EAAQ+S,EAAUvT,GAOxB,GAAIiQ,EAAgB/O,IAAIV,GAAQ,CAC5B,MAAMrD,EAAOzG,KAAKuV,OAAO,CAACzL,IACpBoZ,EAAWtD,GAAyBzW,IAAI1C,EAAK6D,MAAM,GAAI,IAE7D,QAAiB/E,IAAb2d,EAAwB,CAIxB,GAAsB,OAAlBpB,GAA0BoB,IAAapB,IAAkBJ,EAAmB,CAC5EW,EAAgBlf,KAAK6f,GACrB,MAAMG,EAAkBnjB,KAAKojB,0BAA0Bf,GAAiB,GAClEgB,EAAgBrjB,KAAKuV,OAAO4N,GAClCjB,EAAMzb,KAAO4c,EACbpB,EAAO9e,KAAK+e,GAGZG,EAAkB,GAClBW,EAAiB,GACjBd,EAAQF,GACZ,CAEAF,EAAgBI,EAAMgB,SAAWA,CACrC,CAGJ,MAAO,GAAIpZ,GAASsY,EAAiB,CAEjC,MAAMkB,GAAQxZ,EAAQsY,GAAmBR,EAAiBO,EACpDoB,GAAeC,EAAAA,EAAAA,IAAMF,EAAM,GAEjC,GAAuB,OAAnBZ,GAA2B5Y,GAAS4Y,EAMpCH,GAAO,OACJ,GAAIA,GAASF,EAAgBlgB,OAAS,GAAK2H,EAAQ6Y,EACtDJ,GAAO,OACJ,GAA2B,OAAvBL,EAAMuB,UAAU,GACvBvB,EAAMuB,UAAU,GAAKF,OAGrB,GAAIA,IAAiBrB,EAAMuB,UAAU,QAM9B,CACHvB,EAAMuB,UAAU,GAAKF,EAGrBlB,EAAgBlf,KAAK6f,GAEjBjB,GACAO,EAA0Bnf,KAAK8f,GAEnC,MAAOE,EAAiBO,GAA6B1jB,KAAKojB,0BACtDf,EAAiBC,GAGfe,EAAgBrjB,KAAKuV,OAAO4N,GAClCjB,EAAMzb,KAAO4c,EAETtB,IACAG,EAAMyB,MAAQ3jB,KAAK4jB,sBACfT,EAAiBO,EAA2B5B,IAIpDG,EAAO9e,KAAK+e,GAGZG,EAAkB,GAClBW,EAAiB,GACjBV,EAA4B,GAC5BW,EAA2B,GAC3Bf,EAAQF,GACZ,CAGR,MAMI,GAFAgB,EAAe7f,KAAK2G,GAEhBiY,EAAsB,CACtB,IAEI8B,EAFAC,GAAaN,EAAAA,EAAAA,IAAMf,EAAiBnZ,GAAK6Y,EAAa,GAItD0B,EADAva,EAAI,EAAImZ,EAAiBtgB,QACdqhB,EAAAA,EAAAA,IAAMf,EAAiBnZ,EAAI,GAAK6Y,EAAa,GAG7C,KAEfc,EAAyB9f,KAAK,CAAC2gB,EAAYD,GAC/C,CAGR,CAEA,GAAI,WAAYtS,EAAQ,CACpB,MAAOqR,EAAWC,EAAaC,GAAgBvR,EAAOwR,OACtDZ,GAAeS,EAAYE,CAC/B,CAGIE,EAAe7gB,OAAS,GACxBkgB,EAAgBlf,KAAK6f,GACjBjB,GACAO,EAA0Bnf,KAAK8f,IAE5BZ,EAAgB0B,OAAMvlB,GAAkB,IAAbA,EAAE2D,WAEpC+f,EAAQF,IACRK,EAAkB,GAClBW,EAAiB,GACjBV,EAA4B,GAC5BW,EAA2B,GAGnC,CAEA,GAAIZ,EAAgBlgB,OAAS,EAAG,CAC5B,GAAI0f,GAAwBH,EAGxB,MAAM,IAAIhhB,MACN,yLAMR,MAAOyiB,EAAiBO,GAA6B1jB,KAAKojB,0BAA0Bf,EAAiBC,GAG/Fe,EAAgBrjB,KAAKuV,OAAO4N,GAClCjB,EAAMzb,KAAO4c,EACTtB,IACAG,EAAMyB,MAAQ3jB,KAAK4jB,sBACfT,EAAiBO,EAA2B5B,IAGpDG,EAAO9e,KAAK+e,EAChB,CAEA,IAAI8B,EAAWvkB,OAAOqe,OAAO,MAG7B,MAAMmG,EAAYhC,EAAOhZ,KAAIiZ,GAASA,EAAMzb,OAAM/C,KAAK,IACvD,GAAIge,GAAqBC,EAAiB,CACtC,IAAK,IAAIrY,EAAI,EAAGA,EAAI2Y,EAAO9f,SAAUmH,EAAG,CACpC,MAAM4Y,EAAQD,EAAO3Y,GAChBoY,UACMQ,EAAiB,UAGvBP,UACMO,EAAgB,QAE/B,CACA,GAAIH,EAAsB,CACtB,IAAImC,EAAa,GACjB,IAAK,IAAIhC,KAASD,EACd,IAAK,IAAI/T,KAAQgU,EAAMyB,MACnBO,EAAW/gB,KAAK+K,GAGxB8V,EAAW,CAAE,OAAUE,EAC3B,MACIF,EAAW,CAAE,OAAU/B,EAE/B,CACA,MAAO,CAACgC,EAAWD,EAEvB,CASAZ,yBAAAA,CAA0B3B,GAA6C,IAAlC0C,EAAyBjiB,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,KAMzDkiB,EAAe3C,EAAU,GACzB4C,EAAaD,EAAajiB,OAC1BmiB,EAAgB,GAEpB,MAAMC,EAAgCvjB,MAAMC,QAAQkjB,IAA8BA,EAA0BhiB,OAAS,EACrH,IAAIqiB,EAAiCD,EAAgC,GAAK,KACtEE,EAAgCF,EAAgCJ,EAA0B,GAAK,KACnG,IAAK,IAAI7a,EAAI,EAAGA,EAAImY,EAAUtf,SAAUmH,EAAG,CACvC,MAAMob,EAAgBjD,EAAUnY,GAChC,IAAImS,EAAM,EACNkJ,EAAa,CAACN,EAAYA,EAAY,EAAG,GA+B7C,MAAMO,EAAcF,EAAcviB,OAClC,IAAK,IAAI0iB,EAAI,EAAGA,EAAIR,EAAaO,IAAeC,EAAG,CAC/C,MAAMC,EAAMD,EAAI,IACVE,EAAYrJ,KAAKD,IAAI,EAAG4I,EAAaQ,GACrCG,EAAWtJ,KAAKrQ,IAAIgZ,EAAYA,EAAaO,EAAcC,GAC3DI,EAAOb,EAAa9Z,MAAMya,EAAWC,GACrCE,EAAaxJ,KAAKD,IAAI,EAAGoJ,EAAIR,GAC7Bc,EAAYzJ,KAAKrQ,IAAIuZ,EAAaC,GAClCO,EAAQV,EAAcpa,MAAM4a,EAAYC,GAC9C,GAAIF,EAAK9iB,SAAWijB,EAAMjjB,OACtB,MAAM,IAAIzB,MAAM,6GAEpB,MAAM2kB,EAAUJ,EAAK9N,QAAO,CAACmO,EAAMC,IAAQD,IAASF,EAAMG,KAAMpjB,OAC1DqjB,EAAWH,EAAUR,EAAIC,EAC3BO,EAAU,GAAKG,EAAW/J,IAC1BA,EAAM+J,EACNb,EAAa,CAACI,EAAWC,EAAUE,EAAYC,GAEvD,CACA,MAAOJ,EAAWC,EAAUE,EAAYC,GAAaR,EAC/Cc,EAAU/J,KAAKgK,OAAOV,EAAWD,GAAa,GAC9CY,EAAWjK,KAAKgK,OAAOP,EAAYD,GAAc,GACvDZ,EAAcnhB,QAAQihB,EAAa9Z,MAAM,EAAGmb,IAC5CrB,EAAeM,EAAcpa,MAAMqb,GACnCtB,EAAaD,EAAajiB,OAEtBoiB,IACAC,EAA+BrhB,QAAQshB,EAA8Bna,MAAM,EAAGmb,IAC9EhB,EAAgCN,EAA0B7a,GAAGgB,MAAMqb,GAE3E,CAGA,OAFArB,EAAcnhB,QAAQihB,GAElBG,GACAC,EAA+BrhB,QAAQshB,GAChC,CAACH,EAAeE,IAEhB,CAACF,EAAe,GAE/B,CAGAV,qBAAAA,CAAsB/a,EAAQ4Z,EAAkBS,GAE5C,IAAKS,EAAO9W,EAAG+Y,GAAiB5lB,KAAK6lB,uBAAuBhd,EAAQqa,GAEhE4C,EAAU,GACd,IAAK,IAAIxc,EAAI,EAAGA,EAAIqa,EAAMxhB,SAAUmH,EAAG,CACnC,MAAMyc,EAAUH,EAActc,GAC9Bwc,EAAQ3iB,KAAK,CACTsD,KAAMkd,EAAMra,GACZma,UAAW,CACPhB,EAAiBsD,EAAQ7O,GAAG,IAAI,GAChCuL,EAAiBsD,EAAQ7O,IAAI,IAAI,KAG7C,CACA,OAAO4O,CACX,CAYAD,sBAAAA,CAAuBhd,EAAQqa,GAA6F,IAAA8C,EAAA,IAGpHrC,EAAOsC,EAAaL,EAHaM,EAAsBhkB,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,wBAAcikB,EAAmBjkB,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,wDAYlG,OAXAghB,EAAmB,QAAX8C,EAAG9C,SAAQ,IAAA8C,EAAAA,EAAI,UAInB,CAAC,UAAW,WAAY,OAAQ,MAAO,WAAWjZ,SAASmW,IAE1DS,EAAOsC,EAAaL,GAAiB5lB,KAAKomB,qBAAqBvd,IAE/D8a,EAAOsC,EAAaL,GAAiB5lB,KAAKqmB,oBAAoBxd,GAG5D7I,KAAKsmB,kBAAkB3C,EAAOsC,EAAaL,EAAeM,EAAwBC,EAC7F,CAGA5Q,MAAAA,CACIsH,EACAD,GAEA,IAAInW,EAcJ,OAZImW,GAAeA,EAAY2J,wBACvB1J,aAAqBb,EAAAA,KACrBa,EAAYzW,EAAuByW,IAEvCpW,EAAOzG,KAAKwmB,qBAAqB3J,EAAWD,IAE5CnW,EAAOoB,MAAM0N,OAAOsH,EAAWD,GAM5BnW,CACX,CAOA+f,oBAAAA,CAAqB3J,EAAWD,GAAa,IAAA6J,EACzC,MAAM7E,EAA4C,QAA9B6E,EAAc,OAAX7J,QAAW,IAAXA,OAAW,EAAXA,EAAagF,sBAAc,IAAA6E,EAAAA,EAAI,IAEhDrE,EAAkBphB,MAAM2L,KAAK3M,KAAKuZ,iBAAiBrC,IAAI,GAAK,EAElE,IAAIwP,EAAU,CAAC,IACf,IAAK,IAAI5c,KAAS+S,EACd,GAAI/S,GAASsY,EAAiB,CAC1B,IAAIqB,GAAa3Z,EAAQsY,GAAmBR,EAC5C6B,GAAYD,EAAAA,EAAAA,IAAMC,EAAW,GAC7BiD,EAAQvjB,KAAK,KAAD0C,OAAM4d,EAAS,OAC3BiD,EAAQvjB,KAAK,GACjB,MACIujB,EAAQA,EAAQvkB,OAAS,GAAGgB,KAAK2G,GAazC,OAVA4c,EAAUA,EAAQzd,KACd0d,GACqB,kBAANA,EACAA,EAEA9e,MAAM0N,OAAOoR,EAAG/J,KAK5B8J,EAAQhjB,KAAK,GACxB,CAQA0iB,oBAAAA,CAAqBvd,GACjB,MAAM+d,EAAe5mB,KAAKuV,OAAO1M,EAAQ,CAErC0d,wBAAwB,IAI5B,IAAI5C,EAAQ,GACRsC,EAAc,GACdL,EAAgB,GAChB5C,EAAiB,GACjB6D,EAAkB,GAClBC,EAAiB,EAErB,IAAK,IAAIC,EAAY,EAAGA,EAAYle,EAAO1G,SAAU4kB,EAAW,CAC5D,MAAMjd,EAAQjB,EAAOke,GAErB/D,EAAe7f,KAAK2G,GACpB+c,EAAgB1jB,KAAK4jB,GAErB,MAAM7J,EAAUld,KAAKuV,OAAOyN,EAAgB,CAExCuD,wBAAwB,IAGvBrJ,EAAQnQ,SApBQ,sBAoBsB6Z,EAAaE,EAAiB5J,EAAQ8J,QApB5D,aAqBjBrD,EAAMxgB,KAAK+Z,GACX+I,EAAY9iB,KAAK6f,GACjB4C,EAAcziB,KAAK0jB,GACnB7D,EAAiB,GACjB6D,EAAkB,GAClBC,GAAkB5J,EAAQ/a,OAGlC,CAEA,MAAO,CAACwhB,EAAOsC,EAAaL,EAChC,CAOAS,mBAAAA,CAAoBxd,GAEhB,IAAKoe,EAAUC,EAAqBC,GAAwBnnB,KAAKomB,qBAAqBvd,GAElF8a,EAAQ,GACRsC,EAAc,GACdL,EAAgB,GAEpB,MAAMwB,EAAmB,IAAI1hB,OAAO,KAADG,OAAMc,EAAiB,MAAM,MAEhE,IAAK,IAAI2C,EAAI,EAAGA,EAAI2d,EAAS9kB,SAAUmH,EAAG,CAEtC,MAAM+d,EAAUJ,EAAS3d,GACnBge,EAAiBJ,EAAoB5d,GACrCie,EAAkBJ,EAAqB7d,GAGvC9B,EAAU8f,EAAe,IAAMtnB,KAAKoZ,MAAMrR,cAAcoB,IAAI,iBAC5Dqe,EAAaH,EAAQnU,WAAW,KAChCuU,EAAUJ,EAAQvW,OAClB4W,EAAcN,EAAiB3I,KAAKgJ,GAE1C,GAAIjgB,GAAWggB,GAAcE,GAAgC,IAAjB/D,EAAMxhB,OAC9CwhB,EAAMxgB,KAAKkkB,GACXpB,EAAY9iB,KAAKmkB,GACjB1B,EAAcziB,KAAKokB,OAChB,CACH,MAAMI,EAAKhE,EAAMxhB,OAAS,EAC1BwhB,EAAMgE,IAAON,EACbpB,EAAY0B,GAAIxkB,QAAQmkB,GACxB1B,EAAc+B,GAAIxkB,QAAQokB,EAC9B,CACJ,CAEA,MAAO,CAAC5D,EAAOsC,EAAaL,EAEhC,CAWAU,iBAAAA,CAAkB3C,EAAO9a,EAAQkd,EAAS6B,EAAWC,GAEjD,IAAIC,EAAWC,gBAAgBpE,GAC3BqE,EAAYD,gBAAgBlf,GAC5Bof,EAAaF,gBAAgBhC,GAI7Bzc,EAAIwe,EAAS3lB,OAAS,EACtB0iB,EAAIiD,EAAS3lB,OAAS,EAE1B,KAAOmH,GAAK,GACJwe,EAASxe,GAAG4J,WAAW,MAAQ0U,EAAU7a,SAAS+a,EAASxe,GAAGwH,SAC9DgX,EAASjD,GAAKiD,EAASxe,GAAKwe,EAASjD,GACrCmD,EAAUnD,IAAKxQ,EAAAA,EAAAA,IAAY2T,EAAU1e,GAAI0e,EAAUnD,IACnDoD,EAAWpD,IAAKxQ,EAAAA,EAAAA,IAAY4T,EAAW3e,GAAI2e,EAAWpD,IACtDiD,EAASxe,GAAK,GACd0e,EAAU1e,GAAK,GACf2e,EAAW3e,GAAK,IAEhBub,EAAIvb,IAENA,EAMN,IAFAA,EAAI,EACJub,EAAI,EACGA,EAAIiD,EAAS3lB,SACX2lB,EAASxe,GAAGwM,SAAS,MAAQ+R,EAAS9a,SAAS+a,EAASjD,KACzDiD,EAASxe,IAAMwe,EAASjD,GACxBmD,EAAU1e,IAAK+K,EAAAA,EAAAA,IAAY2T,EAAU1e,GAAI0e,EAAUnD,IACnDoD,EAAW3e,IAAK+K,EAAAA,EAAAA,IAAY4T,EAAW3e,GAAI2e,EAAWpD,IACtDiD,EAASjD,GAAK,GACdmD,EAAUnD,GAAK,GACfoD,EAAWpD,GAAK,IAEhBvb,EAAIub,IAENA,EAGN,MAAO,CACHiD,EAAS3Q,QAAOpY,GAAKA,IACrBipB,EAAU7Q,QAAOpY,GAAKA,EAAEoD,OAAS,IACjC8lB,EAAW9Q,QAAOpY,GAAKA,EAAEoD,OAAS,IAE1C,CA2BA+lB,sBAAAA,GAIQ,IAJe,SACnBhF,EAAW,KAAI,KACfiF,EAAO,KAAI,cACXC,GAAgB,GACnBlmB,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAIGmmB,EAAqB,GAEzB,GAAInF,EAAU,CAEVA,EAAWA,EAASjS,cAGpB,IAAIqX,EAAgBzI,GAAiC1W,IAAI+Z,GAEzD,QAAsB3d,IAAlB+iB,EAA6B,CAG7B,IAAI1I,GAAyBpV,IAAI0Y,GAI1B,CAEH,MACMqF,EADuC,IAApBrF,EAAS/gB,OACDyd,GAAyBnc,OAASmc,GAAyB4I,SAE5F,MAAM,IAAI9nB,MAAM,aAADmF,OAAcqd,EAAQ,wCAAArd,OAAuC4iB,KAAKC,UAAUH,IAC/F,CARID,EAAgBpF,CASxB,CAEA,IAAIyF,EAAoB3oB,KAAKoZ,MAAMrR,cAAcoB,IAAI,KAADtD,OAAMyiB,EAAa,OACvE,QAA0B/iB,IAAtBojB,EACA,MAAM,IAAIjoB,MAAM,4BAADmF,OAA6ByiB,EAAa,oHAG7DD,EAAmBllB,KAAKwlB,EAC5B,MAEIN,EAAmBllB,KAAK,MAG5B,GAAIglB,EAAM,CAEN,GADAA,EAAOA,EAAKlX,cACC,eAATkX,GAAkC,cAATA,EACzB,MAAM,IAAIznB,MAAM,SAADmF,OAAUsiB,EAAI,oEAGjC,IAAIS,EAAgB5oB,KAAKoZ,MAAMrR,cAAcoB,IAAI,KAADtD,OAAMsiB,EAAI,OAC1D,QAAsB5iB,IAAlBqjB,EACA,MAAM,IAAIloB,MAAM,wBAADmF,OAAyBsiB,EAAI,oHAGhDE,EAAmBllB,KAAKylB,EAC5B,MAEIP,EAAmBllB,KAAK,MAG5B,GAAIilB,EAAe,CACf,IAAIS,EAAmB7oB,KAAKoZ,MAAMrR,cAAcoB,IAAI,oBACpD,QAAyB5D,IAArBsjB,EACA,MAAM,IAAInoB,MAAM,mJAGpB2nB,EAAmBllB,KAAK0lB,EAC5B,CAEA,OAAOR,EAAmBpf,KAAI,CAAClK,EAAGuK,IAAM,CAACA,EAAI,EAAGvK,KAAIoY,QAAOpY,GAAc,OAATA,EAAE,IAEtE,GAkGI+pB,iBAhGD,cAA+BpQ,KAiG9BqQ,cAhGD,cAA4BrQ,KAiG3BsQ,gBA1FD,cAA8BtQ,GAMjC7X,WAAAA,CAAY8X,EAAeC,GACvB/Q,MAAM8Q,EAAeC,GAErB5Y,KAAKue,cAAgB,iBAErBve,KAAKipB,yBAA2BjpB,KAAKoZ,MAAMtR,MAAMqP,QAC7CpY,GAAKiB,KAAKue,cAAcE,KAAK1f,KAGjC+G,QAAQC,KAAK,2JACjB,CAUAsW,YAAAA,CAAa5V,GACT,GAAa,OAATA,EAAe,OAAO,KAG1B,IAAKyiB,KAAcC,GAAa1iB,EAAKqK,OAAOpD,MAAM1N,KAAKue,eAEvD,GAAyB,IAArB4K,EAAUhnB,OAEV,OAAO0F,MAAMwU,aAAa6M,GAEvB,GAAyB,IAArBC,EAAUhnB,OAAc,CAE/B,IAAK+gB,EAAUzc,GAAQ0iB,EAKvB,OAHKnpB,KAAKipB,yBAAyBlc,SAASmW,IACxCpd,QAAQC,KAAK,8BAADF,OAA+Bqd,EAAQ,yEAAArd,OAAwE4iB,KAAKC,UAAU1oB,KAAKipB,6BAE5I5U,EAAAA,EAAAA,IAAY,CAAC6O,GAAWrb,MAAMwU,aAAa5V,GACtD,CACJ,GA8CI2iB,eAroCD,cAA6B/K,GAEhCxd,WAAAA,CAAY8X,EAAeC,GAAiB,IAAAyQ,EAGxC,MAAMC,EAAa,iDACbC,EAA2C,QAA9BF,EAAG1Q,EAAcQ,qBAAa,IAAAkQ,GAAkB,QAAlBA,EAA3BA,EAA6BhR,cAAc,UAAE,IAAAgR,OAAA,EAA7CA,EAA+ChkB,QACjEkkB,GAAiBA,EAAc/jB,QAAU,aAALK,OAAkByjB,EAAU,UAChEC,EAAc/jB,MAAQ,UAAHK,OAAayjB,EAAU,OAE9CzhB,MAAM8Q,EAAeC,EACzB,GA2nCI4Q,cAv/BD,cAA4B9Q,GAE/B7X,WAAAA,CAAY8X,EAAeC,GACvB/Q,MAAM8Q,EAAeC,GAErB5Y,KAAKue,cAAgB,2BACrBve,KAAKwe,eAAiBxe,KAAKsZ,eAAenC,QAAOpY,GAAKiB,KAAKue,cAAcE,KAAK1f,KAC9EiB,KAAK0e,cAAgB3f,GAAKA,CAC9B,CASA4f,yBAAAA,CAA0BC,EAAYC,EAAmBC,GACrD,OAAOH,GAA0B3e,KAAM4e,EAAYC,EAAmBC,EAC1E,GAq+BI2K,gBAx9BD,cAA8B/Q,GACjC7X,WAAAA,CAAY8X,EAAeC,GACvB/Q,MAAM8Q,EAAeC,GAErB5Y,KAAKue,cAAgB,mBACrBve,KAAKwe,eAAiBxe,KAAKsZ,eACtBnC,QAAOpY,GAAKiB,KAAKue,cAAcE,KAAK1f,KACpCkK,KAAIlK,GAAKA,EAAEuL,MAAM,GAAI,KAC1BtK,KAAK0e,cAAgB3f,GAAK,KAAJ8G,OAAS9G,EAAC,KACpC,CASA4f,yBAAAA,CAA0BC,EAAYC,EAAmBC,GACrD,OAAOH,GAA0B3e,KAAM4e,EAAYC,EAAmBC,EAC1E,GAq8BIE,kBACA0K,mBArkCD,cAAiC1K,KAskChC2K,oBApkCD,cAAkCjR,KAqkCjCkR,eApkCD,cAA6BlR,KAqkC5BmR,gBAnkCD,cAA8BnR,KAokC7BoR,iBAlkCD,cAA+BpR,KAmkC9BqR,aAjkCD,cAA2BrR,KAkkC1BsR,qBApDD,cAAmCtR,KAqDlCqH,uBACAkK,yBAjDD,cAAuClK,KAkDtCmK,kBAhDD,cAAgCxR,KAiD/ByR,gBA/CD,cAA8BzR,KAkD7BA,8DCh/HR,MAAM,iBAAE0R,GAAkBpO,OAAQqO,GAAU,IAAEC,IAAQC,GAAAA,KAMhDC,GACW,EADXA,GAEc,EAFdA,GAGO,EAHPA,GAIU,EAJVA,GAKW,EASXC,GAAqB,IAAIvkB,IACzBwkB,GAA8B,IAAIxkB,IAClCykB,GAA8B,IAAIzkB,IAWxCvB,eAAeimB,GAAiB/lB,EAA+BgmB,EAAU/lB,GAErE,IAAIgmB,EAAgB,QAAHjlB,OAAWglB,GAAQhlB,OAAGf,EAAQqb,UAAY,aAAe,GAAE,SACxE4K,QAAeC,EAAAA,EAAAA,IAAanmB,EAA+BimB,GAAe,EAAMhmB,GAEpF,IACI,aAAaslB,GAAiBtM,OAAOiN,EAAQ,CACzCE,mBAAkBA,GAAAA,GAE1B,CAAE,MAAOC,GAEL,GAAkC,IAA9BD,GAAAA,EAAmB9oB,QAA0C,SAA1B8oB,GAAAA,EAAmB,GACtD,MAAMC,EAQV,OALAplB,QAAQC,KAAKmlB,GACbplB,QAAQC,KACJ,wHAGSqkB,GAAiBtM,OAAOiN,EAAQ,CACzCE,mBAAoB,CAAC,SAE7B,CACJ,CA2DAtmB,eAAewmB,GAAWC,EAASnQ,GAC/B,MAAMoQ,EAlDV,SAAwBD,EAASnQ,GAK7B,MAAMoQ,EAAgB5rB,OAAOqe,OAAO,MAC9BwN,EAAgB,GACtB,IAAK,MAAMC,KAAaH,EAAQI,WAAY,CACxC,MAAMnlB,EAAS4U,EAAOsQ,GAIhBllB,aAAkB2V,EAAAA,GAOxBqP,EAAcE,GAAajB,GAAImB,KAAKC,MAAQrlB,EAAOslB,QAAUtlB,EANzDilB,EAAcnoB,KAAKooB,EAO3B,CACA,GAAID,EAAcnpB,OAAS,EACvB,MAAM,IAAIzB,MAAM,4EAADmF,OACiEylB,EAAc5nB,KAAK,MAAK,MAG5G,MAAMkoB,EAAoBnsB,OAAOgE,KAAKwX,GAAQ9Y,OACxC0pB,EAAkBT,EAAQI,WAAWrpB,OAC3C,GAAIypB,EAAoBC,EAAiB,CAGrC,IAAIC,EAAUrsB,OAAOgE,KAAKwX,GAAQ9D,QAAOoU,IAAcH,EAAQI,WAAWze,SAASwe,KACnFzlB,QAAQC,KAAK,2CAADF,OAA4C+lB,EAAiB,OAAA/lB,OAAMgmB,EAAe,8CAAAhmB,OAA6CimB,EAAQpoB,KAAK,MAAK,MACjK,CAEA,OAAO2nB,CACX,CAc0BU,CAAeX,EAASnQ,GAC9C,IAEI,IAAI1J,QAAe6Z,EAAQY,IAAIX,GAE/B,OADA9Z,EAAS0a,GAAe1a,GACjBA,CACX,CAAE,MAAOxR,GAIL,MAFA+F,QAAQomB,MAAM,8CAADrmB,OAA+C9F,EAAC,OAC7D+F,QAAQomB,MAAM,yBAA0Bb,GAClCtrB,CACV,CACJ,CAQA,SAASksB,GAAehmB,GACpB,IAAK,IAAIkmB,KAAQlmB,EACTA,EAAIkmB,aAAiB9B,GACrBpkB,EAAIkmB,GAAQ,IAAInQ,EAAAA,GAAO/V,EAAIkmB,IACC,kBAAdlmB,EAAIkmB,IAClBF,GAAehmB,EAAIkmB,IAG3B,OAAOlmB,CACX,CAUA,SAASmmB,GAAYC,GACjB,GAAIA,aAAiBrQ,EAAAA,GACjB,OAAOqQ,EAGX,GAAqB,IAAjBA,EAAMlqB,OACN,MAAMzB,MAAM,2BAGhB,GAAIM,MAAMC,QAAQorB,EAAM,IAAK,CAEzB,GAAIA,EAAMtQ,MAAKhd,GAAKA,EAAEoD,SAAWkqB,EAAM,GAAGlqB,SACtC,MAAMzB,MAAM,8KAGhB,OAAO,IAAIsb,EAAAA,GAAO,QACdC,cAActP,KAAK0f,EAAM1Z,OAAO1J,KAAIlK,GAAKmd,OAAOnd,MAChD,CAACstB,EAAMlqB,OAAQkqB,EAAM,GAAGlqB,QAEhC,CAEI,OAAO,IAAI6Z,EAAAA,GAAO,QACdC,cAActP,KAAK0f,EAAMpjB,KAAIlK,GAAKmd,OAAOnd,MACzC,CAAC,EAAGstB,EAAMlqB,QAGtB,CASA,SAASmqB,GAAqBjN,EAAMxW,GAAQ,IAAA0jB,EAAAC,EAGxC,IAAI3c,EAAuC,QAA3B0c,EAAGlN,EAAKxY,OAAOgJ,oBAAY,IAAA0c,EAAAA,EAAI,KAC3C5c,EAAuC,QAA3B6c,EAAGnN,EAAKxY,OAAO8I,oBAAY,IAAA6c,EAAAA,EAAI,MAC3C1P,EAAAA,EAAAA,IAAiBnN,KACjBA,EAAe,CAACA,IAGpB,IAAI8c,GAA2D,IAAlC5jB,EAAOme,QAAQnX,GACxC6c,EAA2D,OAAjB/c,IAA2BA,EAAa5C,SAAS8C,GAE/F,GAAI4c,GAA0BC,EAAwC,CAClE,IAAItO,EAAOnC,cAActP,KAGrB9D,EAAOuV,KAAKnV,KAAIlK,GAAKA,GAAK8Q,KAE9B,OAAO,IAAImM,EAAAA,GAAO,QAASoC,EAAMvV,EAAOvC,KAC5C,CACI,OAAOqmB,EAAAA,EAAAA,IAAU9jB,EAEzB,CAUA,SAAS+jB,GAAmBxB,EAASyB,EAAOC,GACxC,IAAK1B,EAAQI,WAAWze,SAAS,gBAAiB,OAElD,MAAMqR,EAAO,IAAInC,cAAc4Q,EAAMlR,eAAeyC,KAAKjc,QAGzD,IAAK,IAAImH,EAAI,EAAGA,EAAIujB,EAAMlR,eAAerV,KAAK,KAAMgD,EAAG,CACnD,IAAIW,EAAQX,EAAIujB,EAAMlR,eAAerV,KAAK,GACtCymB,EAAM7Q,OAAO,GACjB,IAAK,IAAI2I,EAAI,EAAGA,EAAIgI,EAAMlR,eAAerV,KAAK,KAAMue,EAAG,CACnD,MAAMvR,EAAQrJ,EAAQ4a,EACmB,KAArCgI,EAAMlR,eAAeyC,KAAK9K,GAC1B8K,EAAK9K,GAAS4I,OAAO,IAErBkC,EAAK9K,GAASyZ,EACdA,GAAOF,EAAMlR,eAAeyC,KAAK9K,GAEzC,CACJ,CAEAuZ,EAAMG,aAAe,IAAIhR,EAAAA,GAAO,QAASoC,EAAMyO,EAAMlR,eAAerV,MAEhEwmB,IACAD,EAAMG,aAAeH,EAAMG,aAAa1iB,MAAM,MAAO,GAAG2iB,YAAY,GAE5E,CAQA,SAASC,GAAW3pB,GAChB,OAAO,IAAIyY,EAAAA,GAAO,OAAQ,CAACzY,GAAQ,CAAC,GACxC,CAUAoB,eAAewoB,GAAe9N,EAAM+N,GAEhC,IAAI,gBAAEC,EAAe,gBAAEC,GAAoBF,EAEtCC,IAEDA,SAAyBE,GAAelO,EAAM+N,IAAeI,mBAEjE,IAAIC,EAAe,CACfrR,UAAWgR,EAAaM,kBACxBC,sBAAuBN,GAE3B,MAAMP,IAAqBQ,EAEvBjO,EAAKuO,uBAAuBpC,WAAWze,SAAS,sBAChD0gB,EAAaX,iBAAmBI,GAAWJ,IAG3CzN,EAAKuO,uBAAuBpC,WAAWze,SAAS,4BAChD0gB,EAAaI,uBAAyBT,EAAazR,gBAGvDiR,GAAmBvN,EAAKuO,uBAAwBH,EAAcX,GAC9DzN,EAAKyO,iBAAiBL,EAAcH,GAEpC,MAAMS,QAAuB5C,GAAW9L,EAAKuO,uBAAwBH,GACrE,IAAIO,EAASD,EAAeC,OAC5BV,EAAkBjO,EAAK4O,iBAAiBF,EAAgBT,GAGxD,MAAMY,EAAQ7O,EAAK8O,cAAcJ,GAEjC,OAAO,IAAIK,GAAgB,CAAEJ,SAAQV,kBAAiBD,qBAAoBa,GAC9E,CAWA,SAASG,GAAkBhP,EAAMiP,EAAeC,EAAmBC,GAAiB,IAAAC,EAAAzR,EAAA8C,EAAA4O,EAChF,IAAIC,EAAQ,GACRC,EAAS,EAGb,MAAMC,EAAsD,QAA/BJ,EAAGpP,EAAKwP,+BAAuB,IAAAJ,GAAAA,EAG5D,IAAIf,EAGiC,QAHhB1Q,EAE0B,QAF1B8C,EACkB,QADlB4O,EACjBH,EAAkBb,yBAAiB,IAAAgB,EAAAA,EAChCH,EAAkBO,8BAAsB,IAAAhP,EAAAA,EACxCyO,EAAkB7e,oBAAY,IAAAsN,EAAAA,EAC9BuR,EAAkB5e,aAIrB+d,aAA6B1R,EAAAA,GAC7B0R,EAAoBA,EAAkBnnB,SAASoM,OACvC3R,MAAMC,QAAQysB,KACtBA,EAAoB,CAACA,IAGzB,IAAK,IAAI7kB,KAAUylB,EAAe,CAI9BzlB,EAAOvC,KAAO,CAAC,KAAMuC,EAAOvC,MAG5B,IAAI2D,EAAQ,CACRgR,OAAQpS,EACRwkB,gBAAiB,KACjB0B,mBAAoB,KAEpBC,iBAAkBtB,EAClBpqB,MAAM,EACNgL,MAAO,EACPlH,GAAIwnB,KAGJC,IACA5kB,EAAM0R,eAAiB2Q,GAAqBjN,EAAMxW,IAGtD8lB,EAAMxrB,KAAK8G,EACf,CAEA,OAAO0kB,CACX,CAWAhqB,eAAesqB,GAAe5P,EAAM6P,GAAM,IAAAC,EACtC,MAAMC,EAAa/P,EAAKgQ,gBAExB,IAAI3B,EAAoBwB,EAAKF,iBACzBE,EAAKH,qBAGLrB,EAAoBA,EAAkBpjB,OAAO,IAIjD,IAAI8iB,EAAe,CACf,CAACgC,GAAaF,EAAKjU,OACnByS,kBAAmBtB,GAAYsB,GAC/BL,gBAAiB6B,EAAK7B,gBACtBC,gBAAwC,QAAzB6B,EAAED,EAAKH,0BAAkB,IAAAI,OAAA,EAAvBA,EAAyB7B,iBAE1C4B,EAAKvT,iBACLyR,EAAazR,eAAiBuT,EAAKvT,gBAIvC,IAAIpK,QAAe8N,EAAKiQ,QAAQlC,GAMhC,OAHA8B,EAAKH,mBAAqBxd,EAC1B2d,EAAK7B,gBAAkB9b,EAAO8b,gBAEvB9b,CACX,CAQA,SAASge,GAAkBL,EAAMM,GAC7BN,EAAKF,iBAAmB,IAAIE,EAAKF,iBAAkBQ,EACvD,CASA7qB,eAAe4oB,GAAelO,EAAM+N,GAChC,MAAMqC,EAAehwB,OAAOqe,OAAO,MACnC,IAAK,MAAMvc,KAAO8d,EAAK+L,QAAQI,WAC3BiE,EAAaluB,GAAO6rB,EAAa7rB,GAOrC,OALI8d,EAAK+L,QAAQI,WAAWze,SAAS,oBAAsB0iB,EAAatR,gBAGpED,GAAgBuR,SAEPtE,GAAW9L,EAAK+L,QAASqE,EAC1C,CAUA9qB,eAAe+qB,GAAerQ,EAAM+N,GAChC,IAAI,UAAEhR,EAAS,gBAAEkR,EAAe,eAAE3R,GAAmByR,EACjDK,EAAe,CACfrR,UAAWA,EACXT,eAA8B,OAAdA,QAAc,IAAdA,EAAAA,EAAkB2Q,GAAqBjN,EAAMjD,IAEjE,MAAM0Q,IAAqBQ,EAEvBjO,EAAK+L,QAAQI,WAAWze,SAAS,sBACjC0gB,EAAaX,iBAAmBI,GAAWJ,IAG/CF,GAAmBvN,EAAK+L,QAASqC,EAAcX,GAE/CzN,EAAKyO,iBAAiBL,EAAcH,GAEpC,IAAIS,QAAuB5C,GAAW9L,EAAK+L,QAASqC,GAEhDO,EAASD,EAAeC,OAG5B,OADAV,EAAkBjO,EAAK4O,iBAAiBF,EAAgBT,GACjD,CAAEU,SAAQV,kBACrB,CAYA,SAASqC,GAAkBtQ,EAAMiP,EAAeC,EAAmBC,EAAiBoB,GAChF,IAAIjB,EAAQ,GAERC,EAAS,EACb,IAAK,IAAI/lB,KAAUylB,EAAe,CAC9B,IAOIuB,EAPAb,EAAmBnmB,EAAOtC,SAAS0C,IAAI6mB,QAK3CjnB,EAAOvC,KAAO,CAAC,KAAMuC,EAAOvC,MAGxBspB,GACAC,EAAYD,EAAsBhB,GAClCiB,EAAUvpB,KAAO,CAAC,KAAMupB,EAAUvpB,OAGlCupB,EAAYvD,GAAqBjN,EAAMxW,GAG3C,IAAIoB,EAAQ,CACR8lB,MAAOlnB,EACPmnB,gBAAiBnnB,EACjB8S,eAAgBkU,EAChBd,mBAAoB,KAEpBC,iBAAkBA,EAClBiB,kBAAmBzB,EAEnBlrB,MAAM,EACNgL,MAAO,EACPlH,GAAIwnB,KAGRD,EAAMxrB,KAAK8G,EACf,CACA,OAAO0kB,CACX,CAeAhqB,eAAeurB,GAAe7Q,EAAM6P,GAAM,IAAAiB,EACtC,IAAIC,EAAe,IAAInU,cAAciT,EAAKF,iBAAiB7sB,QAAQyZ,KAAK,IAGpEwR,EAAe,CACfhR,UAAW8S,EAAKc,gBAChBrU,eAAgB,IAAIK,EAAAA,GAChB,QACAoU,EACA,CAAC,EAAGA,EAAajuB,SAErBmrB,gBAAwC,QAAzB6C,EAAEjB,EAAKH,0BAAkB,IAAAoB,OAAA,EAAvBA,EAAyB7C,iBAI1C/b,QAAe8N,EAAKiQ,QAAQlC,GAKhC,OAFA8B,EAAKH,mBAAqBxd,EAEnBA,CACX,CAQA,SAAS8e,GAAkBnB,EAAMM,GAC7BN,EAAKF,iBAAmB,IAAIE,EAAKF,iBAAkBQ,GACnDN,EAAKc,gBAAkB,IAAIhU,EAAAA,GAAO,QAAS,CAACE,OAAOsT,IAAc,CAAC,EAAG,GACzE,CAQO,MAAMc,WAAwB3oB,EAAAA,GAQjC9G,WAAAA,CAAYgG,EAAQukB,GAChBvjB,SAAQoR,EAAAA,EAAAA,GAAA,uBARM,aAUdjZ,KAAK6G,OAASA,EACd7G,KAAKorB,QAAUA,EAEf,MAAMmF,EAAY5F,GAA4BxhB,IAAInJ,KAAKa,aACjD2vB,EAAY/F,GAAmBthB,IAAIonB,GAEzCvwB,KAAKywB,cAAe,EACpBzwB,KAAK0wB,SAAW,KAChB1wB,KAAK2wB,eAAiB,KACtB3wB,KAAK4wB,YAAc,KACnB5wB,KAAK6wB,SAAW,KACZL,IAAchG,IACdxqB,KAAKywB,cAAe,EAEpBzwB,KAAK0wB,SAAWR,GAChBlwB,KAAK2wB,eAAiBhB,GACtB3vB,KAAK4wB,YAAcP,GACnBrwB,KAAK6wB,SAAWnB,IAETc,IAAchG,IAAuBgG,IAAchG,IAC1DxqB,KAAKywB,cAAe,EAEpBzwB,KAAK0wB,SAAWzB,GAChBjvB,KAAK2wB,eAAiBtC,GACtBruB,KAAK4wB,YAAcrB,GACnBvvB,KAAK6wB,SAAW1D,IAGhBntB,KAAK6wB,SAAWtD,EAKxB,CAOA,aAAMuD,GACF,MAAMC,EAAW,GACjB,IAAK,IAAIxvB,KAAO9B,OAAOgE,KAAKzD,MAAO,CAC/B,MAAMwU,EAAOxU,KAAKuB,GAEdiT,aAAgB4V,IAChB2G,EAAS5tB,KAAKqR,EAAKwc,QAAQF,UAEnC,CACA,aAAa9rB,QAAQC,IAAI8rB,EAC7B,CAiBA,4BAAapW,CAAgB9V,GAQrB,IARoD,UACxDsb,GAAY,EAAI,kBAChBvF,EAAoB,KAAI,OACxB/T,EAAS,KAAI,UACbgU,EAAY,KAAI,iBAChBC,GAAmB,EAAK,SACxBC,EAAW,OAAM,gBACjBkW,EAAkB,MACrB/uB,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAEG4C,EAAU,CACVqb,YACAvF,oBACA/T,SACAgU,YACAC,mBACAC,WACAkW,mBAGJ,MAAMV,EAAY5F,GAA4BxhB,IAAInJ,MAC5CwwB,EAAY/F,GAAmBthB,IAAIonB,GAEzC,IAAIxrB,EACuC,IAAAmsB,EAA3C,GAAIV,IAAchG,GACdzlB,QAAaC,QAAQC,IAAI,CACrBksB,GAAAA,EAAWxW,gBAAgB9V,EAA+BC,GAC1D8lB,GAAiB/lB,EAAsD,QAAzBqsB,EAAEpsB,EAAQmsB,uBAAe,IAAAC,EAAAA,EAAI,uBAAwBpsB,IACnGI,EAAAA,EAAAA,IAAaL,EAA+B,0BAA0B,EAAOC,UAG9E,GAAI0rB,IAAchG,IAAuBgG,IAAchG,GAC1DzlB,QAAaC,QAAQC,IAAI,CACrBksB,GAAAA,EAAWxW,gBAAgB9V,EAA+BC,GAC1D8lB,GAAiB/lB,EAA+B,gBAAiBC,GACjE8lB,GAAiB/lB,EAA+B,uBAAwBC,IACxEI,EAAAA,EAAAA,IAAaL,EAA+B,0BAA0B,EAAOC,UAG9E,GAAI0rB,IAAchG,GACrBzlB,QAAaC,QAAQC,IAAI,CACrBksB,GAAAA,EAAWxW,gBAAgB9V,EAA+BC,GAC1D8lB,GAAiB/lB,EAA+B,gBAAiBC,GACjE8lB,GAAiB/lB,EAA+B,uBAAwBC,SAGzE,KAAAssB,EACCZ,IAAchG,IACd1kB,QAAQC,KAAK,mBAADF,OAAoB0qB,EAAS,wIAE7CxrB,QAAaC,QAAQC,IAAI,CACrBksB,GAAAA,EAAWxW,gBAAgB9V,EAA+BC,GAC1D8lB,GAAiB/lB,EAAsD,QAAzBusB,EAAEtsB,EAAQmsB,uBAAe,IAAAG,EAAAA,EAAI,QAAStsB,IAE5F,CAGA,OAAO,IAAI9E,QAAQ+E,EACvB,CAOA,WAAM6D,CAAMwkB,GACR,aAAaptB,KAAKsvB,QAAQlC,EAC9B,CASA,aAAMkC,CAAQlC,GACV,aAAaptB,KAAK6wB,SAAS7wB,KAAMotB,EACrC,CAQAiE,qBAAAA,CACI9C,EACA+C,GAIF,IADEC,EAAgBrvB,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,KAEnB,MAAMsvB,EAAa,IAAIC,GAAAA,GAuFvB,GAtE6C,OAAzClD,EAAkBmD,oBAAwE,IAAzCnD,EAAkBmD,oBACnEF,EAAWruB,KAAK,IAAIwuB,GAAAA,GAAiCpD,EAAkBmD,qBAG5B,OAA3CnD,EAAkBqD,sBAAiCrD,EAAkBqD,qBAAuB,GAC5FJ,EAAWruB,KAAK,IAAI0uB,GAAAA,GAA6BtD,EAAkBqD,uBAc/B,OAApCrD,EAAkBuD,eAClBN,EAAWruB,KAAK,IAAI4uB,GAAAA,GAA0BxD,EAAkBuD,cAAevD,EAAkB5e,eAGhE,OAAjC4e,EAAkByD,YAA0D,OAAnCzD,EAAkB5e,cAAyB4e,EAAkByD,WAAa,GACnHR,EAAWruB,KAAK,IAAI8uB,GAAAA,GAAyB1D,EAAkByD,WAAYzD,EAAkB5e,eAGxD,OAArC4e,EAAkB2D,gBAA8D,OAAnC3D,EAAkB5e,cAAyB4e,EAAkB2D,eAAiB,GAC3HV,EAAWruB,KAAK,IAAIgvB,GAAAA,GAChBb,EACA/C,EAAkB2D,eAClB3D,EAAkB5e,eAYoB,OAA1C4e,EAAkB7O,qBAClB8R,EAAWruB,KAAK,IAAIivB,GAAAA,GAA8B7D,EAAkB7O,sBAG1B,OAA1C6O,EAAkB8D,qBAClBb,EAAWruB,KAAK,IAAImvB,GAAAA,GAChB/D,EAAkBjT,WAClBiT,EAAkB8D,sBAoBsB,OAA5C9D,EAAkBgE,sBAAgC,CAClD,IAAIC,EAAelB,EAAuB,GAA+C,OAA1C/C,EAAkB7O,oBAC3D4R,EACAA,EAAuB,EAEgB,OAAzC/C,EAAkBlG,qBAElBmK,GAAejE,EAAkBlG,mBAAmBkG,EAAkBlG,mBAAmBlmB,OAAS,GAAG,IAEzGqvB,EAAWruB,KAAK,IAAIsvB,GAAAA,GAAqClE,EAAkBgE,sBAAuBC,GACtG,CAeA,OAb6C,OAAzCjE,EAAkBlG,oBAClBmJ,EAAWruB,KAAK,IAAIuvB,GAAAA,EAA2BnE,EAAkBlG,qBAG5C,OAArBkJ,GACAC,EAAW/lB,OAAO8lB,GAQfC,CACX,CASAmB,sBAAAA,CAAuBpE,GAGnB,IAAIqE,EAAa,IAAIC,GAAAA,GAAiB7yB,KAAK6G,QAY3C,MATI,sBAAuB7G,MACvBP,OAAOC,OAAOkzB,EAAY5yB,KAAKuuB,mBAKT,OAAtBA,GACA9uB,OAAOC,OAAOkzB,EAAYrE,GAEvBqE,CACX,CAmBA,cAAME,CACF7X,GAMF,IAAA8X,EAAAC,EAAAC,EAAA,IAwBM3B,EA7BJ/C,EAAiBrsB,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,KACpBqvB,EAAgBrvB,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,MACnB,sBACI0tB,EAAwB,MAC3B1tB,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAEL,IAAKlC,KAAKywB,aAAc,KAAAyC,EAAAC,EAAAC,EACpB,MAAM7C,EAAY5F,GAA4BxhB,IAAInJ,KAAKa,aACvD,IAAIwyB,EAAe,4BAAHxtB,OAA+B0qB,EAAS,qFAExD,MAAMC,EAAYxwB,KAAK6G,OAAOysB,WACxBC,EAIF,QAJcL,EAEgD,QAFhDC,EACiC,QADjCC,EACdI,GAAiCrqB,IAAIqnB,UAAU,IAAA4C,EAAAA,EAC5CK,GAA6CtqB,IAAIqnB,UAAU,IAAA2C,EAAAA,EAC3DO,GAAyCvqB,IAAIqnB,UAChD,IAAA0C,EAAAA,EACGS,GAAqCxqB,IAAIqnB,GAMhD,MAJI+C,IAEAF,GAAgB,6CAAJxtB,OAAiD0tB,EAAa,GAAE,MAE1E7yB,MAAM2yB,EAChB,CAEA,KAAMpY,aAAkBe,EAAAA,OAAY4X,EAAAA,EAAAA,IAAa3Y,KAAYja,MAAMC,QAAQga,GACvE,MAAMva,MAAM,4DAADmF,OAA+DoV,EAAOpa,YAAYgzB,KAAI,OAOrG,GAAI7zB,KAAK6G,OAAOitB,mBAEZxC,EAAuB,OAMvB,GAHAA,EAAuBrW,aAAkBe,EAAAA,GAASf,EAAO3U,KAAK4Q,IAAI,GAAK+D,EAAO9Y,OAGjD,IAAzBmvB,EACA,MAAM5wB,MAAM,qDAKpB6tB,EAAoBvuB,KAAK2yB,uBAAuBpE,GAEhDgD,EAAmC,QAAnBwB,EAAGxB,SAAgB,IAAAwB,EAAAA,EAAI,IAAItB,GAAAA,GAG3CF,EAAmBvxB,KAAKqxB,sBACpB9C,EACA+C,EACAC,GAIJ,IAAIwC,EAAgBxF,EAAkB5e,aAChB,OAAlBokB,GAA2B/yB,MAAMC,QAAQ8yB,KACzCA,EAAgB,CAACA,IAMrB,IAAIvF,EAAkB,EACtB,MAAMwF,EAAkBxF,GAAmD,QAApCwE,EAAIzE,EAAkB0F,sBAAc,IAAAjB,EAAAA,EAAIkB,KAGzEC,EAAerE,OAAOsE,UAAU7F,EAAkBjT,aAA8D,QAAd,QAAjC2X,EAAC1E,EAAkB0F,sBAAc,IAAAhB,EAAAA,EAAI,MAC5G,IAAIoB,EAAUC,GAAAA,GAAQC,WAAWhG,GAG7BI,EAAQ3uB,KAAKw0B,cAAcvZ,EAAQsT,EAAmBC,EAAiBoB,GAE3E,KAAOjB,EAAM5S,MAAKhd,IAAMA,EAAEuE,QAASkrB,EAAkBwF,GAAiB,CAClE,IAAIS,EAAe,GACnB,IAAK,IAAIvF,KAAQP,EAAO,CACpB,GAAIO,EAAK5rB,KAAM,CAEXmxB,EAAatxB,KAAK+rB,GAClB,QACJ,CACA,GAAIiF,GAAgBjF,EAAKF,iBAAiB7sB,QAAUosB,EAAkBjT,WAAY,CAE9E4T,EAAK5rB,MAAO,EACZmxB,EAAatxB,KAAK+rB,GAClB,QACJ,CAGA,IAAI3d,QAAevR,KAAK00B,QAAQxF,GAG5BX,EAAkBoG,mBAClB30B,KAAK40B,oBAAoB1F,EAAM3d,GAE/Bgd,EAAkBsG,cAQtB,IAAI7G,EAASzc,EAAOyc,OAAO1jB,MAAM,MAAO,EAAG,MAG3CinB,EAAiBrC,EAAKF,iBAAkBhB,GAExC,IAAI8G,EAAgBT,EAAQrG,GAC5B,IAAK,IAAKwB,EAAYuF,KAAYD,EAAe,CAE7C,IAAIE,EAAU,IAAK9F,GAInBlvB,KAAKi1B,WAAWD,EAASxF,GAEzBwF,EAAQ1mB,OAASymB,EAEbhB,GAAiBA,EAAchnB,SAASyiB,KACxCwF,EAAQ1xB,MAAO,GAGnBmxB,EAAatxB,KAAK6xB,EACtB,CACJ,GACExG,EAGFiG,EAAez0B,KAAKk1B,WAAWT,GAAcxrB,KACzCksB,GAASA,EACJC,MAAK,CAACv1B,EAAGC,IAAMA,EAAEwO,MAAQzO,EAAEyO,QAC3BhE,MAAM,EAAGikB,EAAkB8G,aAIpC1G,EAAQ8F,EAAa9hB,OAGjB4b,EAAkB+G,mBAClB/G,EAAkB+G,kBAAkB3G,EAE5C,CAIA,MAAM4G,EAAev1B,KAAKk1B,WAAWvG,GAE/B6G,EAAgBj0B,GAAQg0B,EAAatsB,KACvC0T,GACQ4R,EAAkBkH,qBAAuB,EAClC9Y,EAAMrS,MAAM,EAAGikB,EAAkBkH,sBAAsBxsB,KAAIlK,GAAKA,EAAEwC,KAElE,CAACob,EAAM,GAAGpb,MAG3BoR,OAEI8O,EAAY+T,EAAa,oBAE/B,GAAIjH,EAAkBmH,wBAAyB,CAgB3C,MAAO,CACHjU,YAEAkU,mBANuBH,EAAa,sBAOpCI,iBANqBJ,EAAa,oBAQ1C,CACI,OAAO/T,CAEf,CAQAmT,mBAAAA,CAAoB1F,EAAM3d,GACtB,GAAIvR,KAAK6G,OAAOitB,mBAAoB,CAChC,IAAKviB,EAAOqkB,kBAAuD,IAAnCrkB,EAAOqkB,iBAAiBzzB,OACpD,MAAMzB,MACF,sKAIHwuB,EAAK0G,mBACN1G,EAAK0G,iBAAmB,IAE5B1G,EAAK0G,iBAAiBzyB,KAAKoO,EAAOqkB,iBACtC,CAEA,IAAKrkB,EAAOokB,oBAA2D,IAArCpkB,EAAOokB,mBAAmBxzB,OACxD,MAAMzB,MACF,wKAIHwuB,EAAKyG,qBACNzG,EAAKyG,mBAAqB,IAE9BzG,EAAKyG,mBAAmBxyB,KAAKoO,EAAOokB,mBACxC,CAQAT,UAAAA,CAAWvG,GAEP,MAAMkH,EAASp2B,OAAOqe,OAAO,MAC7B,IAAK,MAAM7X,KAAO0oB,OACSppB,IAAnBswB,EAAO5vB,EAAImB,IACXyuB,EAAO5vB,EAAImB,IAAM,CAACnB,GAElB4vB,EAAO5vB,EAAImB,IAAIjE,KAAK8C,GAI5B,OAAOxG,OAAO+oB,OAAOqN,EACzB,CASA5H,gBAAAA,CAAiBF,EAAgB+H,GAE7B,MAAMC,EAAOt2B,OAAOqe,OAAO,MAE3B,IAAK,MAAM+V,KAAQ9F,EACf,GAAI8F,EAAK3gB,WAAW,WAAY,CAC5B,IAAI8iB,EAAUnC,EAAK/wB,QAAQ,UAAW,mBAElCgzB,GAAiBjC,EAAK9mB,SAAS,WAI/BgpB,EAAKC,GAAWF,EAAcE,GAE9BD,EAAKC,GAAWjI,EAAe8F,EAEvC,CAEJ,OAAOkC,CACX,CAQA5H,aAAAA,CAAcJ,GACV,MAAMG,EAAQzuB,OAAOqe,OAAO,MAE5B,IAAK,MAAMmY,IAAY,CAAC,mBAAoB,sBAAuB,CAC/D,MAAM9nB,EAAS,GACf,IAAK,MAAM0lB,KAAQ9F,EACf,GAAI8F,EAAK3gB,WAAW+iB,GAAW,CAE3B9nB,EADc0lB,EAAKnmB,MAAM,KAAKqB,OACdgf,EAAe8F,EACnC,CAEJ3F,EAAM+H,GAAY9nB,CACtB,CACA,OAAO+f,CACX,CAQAJ,gBAAAA,CAAiBL,EAAcqI,GAC3B,GAAIA,EACAr2B,OAAOC,OAAO+tB,EAAcqI,OACzB,KAAAI,EAEH,MAAMC,EAAa,EAGnB,GAAIn2B,KAAK6G,OAAOitB,qBAA2C,QAAzBoC,EAAKl2B,KAAKo2B,uBAAe,IAAAF,GAAAA,GAAW,CAElE,IAAIG,EAAe,CAACF,EAAYn2B,KAAKs2B,kBAAmB,EAAGt2B,KAAKu2B,gBAE5DC,EAAe,CAACL,EAAYn2B,KAAKy2B,kBAAmB,EAAGz2B,KAAK02B,gBAEhE,IAAK,IAAIptB,EAAI,EAAGA,EAAItJ,KAAK22B,qBAAsBrtB,EAC3CmkB,EAAa,mBAAD5nB,OAAoByD,EAAC,iBAAkB,IAAI0S,EAAAA,GAAO,UAAW,GAAIqa,GAC7E5I,EAAa,mBAAD5nB,OAAoByD,EAAC,mBAAoB,IAAI0S,EAAAA,GAAO,UAAW,GAAIqa,GAC/E5I,EAAa,mBAAD5nB,OAAoByD,EAAC,iBAAkB,IAAI0S,EAAAA,GAAO,UAAW,GAAIwa,GAC7E/I,EAAa,mBAAD5nB,OAAoByD,EAAC,mBAAoB,IAAI0S,EAAAA,GAAO,UAAW,GAAIwa,EAEvF,MAAO,GAA+B,WAA3Bx2B,KAAK6G,OAAOysB,WAAyB,CAG5C,IAAIhtB,EAAO,CAAC6vB,EAAan2B,KAAK42B,UAAW,EAAG52B,KAAK62B,QAEjD,IAAK,IAAIvtB,EAAI,EAAGA,EAAItJ,KAAK82B,aAAcxtB,EACnCmkB,EAAa,mBAAD5nB,OAAoByD,EAAC,SAAU,IAAI0S,EAAAA,GAAO,UAAW,GAAI1V,GACrEmnB,EAAa,mBAAD5nB,OAAoByD,EAAC,WAAY,IAAI0S,EAAAA,GAAO,UAAW,GAAI1V,EAE/E,MAAO,GAAItG,KAAK6G,OAAOkwB,YAAa,CAEhC,IAAIzwB,EAAO,CAAC6vB,EAAan2B,KAAK42B,UAAW,EAAG,EAAI52B,KAAK62B,QAErD,IAAK,IAAIvtB,EAAI,EAAGA,EAAItJ,KAAK82B,aAAcxtB,EACnCmkB,EAAa,mBAAD5nB,OAAoByD,EAAC,eAAgB,IAAI0S,EAAAA,GAAO,UAAW,GAAI1V,EAEnF,MAAO,GAA+B,UAA3BtG,KAAK6G,OAAOysB,WAAwB,CAI3C,IAAI0D,EAAU,CAACb,EAAan2B,KAAK42B,UAAW52B,KAAK62B,OAAQ,GAErDI,EAAY,CAACd,EAAan2B,KAAK42B,UAAW,EAAG52B,KAAK62B,QAEtD,IAAK,IAAIvtB,EAAI,EAAGA,EAAItJ,KAAK82B,aAAcxtB,EACnCmkB,EAAa,mBAAD5nB,OAAoByD,EAAC,SAAU,IAAI0S,EAAAA,GAAO,UAAW,GAAIgb,GACrEvJ,EAAa,mBAAD5nB,OAAoByD,EAAC,WAAY,IAAI0S,EAAAA,GAAO,UAAW,GAAIib,EAE/E,KAAO,CAEH,IAAI3wB,EAAO,CAAC6vB,EAAYn2B,KAAK42B,UAAW,EAAG52B,KAAK62B,QAEhD,IAAK,IAAIvtB,EAAI,EAAGA,EAAItJ,KAAK82B,aAAcxtB,EACnCmkB,EAAa,mBAAD5nB,OAAoByD,EAAC,SAAU,IAAI0S,EAAAA,GAAO,UAAW,GAAI1V,GACrEmnB,EAAa,mBAAD5nB,OAAoByD,EAAC,WAAY,IAAI0S,EAAAA,GAAO,UAAW,GAAI1V,EAE/E,CACJ,CACJ,CAWAkuB,aAAAA,CAAclG,EAAeC,EAAmBC,EAAiBoB,GAC7D,OAAO5vB,KAAK2wB,eAAe3wB,KAAMsuB,EAAeC,EAAmBC,EAAiBoB,EACxF,CAQA,aAAM8E,CAAQxF,GACV,aAAalvB,KAAK0wB,SAAS1wB,KAAMkvB,EACrC,CAQA+F,UAAAA,CAAW/F,EAAMM,GACb,OAAOxvB,KAAK4wB,YAAY1B,EAAMM,EAClC,EAKG,MAAM0H,IAqBN,MAAMC,WAA4B7G,IAmElC,MAAM8G,WAAgC9G,IAyEtC,MAAM+G,WAA+B/G,IA0ErC,MAAMgH,WAAiChH,IAsEvC,MAAMiH,WAA+BjH,IAuErC,MAAMkH,WAAiClH,IAuEvC,MAAMmH,WAAkCnH,IAoExC,MAAMoH,WAA2BpH,IAyDjC,MAAMqH,WAAkCrH,IAmDxC,MAAMsH,WAA6BtH,IAuEnC,MAAMuH,WAAmCvH,IAwCzC,MAAMwH,WAA8BxH,IAwCpC,MAAMyH,WAA0BzH,IAsChC,MAAM0H,WAA8B1H,IAqCpC,MAAM2H,WAA2B3H,IAkCjC,MAAM4H,WAA4B5H,IAsDlC,MAAM6H,WAA6B7H,IA4EnC,MAAM8H,WAAkC9H,IAsCxC,MAAM+H,WAAuC/H,IAsC7C,MAAMgI,WAA+BhI,IAsErC,MAAMiI,WAA2BjI,IAsEjC,MAAMkI,WAAkClI,IAkExC,MAAMmI,WAA2BnI,IAgBjC,MAAMoI,WAA+BpI,IAuNrC,MAAMqI,WAAkCrI,GAU3CzvB,WAAAA,CAAYgG,EAAQukB,EAASwC,EAAwBW,GAAmB,IAAAqK,EACpE/wB,MAAMhB,EAAQukB,IAASnS,EAAAA,EAAAA,GAAA,uBAVT,gBAWdjZ,KAAK4tB,uBAAyBA,EAC9B5tB,KAAKuuB,kBAAoBA,EAGzB,MAAMsK,EAAgB74B,KAAK6G,OAAOiyB,QAC5BC,EAAgB/4B,KAAK6G,OAAOyQ,QAG5B0hB,EAAmBH,EAAcvF,YAEmB,QADxCsF,EACdK,GAAiC9vB,IAAI6vB,UAAiB,IAAAJ,EAAAA,EACnDM,GAAoC/vB,IAAI6vB,KAE3ClzB,QAAQC,KAAK,2BAADF,OAA4BmzB,EAAgB,wIAI5D,MAAMG,EAAe3F,GAAiCrqB,IAAI4vB,EAAczF,YACxE,IAAK6F,EACD,MAAM,IAAIz4B,MAAM,2EAADmF,OAA8E7F,KAAK6G,OAAOyQ,QAAQgc,WAAU,MAI/H,MAEMhc,EAAU,IAAI8hB,EAFMD,EAAa,IAEDJ,EAAenL,EAAwBW,GAE7EvuB,KAAKo2B,gBAAkB,uBAAwB9e,EAC3CtX,KAAKo2B,iBAELp2B,KAAK22B,mBAAqBrf,EAAQqf,mBAClC32B,KAAKy2B,kBAAoBnf,EAAQmf,kBACjCz2B,KAAK02B,eAAiBpf,EAAQof,eAE9B12B,KAAKq5B,mBAAqB/hB,EAAQ+hB,mBAClCr5B,KAAKs2B,kBAAoBhf,EAAQgf,kBACjCt2B,KAAKu2B,eAAiBjf,EAAQif,iBAI9Bv2B,KAAK82B,WAAaxf,EAAQwf,WAC1B92B,KAAK42B,UAAYtf,EAAQsf,UACzB52B,KAAK62B,OAASvf,EAAQuf,OAE9B,EAMG,MAAMyC,WAA4BhJ,IA0HlC,MAAMiJ,WAAmCjJ,IAQzC,MAAMkJ,WAA4BlJ,GAOrCzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GACzB1mB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAK42B,UAAY52B,KAAK6G,OAAO4yB,OAC7Bz5B,KAAK82B,WAAa92B,KAAK6G,OAAO6yB,QAC9B15B,KAAK62B,OAAS72B,KAAK6G,OAAO8yB,OAAS35B,KAAK42B,SAC5C,EAgBG,MAAMgD,WAA8BtJ,GAOvCzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GACzB1mB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAK42B,UAAY52B,KAAK6G,OAAO+vB,UAC7B52B,KAAK82B,WAAa92B,KAAK6G,OAAOiwB,WAC9B92B,KAAK62B,OAAS72B,KAAK6G,OAAOgzB,YAAc75B,KAAK42B,SACjD,EASG,MAAMkD,WAA+BxJ,GAOxCzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GACzB1mB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAK42B,UAAY52B,KAAK6G,OAAOkzB,oBAC7B/5B,KAAK82B,WAAa92B,KAAK6G,OAAOmzB,kBAC9Bh6B,KAAK62B,OAAS72B,KAAK6G,OAAOgzB,YAAc75B,KAAK42B,SACjD,EAUG,MAAMqD,WAA4B3J,GAOrCzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GACzB1mB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAK42B,UAAY52B,KAAK6G,OAAO4yB,OAC7Bz5B,KAAK82B,WAAa92B,KAAK6G,OAAO6yB,QAC9B15B,KAAK62B,OAAS72B,KAAK6G,OAAO8yB,OAAS35B,KAAK42B,SAC5C,EAWG,MAAMsD,WAAkC5J,GAO3CzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GACzB1mB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAK42B,UAAY52B,KAAK6G,OAAO4yB,OAC7Bz5B,KAAK82B,WAAa92B,KAAK6G,OAAO6yB,QAC9B15B,KAAK62B,OAAS72B,KAAK6G,OAAO8yB,OAAS35B,KAAK42B,SAC5C,EAUG,MAAMuD,WAA+B7J,GAOxCzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GACzB1mB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAK42B,UAAY52B,KAAK6G,OAAO4yB,OAC7Bz5B,KAAK82B,WAAa92B,KAAK6G,OAAO6yB,QAC9B15B,KAAK62B,OAAS72B,KAAK6G,OAAO8yB,OAAS35B,KAAK42B,SAC5C,EAoBG,MAAMwD,WAA6B9J,GAOtCzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GAAmB,IAAA8L,EAC5CxyB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAK42B,UAA2C,QAAlCyD,EAAGr6B,KAAK6G,OAAOyzB,2BAAmB,IAAAD,EAAAA,EAAIr6B,KAAK6G,OAAOkzB,oBAChE/5B,KAAK82B,WAAa92B,KAAK6G,OAAOmzB,kBAC9Bh6B,KAAK62B,OAAS72B,KAAK6G,OAAOgzB,YAAc75B,KAAK6G,OAAOkzB,mBACxD,EAaG,MAAMQ,WAA2BjK,GAOpCzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GACzB1mB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAK42B,UAAY52B,KAAK6G,OAAOkzB,oBAC7B/5B,KAAK82B,WAAa92B,KAAK6G,OAAOmzB,kBAC9Bh6B,KAAK62B,OAAS72B,KAAK6G,OAAOgzB,YAAc75B,KAAK42B,SACjD,EAgBG,MAAM4D,WAA6BlK,GAOtCzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GACzB1mB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAK42B,UAAY52B,KAAK6G,OAAO4yB,OAC7Bz5B,KAAK82B,WAAa92B,KAAK6G,OAAO6yB,QAC9B15B,KAAK62B,OAAS72B,KAAK6G,OAAOgzB,YAAc75B,KAAK42B,SACjD,EAgBG,MAAM6D,WAA2BnK,GAOpCzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GACzB1mB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAK42B,UAAY52B,KAAK6G,OAAO6zB,QAC7B16B,KAAK82B,WAAa92B,KAAK6G,OAAO8zB,SAC9B36B,KAAK62B,OAAS72B,KAAK6G,OAAO+zB,QAAU56B,KAAK42B,SAC7C,EAiBG,MAAMiE,WAA2BvK,GAOpCzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GACzB1mB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAK42B,UAAY52B,KAAK6G,OAAOkzB,oBAC7B/5B,KAAK82B,WAAa92B,KAAK6G,OAAOmzB,kBAC9Bh6B,KAAK62B,OAAS72B,KAAK6G,OAAOgzB,YAAc75B,KAAK42B,SACjD,EAeG,MAAMkE,WAA2BxK,IAajC,MAAMyK,WAAgCzK,IAoEtC,MAAM0K,WAAiC1K,IAevC,MAAM2K,WAA8B3K,IAOpC,MAAM4K,WAA4B5K,IAclC,MAAM6K,WAA4B7K,IAsBlC,MAAM8K,WAAkClE,GAO3Cr2B,WAAAA,CAAWw6B,GAAyB,IAAxB,OAAErN,EAAM,WAAEsN,GAAYD,EAC9BxzB,QACA7H,KAAKguB,OAASA,EACdhuB,KAAKs7B,WAAaA,CACtB,EAGG,MAAMC,WAA+BrE,GAOxCr2B,WAAAA,CAAW26B,GAAqC,IAApC,OAAExN,EAAM,WAAEsN,EAAU,WAAEG,GAAYD,EAC1C3zB,QACA7H,KAAKguB,OAASA,EACdhuB,KAAKs7B,WAAaA,EAClBt7B,KAAKy7B,WAAaA,CACtB,EAMG,MAAMC,WAA4BpL,IAiBlC,MAAMqL,WAA8BrL,IAsBpC,MAAMsL,WAA4BtL,IAalC,MAAMuL,WAA+BvL,IA2CrC,MAAMwL,WAA2BxL,IA+CjC,MAAMyL,WAA4BzL,IA+ClC,MAAM0L,WAAiC1L,IAiFvC,MAAM2L,WAAgC3L,IAsBtC,MAAM4L,WAAkC5L,IAqBxC,MAAM6L,WAA8B7L,IAsBpC,MAAM8L,WAA6B9L,IAWnC,MAAM+L,WAAmCnF,GAO5Cr2B,WAAAA,CAAWy7B,GAAyB,IAAxB,OAAEtO,EAAM,WAAEsN,GAAYgB,EAC9Bz0B,QACA7H,KAAKguB,OAASA,EACdhuB,KAAKs7B,WAAaA,CACtB,EAMG,MAAMiB,WAA2BjM,IACjC,MAAMkM,WAAiBD,GAO1B,WAAM3zB,CAAMwkB,GACR,OAAO,IAAIqP,SAAiC50B,MAAMe,MAAMwkB,GAC5D,EAOG,MAAMqP,WAAmCvF,GAM5Cr2B,WAAAA,CAAW67B,GAA6B,IAA5B,WAAEC,EAAU,WAAElB,GAAYiB,EAClC70B,QACA7H,KAAK28B,WAAaA,EAClB38B,KAAKy7B,WAAaA,CACtB,EAOG,MAAMmB,WAA8BtM,IA+BpC,MAAMuM,WAA8BvM,IAgCpC,MAAMwM,WAAgCxM,IAuHtC,MAAMyM,WAA6BzM,IA+DnC,MAAM0M,WAAgC1M,IAoDtC,MAAM2M,WAAgCD,GASzCn8B,WAAAA,CAAYgG,EAAQukB,EAASwC,EAAwBW,GACjD1mB,MAAMhB,EAAQukB,GACdprB,KAAK4tB,uBAAyBA,EAC9B5tB,KAAKuuB,kBAAoBA,EAEzBvuB,KAAK22B,mBAAqB32B,KAAK6G,OAAOq2B,eACtCl9B,KAAKy2B,kBAAoBz2B,KAAK6G,OAAOs2B,wBACrCn9B,KAAK02B,eAAiB12B,KAAK6G,OAAOgzB,YAAc75B,KAAKy2B,kBAErDz2B,KAAKq5B,mBAAqBr5B,KAAK6G,OAAOu2B,eACtCp9B,KAAKs2B,kBAAoBt2B,KAAK6G,OAAOw2B,wBACrCr9B,KAAKu2B,eAAiBv2B,KAAK6G,OAAOgzB,YAAc75B,KAAKs2B,iBACzD,CAuBA,qBAAMgH,CAAgBC,EAAcC,GAM5B,IANgD,UACpDC,EAAY,GAAG,YACfC,EAAc,EAAG,YACjBC,EAAc,GAAI,QAClBC,EAAU,MAEb17B,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAED,MAAMkrB,EAAe,CACjBhR,UAAWmhB,IAGT,gBAAElQ,EAAe,uBAAEQ,SAAiCN,GAAevtB,KAAMotB,GAEzE1uB,EAAI2uB,EAAgB/mB,KAAK,GAAKtG,KAAK6G,OAAOg3B,iBAC1CC,EAASpiB,KAAKgK,MAAMhnB,EAAIi/B,GACxBI,EAASriB,KAAKgK,MAAMhnB,EAAIg/B,GAExBM,EAAeh+B,KAAK6G,OAAOm3B,aAEjC,IAAIC,EAAmB,GACnB3Q,EAAkB,KAClB4Q,EAAkB,KAClB3Y,EAAM,EAEV,OAAa,GACPA,EAEF,MAAMuH,EAAmBI,KAAagR,GACtC,IAAIC,EAEAA,EADAD,EACkBA,EAAgBE,oBAEhB,IAAIpiB,EAAAA,GAClB,UACA,IAAIqiB,aAAaL,GACjB,CAAC,EAAG,EAAGA,IAGf,IAAIvQ,EAAe,CACfX,mBACAqR,kBACAtQ,uBAAwBA,EACxB2P,mBAAoBA,EACpB7P,sBAAuBN,GAG3BrtB,KAAK8tB,iBAAiBL,EAAcH,GACpC4Q,QAAwB/S,GAAWnrB,KAAK4tB,uBAAwBH,GAChEH,EAAkBttB,KAAKiuB,iBAAiBiQ,EAAiB5Q,GAEzD,MAAM,KAAEgR,EAAI,SAAEC,GAAaL,EAG3B,GAFAD,EAAiB96B,KAAKo7B,GAElBhZ,GAAOwY,IAEP/8B,MAAM2L,KAAK2xB,EAAKlgB,MAAMjH,QAAO3Y,GAAKA,GAAKi/B,IAAWt7B,OAAS,GAAKojB,GAAOuY,GAEvE,KAER,CAEA,MAAMU,GAAcC,EAAAA,EAAAA,IAAIR,IAClB,SAAES,SAAmBvT,GAAWyS,EAAQxS,QAAS,CAAEoT,gBAEzD,MAAO,CACHA,cACAE,WAGR,EAQG,MAAMC,WAAwBrO,GAAgBzvB,WAAAA,GAAA,SAAAqB,YAAA+W,EAAAA,EAAAA,GAAA,uBAC/B,cAAa,EAO5B,MAAM2lB,WAA6BtO,GAOtCzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GACzB1mB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAKq5B,mBAAqBr5B,KAAK22B,mBAAqB32B,KAAK6G,OAAOq2B,eAChEl9B,KAAKs2B,kBAAoBt2B,KAAKy2B,kBAAoBz2B,KAAK6G,OAAOs2B,wBAC9Dn9B,KAAKu2B,eAAiBv2B,KAAK02B,eAAiB12B,KAAK6G,OAAO+zB,QAAU56B,KAAKy2B,iBAC3E,EAgBG,MAAMoI,WAA+BvO,GAOxCzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GACzB1mB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAK42B,UAAY52B,KAAK6G,OAAOyzB,oBAC7Bt6B,KAAK82B,WAAa92B,KAAK6G,OAAOmzB,kBAC9Bh6B,KAAK62B,OAAS72B,KAAK6G,OAAOgzB,YAAc75B,KAAK6G,OAAOkzB,mBACxD,EAaG,MAAM+E,WAA8BxO,GAOvCzvB,WAAAA,CAAYgG,EAAQukB,EAASmD,GACzB1mB,MAAMhB,EAAQukB,GACdprB,KAAKuuB,kBAAoBA,EAGzBvuB,KAAK6G,OAAOgJ,aAAe7P,KAAK6G,OAAO8I,aAEvC3P,KAAK42B,UAAY52B,KAAK6G,OAAOkzB,oBAC7B/5B,KAAK82B,WAAa92B,KAAK6G,OAAOmzB,kBAC9Bh6B,KAAK62B,OAAS72B,KAAK6G,OAAOgzB,YAAc75B,KAAK6G,OAAOkzB,mBACxD,EAWG,MAAMgF,WAA4BzO,IAqFlC,MAAM0O,GAeT,4BAAarkB,CAAgB9V,GAQrB,IARoD,UACxDsb,GAAY,EAAI,kBAChBvF,EAAoB,KAAI,OACxB/T,EAAS,KAAI,UACbgU,EAAY,KAAI,iBAChBC,GAAmB,EAAK,SACxBC,EAAW,OAAM,gBACjBkW,EAAkB,MACrB/uB,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAEG4C,EAAU,CACVqb,YACAvF,oBACA/T,SACAgU,YACAC,mBACAC,WACAkW,mBAQJ,GANApqB,QAAesqB,GAAAA,EAAWxW,gBAAgB9V,EAA+BC,GACpEA,EAAQ+B,SAET/B,EAAQ+B,OAASA,IAGhB7G,KAAKi/B,qBACN,MAAM,IAAIv+B,MAAM,wEAA0EV,KAAK6zB,MAGnG,IAAK,IAAIqL,KAAuBl/B,KAAKi/B,qBAAsB,CACvD,MAAME,EAAYD,EAAoB/1B,IAAItC,EAAOysB,YACjD,GAAK6L,EAGL,aAAaA,EAAU,GAAGxkB,gBAAgB9V,EAA+BC,EAC7E,CAEA,GAAI9E,KAAKo/B,aAEL,OADAt5B,QAAQC,KAAK,wBAADF,OAAyBgB,EAAOysB,WAAU,sDACzChD,GAAgB3V,gBAAgB9V,EAA+BC,GAE5E,MAAMpE,MAAM,2BAADmF,OAA4BgB,EAAOysB,YAEtD,GAzDAra,EAAAA,EAAAA,GADS+lB,GAAe,uBAKM,OAE9B/lB,EAAAA,EAAAA,GAPS+lB,GAAe,gBAWF,GAkD1B,MAAM/F,GAAmC,IAAI/yB,IAAI,CAC7C,CAAC,OAAQ,CAAC,YArsGP,cAAwBixB,OAssG3B,CAAC,UAAW,CAAC,eApjGV,cAA2BE,OAqjG9B,CAAC,MAAO,CAAC,WArtFN,cAAuBK,OAstF1B,CAAC,WAAY,CAAC,gBAjoGX,cAA4BN,OAkoG/B,CAAC,YAAa,CAAC,iBA/+FZ,cAA6BE,OAg/FhC,CAAC,UAAW,CAAC,eA16FV,cAA2BC,OA26F9B,CAAC,aAAc,CAAC,iBAp2Fb,cAA6BC,OAq2FhC,CAAC,QAAS,CAAC,aA9mFR,cAAyBI,OA+mF5B,CAAC,SAAU,CAAC,cApgFT,cAA0BE,OAqgF7B,CAAC,aAAc,CAAC,kBApyFb,cAA8BL,OAqyFjC,CAAC,UAAW,CAAC,eAnqEV,cAA2Ba,OAoqE9B,CAAC,MAAO,CAAC,WA1lEN,cAAuBC,OA2lE1B,CAAC,cAAe,CAAC,kBAzhEd,cAA8BC,OA0hEjC,CAAC,OAAQ,CAAC,YA9JP,cAAwBuG,OA+J3B,CAAC,OAAQ,CAAC,YAxoDP,cAAwBzF,OAyoD3B,CAAC,eAAgB,CAAC,mBA1jDf,cAA+BC,OA2jDlC,CAAC,aAAc,CAAC,kBA9qFb,cAA8B5B,OA+qFjC,CAAC,cAAe,CAAC,mBArjFd,cAA+BE,OAsjFlC,CAAC,WAAY,CAAC,gBAhlBX,cAA4BiF,OAilB/B,CAAC,SAAU,CAAC,cA1hBT,cAA0BA,OA2hB7B,CAAC,QAAS,CAAC,aA3dR,cAAyBC,OA4d5B,CAAC,gCAAiC,CAAC,WA59DhC,cAAuBtE,OA89D1B,CAAC,OAAQ,CAAC,YAlnCP,cAAwB0C,OAmnC3B,CAAC,MAAO,CAAC,WAxuCN,cAAuBL,OAyuC1B,CAAC,YAAa,CAAC,iBAxpCZ,cAA6BE,OAypChC,CAAC,SAAU,CAAC,cA1oCT,cAA0BC,OA2oC7B,CAAC,OAAQ,CAAC,YApoCP,cAAwBC,OAqoC3B,CAAC,OAAQ,CAAC,YAjkCP,cAAwBQ,OAkkC3B,CAAC,WAAY,CAAC,gBAhzBX,cAA4BO,OAizB/B,CAAC,aAAc,CAAC,kBA3xBb,cAA8BC,OA4xBjC,CAAC,SAAU,CAAC,cAvwBT,cAA0BC,OAwwB7B,CAAC,SAAU,CAAC,cAhjCT,cAA0BR,OAijC7B,CAAC,OAAQ,CAAC,YA/hCP,cAAwBC,OAgiC3B,CAAC,UAAW,CAAC,eA/gCV,cAA2BC,OAghC9B,CAAC,aAAc,CAAC,iBAh0Bb,cAA6BG,OAi0BhC,CAAC,QAAS,CAAC,aA1vBR,cAAyBI,OA2vB5B,CAAC,MAAO,CAAC,WAv+BN,cAAuBN,OAw+B1B,CAAC,OAAQ,CAAC,YAz7BP,cAAwBC,OA27B3B,CAAC,UAAW,CAAC,kBAAmB4C,KAEhC,CAAC,MAAO,CAAC,WAAYnC,OAGnBtD,GAAsC,IAAIhzB,IAAI,CAChD,CAAC,KAAM,CAAC,UAjgFL,cAAsB6xB,OAkgFzB,CAAC,SAAU,CAAC,cAz9ET,cAA0BC,OA09E7B,CAAC,MAAO,CAAC,WAx7EN,cAAuBC,OAy7E1B,CAAC,OAAQ,CAAC,YAp5EP,cAAwBC,OAq5E3B,CAAC,QAAS,CAAC,aA/1ER,cAAyBC,OAg2E5B,CAAC,SAAU,CAAC,cA1sBT,cAA0ByE,OA2sB7B,CAAC,UAAW,CAAC,eA3+DV,cAA2BlE,OA4+D9B,CAAC,UAAW,CAAC,cA7qBV,cAA0BmE,OA8qB7B,CAAC,aAAc,CAAC,kBAvxEb,cAA8BzE,OAwxEjC,CAAC,mBAAoB,CAAC,uBAlvEnB,cAAmCC,SAsvEpCgH,GAAmC,IAAIn5B,IAAI,CAC7C,CAAC,QAAS,CAAC,aAz1CR,cAAyBs0B,OA01C5B,CAAC,OAAQ,CAAC,YA9kDP,cAAwBhB,OA+kD3B,CAAC,OAAQ,CAAC,YAz/CP,cAAwBS,OA0/C3B,CAAC,cAAe,CAAC,kBA99Cd,cAA8BC,OA+9CjC,CAAC,UAAW,CAAC,cAjjDV,cAA0BN,OAkjD7B,CAAC,WAAY,CAAC,eAxhDX,cAA2BE,OAyhD9B,CAAC,UAAW,CAAC,eAp8CV,cAA2BK,OAq8C9B,CAAC,QAAS,CAAC,aAh6CR,cAAyBC,OAi6C5B,CAAC,MAAO,CAAC,WAn4CN,cAAuBG,OAo4C1B,CAAC,MAAO,CAAC,WAj0CN,cAAuBE,OAk0C1B,CAAC,MAAO,CAAC,WAhyCN,cAAuBI,OAiyC1B,CAAC,UAAW,CAAC,eAhQV,cAA2BgE,OAiQ9B,CAAC,SAAU,CAAC,cAnOT,cAA0BC,SAsO3BpL,GAA2C,IAAIxtB,IAAI,CACrD,CAAC,WAAY,CAAC,0BAjcX,cAAsC82B,OAkczC,CAAC,UAAW,CAAC,kCA//DV,cAA8CtE,GAYjD73B,WAAAA,CAAYgG,EAAQukB,EAASwC,EAAwBW,GACjD1mB,MAAMhB,EAAQukB,IAASnS,EAAAA,EAAAA,GAAA,gCAXD,IAAKA,EAAAA,EAAAA,GAAA,uBACb,kBAWdjZ,KAAK4tB,uBAAyBA,EAC9B5tB,KAAKuuB,kBAAoBA,EAEzBvuB,KAAK22B,mBAAqB32B,KAAK6G,OAAOq2B,eACtCl9B,KAAKy2B,kBAAoBz2B,KAAK6G,OAAOs2B,wBACrCn9B,KAAK02B,eAAiB12B,KAAK6G,OAAO+zB,QAAU56B,KAAKy2B,kBAEjDz2B,KAAKq5B,mBAAqBr5B,KAAK6G,OAAOu2B,eACtCp9B,KAAKs2B,kBAAoBt2B,KAAK6G,OAAOw2B,wBACrCr9B,KAAKu2B,eAAiBv2B,KAAK6G,OAAO+zB,QAAU56B,KAAKs2B,iBACrD,CAmBA,cAAMxD,CACF7X,GASF,IAAAqkB,EAAAC,EAAA,IAREhR,EAAiBrsB,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,KACpBqvB,EAAgBrvB,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,KAqBnB,GAZAqsB,EAAoBvuB,KAAK2yB,uBAAuBpE,GAIb,QAAnCgR,GAAAD,EAAA/Q,GAAkB7M,yBAAiB,IAAA6d,IAAnCD,EAAkB5d,mBAAsB,GAIpC6M,EAAkB7M,oBAClB6P,EAAmB,CAAC,IAAIiO,GAAAA,GAAgCjR,KAGxDA,EAAkBkR,0BAClBlR,EAAkBoG,mBAAoB,EACtCpG,EAAkBmH,yBAA0B,EAEb,cAA3BnH,EAAkBpG,MAClBriB,QAAQC,KAAK,qEAGZwoB,EAAkBmR,iBACnB,MAAM,IAAIh/B,MACN,uNAMZ,MAAMgmB,QAAgB7e,MAAMirB,SAAS7X,EAAQsT,EAAmBgD,GAUhE,OARIhD,EAAkBkR,yBAA2BlR,EAAkBmR,kBAC/DhZ,EAA0B,iBAAI1mB,KAAK2/B,0BAC/BjZ,EACA6H,EAAkBmR,gBAClBnR,EAAkBqR,aAInBlZ,CACX,CAcAiZ,yBAAAA,CAA0BE,EAAkBH,GAA2D,IAA1CE,EAAU19B,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,KAAM0f,EAAc1f,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,IAC7F,IAAK29B,EAAiBjK,iBAClB,MAAM,IAAIl1B,MACN,4JAKR,IAAIo/B,EAAsB9/B,KAAK6G,OAAOi5B,yBACVv6B,IAAxBu6B,IACAh6B,QAAQC,KAAK,wEACb+5B,EAAsB,GAG1B,MAAMC,EAAkBF,EAAiBjK,iBAAiB3sB,KAAI0T,IAG1D,IAAIiZ,EAAmB50B,MAAM2L,KAAK,CAAExK,OAAQnC,KAAK6G,OAAOq2B,iBACpD,CAACrwB,EAAGvD,KAAMm1B,EAAAA,EAAAA,IAAI9hB,EAAM1T,KAAIlK,GAAKA,EAAEuK,KAAK,KAGpC02B,GAAUC,EAAAA,EAAAA,IAAMP,EAAgBz2B,KAAIi3B,IAAY,IAAV9hC,EAAG2D,GAAEm+B,EAC3C,OAAON,EACDhK,EAAiBx3B,GAAGkM,MAAM,KAAMvI,EAAG,KAAM,CAAC,EAAG69B,IAC7ChK,EAAiBx3B,GAAGkM,MAAM,KAAMvI,EAAE,KAE5Ci+B,EAAUA,EAAQG,UAAU,EAAG,EAAG,EAAG,GAErC,IAAKC,EAAKC,IAAkBC,EAAAA,EAAAA,IAASN,GAAU,EAAG,GAAG,GAGjDO,EAAkBP,EAAQrU,QAE9B,IAAK,IAAI9rB,EAAI,EAAGA,EAAI0gC,EAAgBj6B,KAAK,KAAMzG,EAAG,CAC9C,IAAI2gC,EAAUD,EAAgB1gC,GAE9B,IAAK,IAAIC,EAAI,EAAGA,EAAI0gC,EAAQl6B,KAAK,KAAMxG,EAAG,CACtC,IAAI2gC,EAAUD,EAAQ1gC,GAEtB,MAAM4gC,EAAYN,EAAIvgC,GAAGC,GAAG,GACtB6gC,EAAaN,EAAexgC,GAAGC,GAAG,GAExC,IAAK,IAAI+B,EAAI,EAAGA,EAAI4+B,EAAQn6B,KAAK,KAAMzE,EAAG,CAEtC,IAAI++B,EAAUH,EAAQ5+B,GACtB,IAAK,IAAID,EAAI,EAAGA,EAAIg/B,EAAQxiB,KAAKjc,SAAUP,EACvCg/B,EAAQxiB,KAAKxc,IAAMg/B,EAAQxiB,KAAKxc,GAAK++B,EAAWviB,KAAKxc,IAAM8+B,EAAUtiB,KAAKxc,GAI9Eg/B,EAAQxiB,KAAKjP,KAAI0xB,EAAAA,EAAAA,IAAaD,EAAQxiB,KAAM0hB,GAChD,CACJ,CACJ,CAIA,OADegB,EAAAA,EAAAA,IAAKP,EAAiB,EACxB,IAGXQ,EAAkB,CAAClB,EAAiBpe,UAAUtf,OAAQ09B,EAAiBpe,UAAU,GAAGtf,QAEpF6+B,EAAa,IAAIhlB,EAAAA,GACnB,UACA,IAAIqiB,aAAa0C,EAAgB,GAAKA,EAAgB,IACtDA,GAIJ,IAAK,IAAIE,EAAY,EAAGA,EAAYF,EAAgB,KAAME,EAAW,CAGjE,MAAMC,EAASnB,EAAgBkB,GAAWE,MAAMC,SAAS,GACzD,IAAKC,EAAcC,IAAgBC,EAAAA,EAAAA,IAAmBL,GAElDM,EAAQxgC,MAAM2L,KAAK,CAAExK,OAAQk/B,EAAal/B,OAAS,IAAK,CAACtD,EAAGyK,IAAM+3B,EAAa/3B,EAAI,GAAK+3B,EAAa/3B,KACrGm4B,GAAQptB,EAAAA,EAAAA,IAAY,CAAC,GAAImtB,GAAOv4B,KAAIlK,KAAOA,IAE3C2iC,EAAa,GACjB,IAAK,IAAIp4B,EAAI,EAAGA,EAAIm4B,EAAMt/B,SAAUmH,EAC5Bm4B,EAAMn4B,IACNo4B,EAAWv+B,KAAKm+B,EAAah4B,GAAKsY,GAI1Cof,EAAWC,GAAW7iB,KAAKjP,IAAIuyB,EAAY,EAC/C,CAEA,OAAOV,CACX,OA6zDEW,GAA8C,IAAIz7B,IAAI,CACxD,CAAC,WAAY,CAAC,0BAA2B+2B,OAGvC2E,GAAkD,IAAI17B,IAAI,CAC5D,CAAC,OAAQ,CAAC,gCAtwGP,cAA4CixB,GAO/C,WAAMvuB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KA8vGA,CAAC,UAAW,CAAC,mCArnGV,cAA+CiK,GAOlD,WAAMzuB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KA6mGA,CAAC,MAAO,CAAC,+BAtxFN,cAA2CsK,GAO9C,WAAM9uB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KA8wFA,CAAC,WAAY,CAAC,oCAlsGX,cAAgDgK,GAOnD,WAAMxuB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KA0rGA,CAAC,YAAa,CAAC,qCAhjGZ,cAAiDkK,GAOpD,WAAM1uB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAwiGA,CAAC,UAAW,CAAC,mCA3+FV,cAA+CmK,GAOlD,WAAM3uB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAm+FA,CAAC,aAAc,CAAC,qCAr6Fb,cAAiDoK,GAOpD,WAAM5uB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KA65FA,CAAC,QAAS,CAAC,iCA/qFR,cAA6CwK,GAOhD,WAAMhvB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAuqFA,CAAC,SAAU,CAAC,kCAxlFT,cAA8C0K,GAOjD,WAAMlvB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAglFA,CAAC,aAAc,CAAC,sCAp3Fb,cAAkDqK,GAOrD,WAAM7uB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KA42FA,CAAC,UAAW,CAAC,mCApuEV,cAA+CkL,GAOlD,WAAM1vB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KA4tEA,CAAC,MAAO,CAAC,+BA3pEN,cAA2CmL,GAO9C,WAAM3vB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAmpEA,CAAC,cAAe,CAAC,sCA1lEd,cAAkDoL,GAOrD,WAAM5vB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAklEA,CAAC,OAAQ,CAAC,gCAp6EP,cAA4C8K,GAO/C,WAAMtvB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KA45EA,CAAC,QAAS,CAAC,iCA/2ER,cAA6C+K,GAOhD,WAAMvvB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAu2EA,CAAC,aAAc,CAAC,sCA9uFb,cAAkDuK,GAOrD,WAAM/uB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAsuFA,CAAC,cAAe,CAAC,uCA7nFd,cAAmDyK,GAOtD,WAAMjvB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,OAunFE0U,GAA+C,IAAI57B,IAAI,CACzD,CAAC,OAAQ,CAAC,6BA3wGP,cAAyCixB,GAO5C,WAAMvuB,CAAMwkB,GACR,OAAO,IAAI2U,SAA4Bl6B,MAAMe,MAAMwkB,GACvD,KAmwGA,CAAC,UAAW,CAAC,gCA1nGV,cAA4CiK,GAO/C,WAAMzuB,CAAMwkB,GACR,OAAO,IAAI2U,SAA4Bl6B,MAAMe,MAAMwkB,GACvD,KAknGA,CAAC,MAAO,CAAC,4BA1xFN,cAAwCsK,GAO3C,WAAM9uB,CAAMwkB,GACR,OAAO,IAAI2U,SAA4Bl6B,MAAMe,MAAMwkB,GACvD,KAkxFA,CAAC,WAAY,CAAC,iCAtsGX,cAA6CgK,GAOhD,WAAMxuB,CAAMwkB,GACR,OAAO,IAAI2U,SAA4Bl6B,MAAMe,MAAMwkB,GACvD,KA8rGA,CAAC,YAAa,CAAC,kCArjGZ,cAA8CkK,GAOjD,WAAM1uB,CAAMwkB,GACR,OAAO,IAAI2U,SAA4Bl6B,MAAMe,MAAMwkB,GACvD,KA6iGA,CAAC,UAAW,CAAC,gCAh/FV,cAA4CmK,GAO/C,WAAM3uB,CAAMwkB,GACR,OAAO,IAAI2U,SAA4Bl6B,MAAMe,MAAMwkB,GACvD,KAw+FA,CAAC,aAAc,CAAC,kCA16Fb,cAA8CoK,GAOjD,WAAM5uB,CAAMwkB,GACR,OAAO,IAAI2U,SAA4Bl6B,MAAMe,MAAMwkB,GACvD,KAk6FA,CAAC,QAAS,CAAC,8BAprFR,cAA0CwK,GAO7C,WAAMhvB,CAAMwkB,GACR,OAAO,IAAI2U,SAA4Bl6B,MAAMe,MAAMwkB,GACvD,KA4qFA,CAAC,aAAc,CAAC,mCAx3Fb,cAA+CqK,GAOlD,WAAM7uB,CAAMwkB,GACR,OAAO,IAAI2U,SAA4Bl6B,MAAMe,MAAMwkB,GACvD,KAg3FA,CAAC,UAAW,CAAC,gCAxuEV,cAA4CkL,GAO/C,WAAM1vB,CAAMwkB,GACR,OAAO,IAAI2U,SAA4Bl6B,MAAMe,MAAMwkB,GACvD,KAguEA,CAAC,MAAO,CAAC,4BA/pEN,cAAwCmL,GAO3C,WAAM3vB,CAAMwkB,GACR,OAAO,IAAI2U,SAA4Bl6B,MAAMe,MAAMwkB,GACvD,KAupEA,CAAC,cAAe,CAAC,mCA9lEd,cAA+CoL,GAOlD,WAAM5vB,CAAMwkB,GACR,OAAO,IAAI2U,SAA4Bl6B,MAAMe,MAAMwkB,GACvD,OAwlEEqG,GAA+C,IAAIvtB,IAAI,CACzD,CAAC,KAAM,CAAC,6BAtkFL,cAAyC6xB,GAS5Cl3B,WAAAA,CAAYgG,EAAQukB,EAASwC,EAAwBW,GACjD1mB,MAAMhB,EAAQukB,GACdprB,KAAK4tB,uBAAyBA,EAC9B5tB,KAAKuuB,kBAAoBA,EAEzBvuB,KAAK22B,mBAAqB32B,KAAK6G,OAAO8vB,mBACtC32B,KAAKy2B,kBAAoBz2B,KAAK6G,OAAO+vB,UACrC52B,KAAK02B,eAAiB12B,KAAK6G,OAAOm7B,KAElChiC,KAAKq5B,mBAAqBr5B,KAAK6G,OAAOiwB,WACtC92B,KAAKs2B,kBAAoBt2B,KAAK6G,OAAO+vB,UACrC52B,KAAKu2B,eAAiBv2B,KAAK6G,OAAOm7B,IACtC,KAkjFA,CAAC,SAAU,CAAC,iCA9hFT,cAA6ChK,GAQhDn3B,WAAAA,CAAYgG,EAAQukB,EAASwC,EAAwBW,GACjD1mB,MAAMhB,EAAQukB,GACdprB,KAAK4tB,uBAAyBA,EAC9B5tB,KAAKuuB,kBAAoBA,EAEzBvuB,KAAK22B,mBAAqB32B,KAAK6G,OAAO8vB,mBACtC32B,KAAKy2B,kBAAoBz2B,KAAK6G,OAAO+vB,UACrC52B,KAAK02B,eAAiB12B,KAAK6G,OAAOm7B,KAElChiC,KAAKq5B,mBAAqBr5B,KAAK6G,OAAOiwB,WACtC92B,KAAKs2B,kBAAoBt2B,KAAK6G,OAAO+vB,UACrC52B,KAAKu2B,eAAiBv2B,KAAK6G,OAAOm7B,IACtC,KA2gFA,CAAC,MAAO,CAAC,8BA7/EN,cAA0C/J,GAS7Cp3B,WAAAA,CAAYgG,EAAQukB,EAASwC,EAAwBW,GACjD1mB,MAAMhB,EAAQukB,GACdprB,KAAK4tB,uBAAyBA,EAC9B5tB,KAAKuuB,kBAAoBA,EAEzBvuB,KAAK22B,mBAAqB32B,KAAK6G,OAAO8vB,mBACtC32B,KAAKy2B,kBAAoBz2B,KAAK6G,OAAO+vB,UACrC52B,KAAK02B,eAAiB12B,KAAK6G,OAAOm7B,KAElChiC,KAAKq5B,mBAAqBr5B,KAAK6G,OAAOiwB,WACtC92B,KAAKs2B,kBAAoBt2B,KAAK6G,OAAO+vB,UACrC52B,KAAKu2B,eAAiBv2B,KAAK6G,OAAOm7B,IACtC,KAy+EA,CAAC,OAAQ,CAAC,+BAz9EP,cAA2C9J,GAS9Cr3B,WAAAA,CAAYgG,EAAQukB,EAASwC,EAAwBW,GACjD1mB,MAAMhB,EAAQukB,GACdprB,KAAK4tB,uBAAyBA,EAC9B5tB,KAAKuuB,kBAAoBA,EAEzBvuB,KAAK22B,mBAAqB32B,KAAK6G,OAAOq2B,eACtCl9B,KAAKy2B,kBAAoBz2B,KAAK6G,OAAOs2B,wBACrCn9B,KAAK02B,eAAiB12B,KAAK6G,OAAO+zB,QAAU56B,KAAKy2B,kBAEjDz2B,KAAKq5B,mBAAqBr5B,KAAK6G,OAAOu2B,eACtCp9B,KAAKs2B,kBAAoBt2B,KAAK6G,OAAOw2B,wBACrCr9B,KAAKu2B,eAAiBv2B,KAAK6G,OAAO+zB,QAAU56B,KAAKs2B,iBACrD,KAq8EA,CAAC,QAAS,CAAC,gCAp6ER,cAA4C6B,GAS/Ct3B,WAAAA,CAAYgG,EAAQukB,EAASwC,EAAwBW,GACjD1mB,MAAMhB,EAAQukB,GACdprB,KAAK4tB,uBAAyBA,EAC9B5tB,KAAKuuB,kBAAoBA,EAEzBvuB,KAAK22B,mBAAqB32B,KAAK6G,OAAOq2B,eACtCl9B,KAAKy2B,kBAAoBz2B,KAAK6G,OAAOs2B,wBACrCn9B,KAAK02B,eAAiB12B,KAAK6G,OAAO+zB,QAAU56B,KAAKy2B,kBAEjDz2B,KAAKq5B,mBAAqBr5B,KAAK6G,OAAOu2B,eACtCp9B,KAAKs2B,kBAAoBt2B,KAAK6G,OAAOw2B,wBACrCr9B,KAAKu2B,eAAiBv2B,KAAK6G,OAAO+zB,QAAU56B,KAAKs2B,iBACrD,KAg5EA,CAAC,SAAU,CAAC,gBAlxBT,cAA4BsG,GAS/B/7B,WAAAA,CAAYgG,EAAQukB,EAASwC,EAAwBW,GACjD1mB,MAAMhB,EAAQukB,GACdprB,KAAK4tB,uBAAyBA,EAC9B5tB,KAAKuuB,kBAAoBA,EAEzBvuB,KAAK22B,mBAAqB32B,KAAK6G,OAAOq2B,eACtCl9B,KAAKy2B,kBAAoBz2B,KAAK6G,OAAOs2B,wBACrCn9B,KAAK02B,eAAiB12B,KAAK6G,OAAO+zB,QAAU56B,KAAKy2B,kBAEjDz2B,KAAKq5B,mBAAqBr5B,KAAK6G,OAAOu2B,eACtCp9B,KAAKs2B,kBAAoBt2B,KAAK6G,OAAOw2B,wBACrCr9B,KAAKu2B,eAAiBv2B,KAAK6G,OAAO+zB,QAAU56B,KAAKs2B,iBACrD,KA8vBA,CAAC,UAAW,CAAC,iCApvBV,cAA6CuG,GAShDh8B,WAAAA,CAAYgG,EAAQukB,EAASwC,EAAwBW,GACjD1mB,MAAMhB,EAAQukB,GACdprB,KAAK4tB,uBAAyBA,EAC9B5tB,KAAKuuB,kBAAoBA,EAEzBvuB,KAAK22B,mBAAqB32B,KAAK6G,OAAOq2B,eACtCl9B,KAAKy2B,kBAAoBz2B,KAAK6G,OAAOs2B,wBACrCn9B,KAAK02B,eAAiB12B,KAAK6G,OAAO+zB,QAAU56B,KAAKy2B,kBAEjDz2B,KAAKq5B,mBAAqBr5B,KAAK6G,OAAOu2B,eACtCp9B,KAAKs2B,kBAAoBt2B,KAAK6G,OAAOw2B,wBACrCr9B,KAAKu2B,eAAiBv2B,KAAK6G,OAAO+zB,QAAU56B,KAAKs2B,iBACrD,KAguBA,CAAC,aAAc,CAAC,qCA31Eb,cAAiD8B,GASpDv3B,WAAAA,CAAYgG,EAAQukB,EAASwC,EAAwBW,GACjD1mB,MAAMhB,EAAQukB,GACdprB,KAAK4tB,uBAAyBA,EAC9B5tB,KAAKuuB,kBAAoBA,EAEzBvuB,KAAK22B,mBAAqB32B,KAAK6G,OAAOq2B,eACtCl9B,KAAKy2B,kBAAoBz2B,KAAK6G,OAAOs2B,wBACrCn9B,KAAK02B,eAAiB12B,KAAK6G,OAAO+zB,QAAU56B,KAAKy2B,kBAEjDz2B,KAAKq5B,mBAAqBr5B,KAAK6G,OAAOu2B,eACtCp9B,KAAKs2B,kBAAoBt2B,KAAK6G,OAAOw2B,wBACrCr9B,KAAKu2B,eAAiBv2B,KAAK6G,OAAO+zB,QAAU56B,KAAKs2B,iBACrD,KAu0EA,CAAC,mBAAoB,CAAC,0CAtzEnB,cAAsD+B,GASzDx3B,WAAAA,CAAYgG,EAAQukB,EAASwC,EAAwBW,GACjD1mB,MAAMhB,EAAQukB,GACdprB,KAAK4tB,uBAAyBA,EAC9B5tB,KAAKuuB,kBAAoBA,EAEzBvuB,KAAK22B,mBAAqB32B,KAAK6G,OAAOq2B,eACtCl9B,KAAKy2B,kBAAoBz2B,KAAK6G,OAAOs2B,wBACrCn9B,KAAK02B,eAAiB12B,KAAK6G,OAAO+zB,QAAU56B,KAAKy2B,kBAEjDz2B,KAAKq5B,mBAAqBr5B,KAAK6G,OAAOu2B,eACtCp9B,KAAKs2B,kBAAoBt2B,KAAK6G,OAAOw2B,wBACrCr9B,KAAKu2B,eAAiBv2B,KAAK6G,OAAO+zB,QAAU56B,KAAKs2B,iBACrD,OAoyEE9C,GAAmC,IAAIttB,IAAI,CAC7C,CAAC,QAAS,CAAC,mBA55CR,cAA+Bs0B,OA65ClC,CAAC,OAAQ,CAAC,kBAjpDP,cAA8BhB,OAkpDjC,CAAC,OAAQ,CAAC,kBA/jDP,cAA8BS,OAgkDjC,CAAC,cAAe,CAAC,wBApiDd,cAAoCC,OAqiDvC,CAAC,UAAW,CAAC,oBAvnDV,cAAgCN,OAwnDnC,CAAC,WAAY,CAAC,qBA9lDX,cAAiCE,OA+lDpC,CAAC,UAAW,CAAC,qBAvgDV,cAAiCK,OAwgDpC,CAAC,QAAS,CAAC,mBAt+CR,cAA+BC,OAu+ClC,CAAC,MAAO,CAAC,iBAz8CN,cAA6BG,OA08ChC,CAAC,MAAO,CAAC,iBAp4CN,cAA6BE,OAq4ChC,CAAC,MAAO,CAAC,iBAn2CN,cAA6BI,OAo2ChC,CAAC,QAAS,CAAC,mBA94ER,cAA+B1C,GAOlCt3B,WAAAA,CAAYgG,EAAQ+mB,EAAwBW,GACxC1mB,MAAMhB,EAAQ+mB,GACd5tB,KAAKuuB,kBAAoBA,EAEzBvuB,KAAK22B,mBAAqB32B,KAAK6G,OAAOq2B,eACtCl9B,KAAKy2B,kBAAoBz2B,KAAK6G,OAAOs2B,wBACrCn9B,KAAK02B,eAAiB12B,KAAK6G,OAAO+zB,QAAU56B,KAAKy2B,kBAEjDz2B,KAAKq5B,mBAAqBr5B,KAAK6G,OAAOu2B,eACtCp9B,KAAKs2B,kBAAoBt2B,KAAK6G,OAAOw2B,wBACrCr9B,KAAKu2B,eAAiBv2B,KAAK6G,OAAO+zB,QAAU56B,KAAKs2B,iBACrD,KA63EA,CAAC,UAAW,CAAC,qBAvUV,cAAiCuI,OAwUpC,CAAC,SAAU,CAAC,oBA1ST,cAAgCC,OA2SnC,CAAC,QAAS,CAAC,mBAzWR,cAA+BF,SA4WhCqD,GAAoC,IAAI/7B,IAAI,CAC9C,CAAC,OAAQ,CAAC,kBAt1GP,cAA8BixB,GAOjC,WAAMvuB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KA80GA,CAAC,UAAW,CAAC,qBArsGV,cAAiCiK,GAOpC,WAAMzuB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KA6rGA,CAAC,MAAO,CAAC,iBAt2FN,cAA6BsK,GAOhC,WAAM9uB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KA81FA,CAAC,WAAY,CAAC,sBAlxGX,cAAkCgK,GAOrC,WAAMxuB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KA0wGA,CAAC,YAAa,CAAC,uBAhoGZ,cAAmCkK,GAOtC,WAAM1uB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KAwnGA,CAAC,UAAW,CAAC,qBA3jGV,cAAiCmK,GAOpC,WAAM3uB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KAmjGA,CAAC,aAAc,CAAC,uBAr/Fb,cAAmCoK,GAOtC,WAAM5uB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KA6+FA,CAAC,QAAS,CAAC,mBA/vFR,cAA+BwK,GAOlC,WAAMhvB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KAuvFA,CAAC,SAAU,CAAC,oBAnoFT,cAAgC0K,GAOnC,WAAMlvB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KA2nFA,CAAC,aAAc,CAAC,wBAv4Fb,cAAoCqK,GAOvC,WAAM7uB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KA+3FA,CAAC,UAAW,CAAC,qBApzEV,cAAiCkL,GAOpC,WAAM1vB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KA4yEA,CAAC,MAAO,CAAC,qBA3uEN,cAAiCmL,GAOpC,WAAM3vB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KAmuEA,CAAC,cAAe,CAAC,wBA1qEd,cAAoCoL,GAOvC,WAAM5vB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KAkqEA,CAAC,aAAc,CAAC,wBA5zFb,cAAoCuK,GAOvC,WAAM/uB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,KAozFA,CAAC,cAAe,CAAC,yBAvsFd,cAAqCyK,GAOxC,WAAMjvB,CAAMwkB,GACR,OAAO,IAAI8U,SAAqBr6B,MAAMe,MAAMwkB,GAChD,OAisFE+U,GAA6C,IAAIj8B,IAAI,CACvD,CAAC,OAAQ,CAAC,2BA3zGP,cAAuCixB,GAO1C,WAAMvuB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,KAmzGA,CAAC,UAAW,CAAC,8BAzqGV,cAA0CiK,GAO7C,WAAMzuB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,KAiqGA,CAAC,WAAY,CAAC,+BApvGX,cAA2CgK,GAO9C,WAAMxuB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,KA4uGA,CAAC,YAAa,CAAC,gCApmGZ,cAA4CkK,GAO/C,WAAM1uB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,KA4lGA,CAAC,UAAW,CAAC,8BA9hGV,cAA0CmK,GAO7C,WAAM3uB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,KAshGA,CAAC,aAAc,CAAC,gCAx9Fb,cAA4CoK,GAO/C,WAAM5uB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,KAg9FA,CAAC,QAAS,CAAC,4BAnuFR,cAAwCwK,GAO3C,WAAMhvB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,KA2tFA,CAAC,SAAU,CAAC,6BA/pFT,cAAyC0K,GAO5C,WAAMlvB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,KAupFA,CAAC,aAAc,CAAC,iCAv6Fb,cAA6CqK,GAOhD,WAAM7uB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,KA+5FA,CAAC,UAAW,CAAC,8BAxxEV,cAA0CkL,GAO7C,WAAM1vB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,KAgxEA,CAAC,MAAO,CAAC,0BA/sEN,cAAsCmL,GAOzC,WAAM3vB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,KAusEA,CAAC,cAAe,CAAC,iCA9oEd,cAA6CoL,GAOhD,WAAM5vB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,KAsoEA,CAAC,aAAc,CAAC,iCA/yFb,cAA6CuK,GAOhD,WAAM/uB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,KAuyFA,CAAC,cAAe,CAAC,kCAlsFd,cAA8CyK,GAOjD,WAAMjvB,CAAMwkB,GACR,OAAO,IAAIgV,SAAmCv6B,MAAMe,MAAMwkB,GAC9D,OA4rFEuG,GAAuC,IAAIztB,IAAI,CACjD,CAAC,yBAA0B,CAAC,4BAA6ByyB,OAGvD0J,GAAsD,IAAIn8B,IAAI,CAChE,CAAC,yBAA0B,CAAC,4BAA6ByyB,OAGvD2J,GAA+C,IAAIp8B,IAAI,CACzD,CAAC,MAAO,CAAC,4BAh5CN,cAAwC40B,GAI3C,WAAMlyB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KA24CA,CAAC,YAAa,CAAC,kCAh0CZ,cAA8C4N,GAIjD,WAAMpyB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KA2zCA,CAAC,OAAQ,CAAC,6BA3yCP,cAAyC8N,GAI5C,WAAMtyB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAsyCA,CAAC,OAAQ,CAAC,6BAxuCP,cAAyCsO,GAI5C,WAAM9yB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAmuCA,CAAC,WAAY,CAAC,iCAn9BX,cAA6C6O,GAIhD,WAAMrzB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KA88BA,CAAC,aAAc,CAAC,mCA97Bb,cAA+C8O,GAIlD,WAAMtzB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAy7BA,CAAC,SAAU,CAAC,+BA16BT,cAA2C+O,GAI9C,WAAMvzB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAq6BA,CAAC,SAAU,CAAC,+BAntCT,cAA2CuO,GAI9C,WAAM/yB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KA8sCA,CAAC,OAAQ,CAAC,6BAtsCP,cAAyCwO,GAI5C,WAAMhzB,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,OAmsCEmV,GAA2C,IAAIr8B,IAAI,CACrD,CAAC,OAAQ,CAAC,yBAvyCP,cAAqCi1B,GAIxC,WAAMvyB,CAAMwkB,GACR,OAAO,IAAIgO,SAAgCvzB,MAAMe,MAAMwkB,GAC3D,KAkyCA,CAAC,QAAS,CAAC,0BAn6BR,cAAsCgP,GAIzC,WAAMxzB,CAAMwkB,GACR,OAAO,IAAIiP,SAAiCx0B,MAAMe,MAAMwkB,GAC5D,OAg6BEoV,GAAqD,IAAIt8B,IAAI,CAC/D,CAAC,SAAU,CAAC,2BAj0CT,cAAuC+0B,SAo0CxCwH,GAA6C,IAAIv8B,IAAI,CACvD,CAAC,OAAQ,CAAC,sBAvyCP,cAAkCi1B,GAMrC,WAAMvyB,CAAMwkB,GACR,OAAO,IAAImO,SAA6B1zB,MAAMe,MAAMwkB,GACxD,OAkyCEsV,GAA0C,IAAIx8B,IAAI,CACpD,CAAC,MAAO,CAAC,WAAYs2B,OAGnBmG,GAA8B,IAAIz8B,IAAI,CACxC,CAAC,WAAY,CAAC,iBA1xBX,cAA6B42B,GAMhC,WAAMl0B,CAAMwkB,GACR,OAAO,IAAIwV,SAAqB/6B,MAAMe,MAAMwkB,GAChD,KAmxBA,CAAC,QAAS,CAAC,cAjqBR,cAA0B2P,GAM7B,WAAMn0B,CAAMwkB,GACR,OAAO,IAAIwV,SAAqB/6B,MAAMe,MAAMwkB,GAChD,KA0pBA,CAAC,SAAU,CAAC,eAluBT,cAA2B0P,GAM9B,WAAMl0B,CAAMwkB,GACR,OAAO,IAAIwV,SAAqB/6B,MAAMe,MAAMwkB,GAChD,OA6tBEyV,GAA+C,IAAI38B,IAAI,CACzD,CAAC,WAAY,CAAC,oCArxBX,cAAgD42B,GAMnD,WAAMl0B,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KA8wBA,CAAC,QAAS,CAAC,iCAzpBR,cAA6C2P,GAMhD,WAAMn0B,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAkpBA,CAAC,SAAU,CAAC,kCA1tBT,cAA8C0P,GAMjD,WAAMl0B,CAAMwkB,GACR,OAAO,IAAIyU,SAA+Bh6B,MAAMe,MAAMwkB,GAC1D,KAmtBA,CAAC,gCAAiC,CAAC,4BAxqEhC,cAAwCqL,SA2qEzCqK,GAAwC,IAAI58B,IAAI,CAClD,CAAC,WAAY,CAAC,0BAt3CX,cAAsC60B,GAIzC,WAAMnyB,CAAMwkB,GACR,OAAO,IAAI2V,SAAyBl7B,MAAMe,MAAMwkB,GACpD,OAm3CE4V,GAAyC,IAAI98B,IAAI,CACnD,CAAC,UAAW,CAAC,iCA1rCV,cAA6C21B,SA6rC9CoH,GAA2C,IAAI/8B,IAAI,CACrD,CAAC,MAAO,CAAC,wBA/oCN,cAAoC41B,OAgpCvC,CAAC,OAAQ,CAAC,yBAjmCP,cAAqCC,SAqmCtCmH,GAA2B,CAC7B,CAACjK,GAAkCzO,IACnC,CAAC0O,GAAqC1O,IACtC,CAAC6U,GAAkC7U,IACnC,CAACoX,GAAiDpX,IAClD,CAACsX,GAA8CtX,IAC/C,CAACiJ,GAA8CjJ,IAC/C,CAACkJ,GAA0ClJ,IAC3C,CAACgJ,GAAkChJ,IACnC,CAACyX,GAAmCzX,IACpC,CAAC2X,GAA4C3X,IAC7C,CAACmJ,GAAsCnJ,IACvC,CAAC8X,GAA8C9X,IAC/C,CAACiY,GAA4CjY,IAC7C,CAACsY,GAAuCtY,IACxC,CAACwY,GAAwCxY,IACzC,CAACyY,GAA0CzY,IAC3C,CAAC+X,GAA0C/X,IAC3C,CAACgY,GAAoDhY,IACrD,CAACkY,GAAyClY,IAC1C,CAACmY,GAA6BnY,IAC9B,CAACqY,GAA8CrY,IAC/C,CAACmX,GAA6CnX,KAGlD,IAAK,MAAO2Y,GAAU1gC,MAASygC,GAE3B,IAAK,MAAOrP,EAAMza,KAAU+pB,GAAS3a,SACjCiC,GAAmBtb,IAAI0kB,EAAMpxB,IAC7BkoB,GAA4Bxb,IAAIiK,EAAOya,GACvCnJ,GAA4Bvb,IAAI0kB,EAAMza,GAI9C,MAAMgqB,GAAiB,CACnB,CAAC,8BAz3DE,cAA0C9J,GAG7C,4BAAa3e,CAAgB9V,GAA6C,IAAAw+B,EAAA,IAAdv+B,EAAO5C,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAGnE,OADuB,QAAvBmhC,EAAAv+B,EAAQmsB,uBAAe,IAAAoS,IAAvBv+B,EAAQmsB,gBAAoB,cACrBppB,MAAM8S,gBAAgB9V,EAA+BC,EAChE,GAk3D6D0lB,IAC7D,CAAC,gCAt1DE,cAA4C8O,GAE/C,4BAAa3e,CAAgB9V,GAA6C,IAAAy+B,EAAA,IAAdx+B,EAAO5C,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAGnE,OADuB,QAAvBohC,EAAAx+B,EAAQmsB,uBAAe,IAAAqS,IAAvBx+B,EAAQmsB,gBAAoB,gBACrBppB,MAAM8S,gBAAgB9V,EAA+BC,EAChE,GAg1DiE0lB,IAEjE,CAAC,8BAnZE,cAA0CuU,GAG7C,4BAAapkB,CAAgB9V,GAA6C,IAAA0+B,EAAA,IAAdz+B,EAAO5C,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAGnE,OADuB,QAAvBqhC,EAAAz+B,EAAQmsB,uBAAe,IAAAsS,IAAvBz+B,EAAQmsB,gBAAoB,cACrBppB,MAAM8S,gBAAgB9V,EAA+BC,EAChE,GA4Y6D0lB,IAC7D,CAAC,+BAhXE,cAA2CuU,GAE9C,4BAAapkB,CAAgB9V,GAA6C,IAAA2+B,EAAA,IAAd1+B,EAAO5C,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAGnE,OADuB,QAAvBshC,EAAA1+B,EAAQmsB,uBAAe,IAAAuS,IAAvB1+B,EAAQmsB,gBAAoB,eACrBppB,MAAM8S,gBAAgB9V,EAA+BC,EAChE,GA0W+D0lB,KAEnE,IAAK,MAAOqJ,GAAMza,GAAO3W,MAAS2gC,GAC9B3Y,GAAmBtb,IAAI0kB,GAAMpxB,IAC7BkoB,GAA4Bxb,IAAIiK,GAAOya,IACvCnJ,GAA4Bvb,IAAI0kB,GAAMza,IAWnC,MAAMqqB,WAAkBzE,KAK/B/lB,EAAAA,EAAAA,GALawqB,GAAS,uBACY,CAACxK,GAAkCC,GAAqCmG,MAAiCpmB,EAAAA,EAAAA,GAD9HwqB,GAAS,gBAEI,GAUnB,MAAMC,WAA2C1E,KAIxD/lB,EAAAA,EAAAA,GAJayqB,GAAkC,uBACb,CAAC9B,KAU5B,MAAM+B,WAAwC3E,KAIrD/lB,EAAAA,EAAAA,GAJa0qB,GAA+B,uBACV,CAAC7B,KAU5B,MAAM8B,WAA8B5E,KAI3C/lB,EAAAA,EAAAA,GAJa2qB,GAAqB,uBACA,CAACnQ,KAU5B,MAAMoQ,WAAkC7E,KAI/C/lB,EAAAA,EAAAA,GAJa4qB,GAAyB,uBACJ,CAACnQ,KAU5B,MAAMoQ,WAAsC9E,KAInD/lB,EAAAA,EAAAA,GAJa6qB,GAA6B,uBACR,CAACnC,KAU5B,MAAMoC,WAA6B/E,KAI1C/lB,EAAAA,EAAAA,GAJa8qB,GAAoB,uBACC,CAACvQ,KAU5B,MAAMwQ,WAA6BhF,KAI1C/lB,EAAAA,EAAAA,GAJa+qB,GAAoB,uBACC,CAAC/B,KAU5B,MAAMgC,WAAsCjF,KAInD/lB,EAAAA,EAAAA,GAJagrB,GAA6B,uBACR,CAAC9B,KAU5B,MAAM+B,WAA+BlF,KAI5C/lB,EAAAA,EAAAA,GAJairB,GAAsB,uBACD,CAACvQ,KAU5B,MAAMwQ,WAAwCnF,KAIrD/lB,EAAAA,EAAAA,GAJakrB,GAA+B,uBACV,CAAC7B,KAU5B,MAAM8B,WAAsCpF,KAInD/lB,EAAAA,EAAAA,GAJamrB,GAA6B,uBACR,CAAC3B,KAU5B,MAAM4B,WAAoCrF,KAEhD/lB,EAAAA,EAAAA,GAFYorB,GAA2B,uBACN,CAAC9B,KAG5B,MAAM+B,WAA4CtF,KAKzD/lB,EAAAA,EAAAA,GALaqrB,GAAmC,uBACd,CAAC9B,MAalCvpB,EAAAA,EAAAA,GAFM,cAAyC+lB,KAAT,uBACL,CAAC0D,KAG5B,MAAM6B,WAAwBvF,KAEpC/lB,EAAAA,EAAAA,GAFYsrB,GAAe,uBACM,CAAC5B,KAG5B,MAAM6B,WAAwCxF,KAEpD/lB,EAAAA,EAAAA,GAFYurB,GAA+B,uBACV,CAAC3B,KAG5B,MAAM4B,WAA8CzF,KAE1D/lB,EAAAA,EAAAA,GAFYwrB,GAAqC,uBAChB,CAACpC,MAKlCppB,EAAAA,EAAAA,GAFM,cAAuC+lB,KAAT,uBACH,CAAC8D,KAG5B,MAAM4B,WAAiC1F,KAE7C/lB,EAAAA,EAAAA,GAFYyrB,GAAwB,uBACH,CAAC1B,KAG5B,MAAM2B,WAAoC3F,KAMjD/lB,EAAAA,EAAAA,GANa0rB,GAA2B,uBACN,CAAC1B,KAM5B,MAAM7U,WAAwB8I,GASjCr2B,WAAAA,CAAW+jC,GAAmG,IAAlG,OAAE5W,EAAM,gBAAEV,EAAe,gBAAED,EAAe,mBAAEsI,EAAqB,KAAI,iBAAEC,EAAmB,MAAMgP,EACxG/8B,QACA7H,KAAKguB,OAASA,EACdhuB,KAAKstB,gBAAkBA,EACvBttB,KAAKqtB,gBAAkBA,EACvBrtB,KAAK21B,mBAAqBA,EAC1B31B,KAAK41B,iBAAmBA,CAC5B,EAMG,MAAMiM,WAAiC3K,GAK1Cr2B,WAAAA,CAAWgkC,GAAa,IAAZ,OAAE7W,GAAQ6W,EAClBh9B,QACA7H,KAAKguB,OAASA,CAClB,EAMG,MAAM+T,WAA8B7K,GAKvCr2B,WAAAA,CAAWikC,GAAa,IAAZ,OAAE9W,GAAQ8W,EAClBj9B,QACA7H,KAAKguB,OAASA,CAClB,EAMG,MAAMkU,WAAuBhL,GAKhCr2B,WAAAA,CAAWkkC,GAAa,IAAZ,OAAE/W,GAAQ+W,EAClBl9B,QACA7H,KAAKguB,OAASA,CAClB,EAMG,MAAMoU,WAAqClL,GAM9Cr2B,WAAAA,CAAWmkC,GAA+B,IAA9B,aAAEC,EAAY,WAAEC,GAAYF,EACpCn9B,QACA7H,KAAKilC,aAAeA,EACpBjlC,KAAKklC,WAAaA,CACtB,EAOG,MAAMtC,WAAuB1L,GAKhCr2B,WAAAA,CAAWskC,GAAa,IAAZ,OAAEnX,GAAQmX,EAClBt9B,QACA7H,KAAKguB,OAASA,CAClB,EAoBG,MAAM+U,WAA2B7L,GAKpCr2B,WAAAA,CAAWukC,GAAa,IAAZ,OAAEC,GAAQD,EAClBv9B,QACA7H,KAAKqlC,OAASA,CAClB,uCC1kKJ1gC,eAAe2gC,GAAcC,GAOzB,OANKvkC,MAAMC,QAAQskC,KACfA,EAAS,CAACA,IAIdA,QAAevgC,QAAQC,IAAIsgC,EAAOt8B,KAAIlK,GAAKymC,GAAAA,EAASC,KAAK1mC,KAE7D,CAOO,MAAM2mC,WAAiB/9B,EAAAA,GAS1B9G,WAAAA,CAAWmc,GAAsD,IAArD,KAAEmL,EAAI,MAAE/O,EAAK,UAAEb,EAAY,KAAI,UAAEotB,EAAY,MAAM3oB,EAC3DnV,QACA7H,KAAKmoB,KAAOA,EACZnoB,KAAKoZ,MAAQA,EACbpZ,KAAKuY,UAAYA,EACjBvY,KAAK2lC,UAAYA,CACrB,CAMA,aAAM7U,SACI9wB,KAAKoZ,MAAM0X,SACrB,CAQA,WAAMloB,CAAMg9B,GAER,IAAIxY,EAAeptB,KAAKuY,UAAUqtB,EAAO,CACrCxqB,SAAS,EACTC,YAAY,IAMhB,MAAO,CAAC+R,QAFYptB,KAAKoZ,MAAMgU,GAGnC,EAkWG,MAAMyY,WAAoCH,GAAS7kC,WAAAA,GAAA,SAAAqB,YAAA+W,EAAAA,EAAAA,GAAA,YAC/C,iBAAgB,CAYvB,WAAMrQ,CAAMg9B,GAA6B,IAAtB9mB,EAAe5c,UAAAC,OAAA,QAAAoD,IAAArD,UAAA,GAAAA,UAAA,GAAG,CAAC,EAC7BlB,MAAMC,QAAQ2kC,KACfA,EAAQ,CAACA,IAIT5lC,KAAKoZ,MAAMvS,OAAO0P,SAClBqvB,EAAQA,EAAM38B,KAAIlK,GAAKiB,KAAKoZ,MAAMvS,OAAO0P,OAASxX,KAItD,IAAI+mC,EAAuB9lC,KAAKoZ,MAAMvS,OAAOi/B,qBACzCA,GAAwBA,EAAqB9lC,KAAKmoB,OAE9C2d,EAAqB9lC,KAAKmoB,MAAM5R,SAChCqvB,EAAQA,EAAM38B,KAAIlK,GAAK+mC,EAAqB9lC,KAAKmoB,MAAM5R,OAASxX,KAMxE,IAIIqd,EAJAyC,EAAoB,CACpBzD,SAAS,EACTC,YAAY,GAOZe,EAJApc,gBAAgB+lC,IAAuB,8BAA+B/lC,KAAKuY,UAI/DvY,KAAKuY,UAAUoG,0BAA0BinB,EAAO/mB,EAAmBC,GAAiB1C,UAGpFpc,KAAKuY,UAAUqtB,EAAO/mB,GAAmBzC,UAGzD,IAAI4pB,QAAuBhmC,KAAKoZ,MAAM0Z,SAAS1W,EAAW0C,GAKtDvS,EAAWvM,KAAKuY,UAAUmE,aAAaspB,EAAgB,CACvD/oB,qBAAqB,IAOzB,OALkB,OAAdjd,KAAKuI,OACLgE,EAAWA,EAAStD,KAAIxC,GACE,OAAdzG,KAAKuI,KAAiB9B,EAAO,CAAE,CAACzG,KAAKuI,MAAO9B,MAGrD8F,CACX,EA0EG,MAAMw5B,WAA4BF,GAA4BhlC,WAAAA,GAAA,SAAAqB,YAAA+W,EAAAA,EAAAA,GAAA,YAC1D,mBAAkB,EAwtDRgtB,GAAAA,EAYAA,GAAAA,EAYAA,GAAAA,EAYAA,GAAAA,EAYAA,GAAAA,EAaAA,GAAAA,EAaAA,GAAAA,EAcAA,GAAAA,EAaAA,GAAAA,EAYAA,GAAAA,EAYAA,GAAAA,EAYAA,GAAAA,EAYAA,GAAAA","sources":["../node_modules/react/cjs/react.production.min.js","../node_modules/react/index.js","../node_modules/@xenova/transformers/src/tokenizers.js","../node_modules/@xenova/transformers/src/models.js","../node_modules/@xenova/transformers/src/pipelines.js"],"sourcesContent":["/**\n * @license React\n * react.production.min.js\n *\n * Copyright (c) Facebook, Inc. and its affiliates.\n *\n * This source code is licensed under the MIT license found in the\n * LICENSE file in the root directory of this source tree.\n */\n'use strict';var l=Symbol.for(\"react.element\"),n=Symbol.for(\"react.portal\"),p=Symbol.for(\"react.fragment\"),q=Symbol.for(\"react.strict_mode\"),r=Symbol.for(\"react.profiler\"),t=Symbol.for(\"react.provider\"),u=Symbol.for(\"react.context\"),v=Symbol.for(\"react.forward_ref\"),w=Symbol.for(\"react.suspense\"),x=Symbol.for(\"react.memo\"),y=Symbol.for(\"react.lazy\"),z=Symbol.iterator;function A(a){if(null===a||\"object\"!==typeof a)return null;a=z&&a[z]||a[\"@@iterator\"];return\"function\"===typeof a?a:null}\nvar B={isMounted:function(){return!1},enqueueForceUpdate:function(){},enqueueReplaceState:function(){},enqueueSetState:function(){}},C=Object.assign,D={};function E(a,b,e){this.props=a;this.context=b;this.refs=D;this.updater=e||B}E.prototype.isReactComponent={};\nE.prototype.setState=function(a,b){if(\"object\"!==typeof a&&\"function\"!==typeof a&&null!=a)throw Error(\"setState(...): takes an object of state variables to update or a function which returns an object of state variables.\");this.updater.enqueueSetState(this,a,b,\"setState\")};E.prototype.forceUpdate=function(a){this.updater.enqueueForceUpdate(this,a,\"forceUpdate\")};function F(){}F.prototype=E.prototype;function G(a,b,e){this.props=a;this.context=b;this.refs=D;this.updater=e||B}var H=G.prototype=new F;\nH.constructor=G;C(H,E.prototype);H.isPureReactComponent=!0;var I=Array.isArray,J=Object.prototype.hasOwnProperty,K={current:null},L={key:!0,ref:!0,__self:!0,__source:!0};\nfunction M(a,b,e){var d,c={},k=null,h=null;if(null!=b)for(d in void 0!==b.ref&&(h=b.ref),void 0!==b.key&&(k=\"\"+b.key),b)J.call(b,d)&&!L.hasOwnProperty(d)&&(c[d]=b[d]);var g=arguments.length-2;if(1===g)c.children=e;else if(1<g){for(var f=Array(g),m=0;m<g;m++)f[m]=arguments[m+2];c.children=f}if(a&&a.defaultProps)for(d in g=a.defaultProps,g)void 0===c[d]&&(c[d]=g[d]);return{$$typeof:l,type:a,key:k,ref:h,props:c,_owner:K.current}}\nfunction N(a,b){return{$$typeof:l,type:a.type,key:b,ref:a.ref,props:a.props,_owner:a._owner}}function O(a){return\"object\"===typeof a&&null!==a&&a.$$typeof===l}function escape(a){var b={\"=\":\"=0\",\":\":\"=2\"};return\"$\"+a.replace(/[=:]/g,function(a){return b[a]})}var P=/\\/+/g;function Q(a,b){return\"object\"===typeof a&&null!==a&&null!=a.key?escape(\"\"+a.key):b.toString(36)}\nfunction R(a,b,e,d,c){var k=typeof a;if(\"undefined\"===k||\"boolean\"===k)a=null;var h=!1;if(null===a)h=!0;else switch(k){case \"string\":case \"number\":h=!0;break;case \"object\":switch(a.$$typeof){case l:case n:h=!0}}if(h)return h=a,c=c(h),a=\"\"===d?\".\"+Q(h,0):d,I(c)?(e=\"\",null!=a&&(e=a.replace(P,\"$&/\")+\"/\"),R(c,b,e,\"\",function(a){return a})):null!=c&&(O(c)&&(c=N(c,e+(!c.key||h&&h.key===c.key?\"\":(\"\"+c.key).replace(P,\"$&/\")+\"/\")+a)),b.push(c)),1;h=0;d=\"\"===d?\".\":d+\":\";if(I(a))for(var g=0;g<a.length;g++){k=\na[g];var f=d+Q(k,g);h+=R(k,b,e,f,c)}else if(f=A(a),\"function\"===typeof f)for(a=f.call(a),g=0;!(k=a.next()).done;)k=k.value,f=d+Q(k,g++),h+=R(k,b,e,f,c);else if(\"object\"===k)throw b=String(a),Error(\"Objects are not valid as a React child (found: \"+(\"[object Object]\"===b?\"object with keys {\"+Object.keys(a).join(\", \")+\"}\":b)+\"). If you meant to render a collection of children, use an array instead.\");return h}\nfunction S(a,b,e){if(null==a)return a;var d=[],c=0;R(a,d,\"\",\"\",function(a){return b.call(e,a,c++)});return d}function T(a){if(-1===a._status){var b=a._result;b=b();b.then(function(b){if(0===a._status||-1===a._status)a._status=1,a._result=b},function(b){if(0===a._status||-1===a._status)a._status=2,a._result=b});-1===a._status&&(a._status=0,a._result=b)}if(1===a._status)return a._result.default;throw a._result;}\nvar U={current:null},V={transition:null},W={ReactCurrentDispatcher:U,ReactCurrentBatchConfig:V,ReactCurrentOwner:K};exports.Children={map:S,forEach:function(a,b,e){S(a,function(){b.apply(this,arguments)},e)},count:function(a){var b=0;S(a,function(){b++});return b},toArray:function(a){return S(a,function(a){return a})||[]},only:function(a){if(!O(a))throw Error(\"React.Children.only expected to receive a single React element child.\");return a}};exports.Component=E;exports.Fragment=p;\nexports.Profiler=r;exports.PureComponent=G;exports.StrictMode=q;exports.Suspense=w;exports.__SECRET_INTERNALS_DO_NOT_USE_OR_YOU_WILL_BE_FIRED=W;\nexports.cloneElement=function(a,b,e){if(null===a||void 0===a)throw Error(\"React.cloneElement(...): The argument must be a React element, but you passed \"+a+\".\");var d=C({},a.props),c=a.key,k=a.ref,h=a._owner;if(null!=b){void 0!==b.ref&&(k=b.ref,h=K.current);void 0!==b.key&&(c=\"\"+b.key);if(a.type&&a.type.defaultProps)var g=a.type.defaultProps;for(f in b)J.call(b,f)&&!L.hasOwnProperty(f)&&(d[f]=void 0===b[f]&&void 0!==g?g[f]:b[f])}var f=arguments.length-2;if(1===f)d.children=e;else if(1<f){g=Array(f);\nfor(var m=0;m<f;m++)g[m]=arguments[m+2];d.children=g}return{$$typeof:l,type:a.type,key:c,ref:k,props:d,_owner:h}};exports.createContext=function(a){a={$$typeof:u,_currentValue:a,_currentValue2:a,_threadCount:0,Provider:null,Consumer:null,_defaultValue:null,_globalName:null};a.Provider={$$typeof:t,_context:a};return a.Consumer=a};exports.createElement=M;exports.createFactory=function(a){var b=M.bind(null,a);b.type=a;return b};exports.createRef=function(){return{current:null}};\nexports.forwardRef=function(a){return{$$typeof:v,render:a}};exports.isValidElement=O;exports.lazy=function(a){return{$$typeof:y,_payload:{_status:-1,_result:a},_init:T}};exports.memo=function(a,b){return{$$typeof:x,type:a,compare:void 0===b?null:b}};exports.startTransition=function(a){var b=V.transition;V.transition={};try{a()}finally{V.transition=b}};exports.unstable_act=function(){throw Error(\"act(...) is not supported in production builds of React.\");};\nexports.useCallback=function(a,b){return U.current.useCallback(a,b)};exports.useContext=function(a){return U.current.useContext(a)};exports.useDebugValue=function(){};exports.useDeferredValue=function(a){return U.current.useDeferredValue(a)};exports.useEffect=function(a,b){return U.current.useEffect(a,b)};exports.useId=function(){return U.current.useId()};exports.useImperativeHandle=function(a,b,e){return U.current.useImperativeHandle(a,b,e)};\nexports.useInsertionEffect=function(a,b){return U.current.useInsertionEffect(a,b)};exports.useLayoutEffect=function(a,b){return U.current.useLayoutEffect(a,b)};exports.useMemo=function(a,b){return U.current.useMemo(a,b)};exports.useReducer=function(a,b,e){return U.current.useReducer(a,b,e)};exports.useRef=function(a){return U.current.useRef(a)};exports.useState=function(a){return U.current.useState(a)};exports.useSyncExternalStore=function(a,b,e){return U.current.useSyncExternalStore(a,b,e)};\nexports.useTransition=function(){return U.current.useTransition()};exports.version=\"18.2.0\";\n","'use strict';\n\nif (process.env.NODE_ENV === 'production') {\n  module.exports = require('./cjs/react.production.min.js');\n} else {\n  module.exports = require('./cjs/react.development.js');\n}\n","\n/**\n * @file Tokenizers are used to prepare textual inputs for a model.\n * \n * **Example:** Create an `AutoTokenizer` and use it to tokenize a sentence.\n * This will automatically detect the tokenizer type based on the tokenizer class defined in `tokenizer.json`.\n * ```javascript\n * import { AutoTokenizer } from '@xenova/transformers';\n * \n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/bert-base-uncased');\n * let { input_ids } = await tokenizer('I love transformers!');\n * // Tensor {\n * //   data: BigInt64Array(6) [101n, 1045n, 2293n, 19081n, 999n, 102n],\n * //   dims: [1, 6],\n * //   type: 'int64',\n * //   size: 6,\n * // }\n * ```\n * \n * @module tokenizers\n */\n\nimport {\n    Callable,\n    reverseDictionary,\n    escapeRegExp,\n    isIntegralNumber,\n    mergeArrays,\n} from './utils/core.js';\n\nimport {\n    getModelJSON,\n} from './utils/hub.js';\n\nimport { max, min, round } from './utils/maths.js';\nimport { Tensor } from './utils/tensor.js';\n\nimport {\n    PriorityQueue,\n    TokenLattice,\n    CharTrie,\n} from './utils/data-structures.js';\n\nimport { Template } from '@huggingface/jinja';\n\n\n/**\n * @typedef {Object} TokenizerProperties Additional tokenizer-specific properties.\n * @property {boolean} [legacy=false] Whether or not the `legacy` behavior of the tokenizer should be used.\n * @typedef {import('./utils/hub.js').PretrainedOptions & TokenizerProperties} PretrainedTokenizerOptions\n */\n\n/**\n * Loads a tokenizer from the specified path.\n * @param {string} pretrained_model_name_or_path The path to the tokenizer directory.\n * @param {PretrainedTokenizerOptions} options Additional options for loading the tokenizer.\n * @returns {Promise<any[]>} A promise that resolves with information about the loaded tokenizer.\n */\nasync function loadTokenizer(pretrained_model_name_or_path, options) {\n\n    let info = await Promise.all([\n        getModelJSON(pretrained_model_name_or_path, 'tokenizer.json', true, options),\n        getModelJSON(pretrained_model_name_or_path, 'tokenizer_config.json', true, options),\n    ])\n\n    // Override legacy option if `options.legacy` is not null\n    if (options.legacy !== null) {\n        info[1].legacy = options.legacy;\n    }\n    return info;\n}\n\n\n/**\n * Helper function to split a string on a regex, but keep the delimiters.\n * This is required, because the JavaScript `.split()` method does not keep the delimiters,\n * and wrapping in a capturing group causes issues with existing capturing groups (due to nesting).\n * @param {string} text The text to split.\n * @param {RegExp} regex The regex to split on.\n * @returns {string[]} The split string.\n */\nfunction regexSplit(text, regex) {\n    const result = [];\n    let prev = 0;\n    for (const match of text.matchAll(regex)) {\n        const fullMatch = match[0];\n        if (prev < match.index) {\n            result.push(text.slice(prev, match.index));\n        }\n        if (fullMatch.length > 0) {\n            result.push(fullMatch);\n        }\n        prev = match.index + fullMatch.length;\n    }\n    if (prev < text.length) {\n        result.push(text.slice(prev));\n    }\n    return result;\n}\n\n\n/**\n * Helper method to construct a pattern from a config object.\n * @param {Object} pattern The pattern object.\n * @param {boolean} invert Whether to invert the pattern.\n * @returns {RegExp|null} The compiled pattern.\n */\nfunction createPattern(pattern, invert = true) {\n\n    if (pattern.Regex !== undefined) {\n        // In certain cases, the pattern may contain unnecessary escape sequences (e.g., \\# or \\& or \\~).\n        // i.e., valid in Python (where the patterns are exported from) but invalid in JavaScript (where the patterns are parsed).\n        // This isn't an issue when creating the regex w/o the 'u' flag, but it is when the 'u' flag is used.\n        // For this reason, it is necessary to remove these backslashes before creating the regex.\n        // See https://stackoverflow.com/a/63007777/13989043 for more information\n        const regex = pattern.Regex.replace(/\\\\([#&~])/g, '$1'); // TODO: add more characters to this list if necessary\n        return new RegExp(regex, 'gu');\n\n    } else if (pattern.String !== undefined) {\n        const escaped = escapeRegExp(pattern.String);\n        // NOTE: if invert is true, we wrap the pattern in a group so that it is kept when performing .split()\n        return new RegExp(invert ? escaped : `(${escaped})`, 'gu');\n\n    } else {\n        console.warn('Unknown pattern type:', pattern)\n        return null;\n    }\n}\n\n/**\n * Helper function to convert an Object to a Map\n * @param {Object} obj The object to convert.\n * @returns {Map<string, any>} The map.\n */\nfunction objectToMap(obj) {\n    return new Map(Object.entries(obj));\n}\n\n/**\n * Helper function to convert a tensor to a list before decoding.\n * @param {Tensor} tensor The tensor to convert.\n * @returns {number[]} The tensor as a list.\n */\nfunction prepareTensorForDecode(tensor) {\n    const dims = tensor.dims;\n    switch (dims.length) {\n        case 1:\n            return tensor.tolist();\n        case 2:\n            if (dims[0] !== 1) {\n                throw new Error('Unable to decode tensor with `batch size !== 1`. Use `tokenizer.batch_decode(...)` for batched inputs.');\n            }\n            return tensor.tolist()[0];\n        default:\n            throw new Error(`Expected tensor to have 1-2 dimensions, got ${dims.length}.`)\n    }\n}\n\n/**\n * Clean up a list of simple English tokenization artifacts like spaces before punctuations and abbreviated forms\n * @param {string} text The text to clean up.\n * @returns {string} The cleaned up text.\n */\nfunction clean_up_tokenization(text) {\n    // Clean up a list of simple English tokenization artifacts\n    // like spaces before punctuations and abbreviated forms\n    return text.replace(/ \\./g, '.')\n        .replace(/ \\?/g, '?')\n        .replace(/ \\!/g, '!')\n        .replace(/ ,/g, ',')\n        .replace(/ \\' /g, \"'\")\n        .replace(/ n\\'t/g, \"n't\")\n        .replace(/ \\'m/g, \"'m\")\n        .replace(/ \\'s/g, \"'s\")\n        .replace(/ \\'ve/g, \"'ve\")\n        .replace(/ \\'re/g, \"'re\");\n}\n\n/**\n * Helper function to remove accents from a string.\n * @param {string} text The text to remove accents from.\n * @returns {string} The text with accents removed.\n */\nfunction remove_accents(text) {\n    return text.replace(/[\\u0300-\\u036f]/g, '');\n}\n\n/**\n * Helper function to lowercase a string and remove accents.\n * @param {string} text The text to lowercase and remove accents from.\n * @returns {string} The lowercased text with accents removed.\n */\nfunction lowercase_and_remove_accent(text) {\n    return remove_accents(text.toLowerCase());\n}\n\n/**\n * Helper function to fuse consecutive values in an array equal to the specified value.\n * @param {Array} arr The input array\n * @param {any} value The value to fuse on.\n */\nfunction fuse(arr, value) {\n    let fused = [];\n    let i = 0;\n    while (i < arr.length) {\n        fused.push(arr[i])\n        if (arr[i] !== value) {\n            ++i;\n            continue;\n        }\n\n        while (i < arr.length && arr[i] === value) {\n            ++i;\n        }\n    }\n\n    return fused;\n}\n\n/**\n * Split a string on whitespace.\n * @param {string} text The text to split.\n * @returns {string[]} The split string.\n */\nfunction whitespace_split(text) {\n    return text.match(/\\S+/g) || [];\n}\n\nconst PUNCTUATION_REGEX = '\\\\p{P}\\\\u0021-\\\\u002F\\\\u003A-\\\\u0040\\\\u005B-\\\\u0060\\\\u007B-\\\\u007E';\n\n/**\n * Represent a token added by the user on top of the existing Model vocabulary.\n * AddedToken can be configured to specify the behavior they should have in various situations like:\n *   - Whether they should only match single words\n *   - Whether to include any whitespace on its left or right\n */\nclass AddedToken {\n    /**\n     * Creates a new instance of AddedToken.\n     * @param {Object} config Added token configuration object.\n     * @param {string} config.content The content of the added token.\n     * @param {number} config.id The id of the added token.\n     * @param {boolean} [config.single_word=false] Whether this token must be a single word or can break words.\n     * @param {boolean} [config.lstrip=false] Whether this token should strip whitespaces on its left.\n     * @param {boolean} [config.rstrip=false] Whether this token should strip whitespaces on its right.\n     * @param {boolean} [config.normalized=false] Whether this token should be normalized.\n     * @param {boolean} [config.special=false] Whether this token is special.\n     */\n    constructor(config) {\n        this.content = config.content;\n        this.id = config.id;\n        this.single_word = config.single_word ?? false;\n        this.lstrip = config.lstrip ?? false;\n        this.rstrip = config.rstrip ?? false;\n        this.special = config.special ?? false;\n        this.normalized = config.normalized ?? null;\n    }\n}\n\n/**\n * Abstract base class for tokenizer models.\n *\n * @extends Callable\n */\nexport class TokenizerModel extends Callable {\n    /**\n     * Creates a new instance of TokenizerModel.\n     * @param {Object} config The configuration object for the TokenizerModel.\n     */\n    constructor(config) {\n        super();\n        this.config = config;\n\n        /** @type {string[]} */\n        this.vocab = [];\n\n        /**\n         * A mapping of tokens to ids.\n         * @type {Map<string, number>}\n         */\n        this.tokens_to_ids = new Map();\n\n        this.unk_token_id = undefined;\n        this.unk_token = undefined;\n        this.end_of_word_suffix = undefined;\n\n        /** @type {boolean} Whether to fuse unknown tokens when encoding. Defaults to false. */\n        this.fuse_unk = this.config.fuse_unk ?? false;\n    }\n\n    /**\n     * Instantiates a new TokenizerModel instance based on the configuration object provided.\n     * @param {Object} config The configuration object for the TokenizerModel.\n     * @param {...*} args Optional arguments to pass to the specific TokenizerModel constructor.\n     * @returns {TokenizerModel} A new instance of a TokenizerModel.\n     * @throws Will throw an error if the TokenizerModel type in the config is not recognized.\n     */\n    static fromConfig(config, ...args) {\n        switch (config.type) {\n            case 'WordPiece':\n                return new WordPieceTokenizer(config);\n            case 'Unigram':\n                // @ts-ignore\n                return new Unigram(config, ...args);\n\n            case 'BPE':\n                return new BPE(config);\n\n            default:\n                if (config.vocab) {\n                    // @ts-ignore\n                    return new LegacyTokenizerModel(config, ...args);\n                }\n                throw new Error(`Unknown TokenizerModel type: ${config.type}`);\n        }\n    }\n\n    /**\n     * Internal function to call the TokenizerModel instance.\n     * @param {string[]} tokens The tokens to encode.\n     * @returns {string[]} The encoded token IDs.\n     */\n    _call(tokens) {\n        return this.encode(tokens);\n    }\n\n    /**\n     * Encodes a list of tokens into a list of token IDs.\n     * @param {string[]} tokens The tokens to encode.\n     * @returns {string[]} The encoded tokens.\n     * @throws Will throw an error if not implemented in a subclass.\n     */\n    encode(tokens) {\n        throw Error(\"encode should be implemented in subclass.\")\n    }\n\n    /**\n     * Converts a list of tokens into a list of token IDs.\n     * @param {string[]} tokens The tokens to convert.\n     * @returns {number[]} The converted token IDs.\n     */\n    convert_tokens_to_ids(tokens) {\n        let ids = tokens.map(t => this.tokens_to_ids.get(t) ?? this.unk_token_id);\n\n        if (this.fuse_unk) {\n            // Fuse unknown tokens\n            ids = fuse(ids, this.unk_token_id);\n        }\n        return ids;\n    }\n\n    /**\n     * Converts a list of token IDs into a list of tokens.\n     * @param {number[]} ids The token IDs to convert.\n     * @returns {string[]} The converted tokens.\n     */\n    convert_ids_to_tokens(ids) {\n        return ids.map(i => this.vocab[i] ?? this.unk_token);\n    }\n}\n\n/**\n * A subclass of TokenizerModel that uses WordPiece encoding to encode tokens.\n * @extends TokenizerModel\n */\nclass WordPieceTokenizer extends TokenizerModel {\n    /**\n     * @param {Object} config The configuration object.\n     * @param {Object} config.vocab A mapping of tokens to ids.\n     * @param {string} config.unk_token The unknown token string.\n     * @param {string} config.continuing_subword_prefix The prefix to use for continuing subwords.\n     * @param {number} [config.max_input_chars_per_word=100] The maximum number of characters per word.\n     */\n    constructor(config) {\n        super(config);\n        /**\n         * A mapping of tokens to ids.\n         * @type {Map<string, number>}\n         */\n        this.tokens_to_ids = objectToMap(config.vocab);\n\n        /**\n         * The id of the unknown token.\n         * @type {number}\n         */\n        this.unk_token_id = this.tokens_to_ids.get(config.unk_token);\n\n        /**\n         * The unknown token string.\n         * @type {string}\n         */\n        this.unk_token = config.unk_token;\n\n        /**\n         * The maximum number of characters allowed per word.\n         * @type {number}\n         */\n        this.max_input_chars_per_word = config.max_input_chars_per_word ?? 100;\n\n        /**\n         * An array of tokens.\n         * @type {string[]}\n         */\n        this.vocab = new Array(this.tokens_to_ids.size);\n        for (const [key, value] of this.tokens_to_ids) {\n            this.vocab[value] = key;\n        }\n    }\n\n    /**\n     * Encodes an array of tokens using WordPiece encoding.\n     * @param {string[]} tokens The tokens to encode.\n     * @returns {string[]} An array of encoded tokens.\n     */\n    encode(tokens) {\n        let outputTokens = [];\n        for (let token of tokens) {\n            let chars = [...token];\n            if (chars.length > this.max_input_chars_per_word) {\n                outputTokens.push(this.unk_token);\n                continue;\n            }\n\n            let isUnknown = false;\n            let start = 0;\n            let subTokens = [];\n\n            while (start < chars.length) {\n                let end = chars.length;\n                let currentSubstring = null;\n                while (start < end) {\n                    let substr = chars.slice(start, end).join('');\n\n                    if (start > 0) {\n                        substr = this.config.continuing_subword_prefix + substr;\n                    }\n                    if (this.tokens_to_ids.has(substr)) {\n                        currentSubstring = substr;\n                        break;\n                    }\n\n                    --end;\n                }\n                if (currentSubstring === null) {\n                    isUnknown = true;\n                    break;\n                }\n                subTokens.push(currentSubstring);\n                start = end;\n            }\n            if (isUnknown) {\n                outputTokens.push(this.unk_token);\n            } else {\n                outputTokens.push(...subTokens);\n            }\n        }\n\n        return outputTokens;\n    }\n\n}\n\n/**\n * Class representing a Unigram tokenizer model.\n * @extends TokenizerModel\n */\nclass Unigram extends TokenizerModel {\n    /**\n     * Create a new Unigram tokenizer model.\n     * @param {Object} config The configuration object for the Unigram model.\n     * @param {number} config.unk_id The ID of the unknown token\n     * @param {any[][]} config.vocab A 2D array representing a mapping of tokens to scores.\n     * @param {Object} moreConfig Additional configuration object for the Unigram model.\n     */\n    constructor(config, moreConfig) {\n        super(config);\n\n        const vocabSize = config.vocab.length;\n        this.vocab = new Array(vocabSize);\n        this.scores = new Array(vocabSize);\n        for (let i = 0; i < vocabSize; ++i) {\n            const piece = config.vocab[i];\n            this.vocab[i] = piece[0];\n            this.scores[i] = piece[1];\n        }\n\n        this.unk_token_id = config.unk_id;\n        this.unk_token = this.vocab[config.unk_id];\n\n        this.tokens_to_ids = new Map(this.vocab.map((x, i) => [x, i]));\n        this.bosToken = ' '; // beginning of a sentence token\n\n        this.bosTokenId = this.tokens_to_ids.get(this.bosToken); // NOTE: may be undefined\n        this.eosToken = moreConfig.eos_token;\n\n        this.eosTokenId = this.tokens_to_ids.get(this.eosToken);\n        this.unkToken = this.vocab[this.unk_token_id];\n\n        this.minScore = min(this.scores)[0];\n\n        this.unkScore = this.minScore - 10.0;\n        this.scores[this.unk_token_id] = this.unkScore;\n\n        this.trie = new CharTrie();\n        this.trie.extend(this.vocab);\n\n        // NOTE: `fuse_unk` is hardcoded to true for Unigram models\n        // See: https://github.com/huggingface/tokenizers/blob/b58227c7f1ccf8b73ee2268354336da56d91e492/tokenizers/src/models/unigram/model.rs#L119\n        this.fuse_unk = true;\n    }\n\n    /**\n     * Populates lattice nodes.\n     * @param {TokenLattice} lattice The token lattice to populate with nodes.\n     */\n    populateNodes(lattice) {\n        const sentence = lattice.sentence;\n        const len = sentence.length;\n        let beginPos = 0;\n        while (beginPos < len) {\n            const mblen = 1;\n            let hasSingleNode = false;\n            const tokens = [];\n\n            for (let token of this.trie.commonPrefixSearch(sentence.slice(beginPos))) {\n                tokens.push(token);\n                const tokenId = this.tokens_to_ids.get(token);\n                const tokenScore = this.scores[tokenId];\n                const n = token.length;\n                lattice.insert(beginPos, n, tokenScore, tokenId);\n                if (!hasSingleNode && n === mblen) {\n                    hasSingleNode = true;\n                }\n            }\n            if (!hasSingleNode) {\n                lattice.insert(beginPos, mblen, this.unkScore, this.unk_token_id);\n            }\n            beginPos += mblen;\n        }\n    }\n\n    /**\n     * Encodes an array of tokens into an array of subtokens using the unigram model.\n     *\n     * @param {string} normalized The normalized string.\n     * @returns {string[]} An array of subtokens obtained by encoding the input tokens using the unigram model.\n     */\n    tokenize(normalized) {\n        const lattice = new TokenLattice(normalized, this.bosTokenId, this.eosTokenId);\n        this.populateNodes(lattice);\n        return lattice.tokens();\n    }\n\n    /**\n     * Encodes an array of tokens using Unigram encoding.\n     * @param {Array} tokens The tokens to encode.\n     * @returns {Array} An array of encoded tokens.\n     */\n    encode(tokens) {\n        let toReturn = [];\n        for (let token of tokens) {\n            const tokenized = this.tokenize(token);\n            toReturn.push(...tokenized);\n        }\n        return toReturn;\n    }\n\n}\n\n/**\n * Returns list of utf-8 byte and a mapping to unicode strings.\n * Specifically avoids mapping to whitespace/control characters the BPE code barfs on.\n * @returns {Object} Object with utf-8 byte keys and unicode string values.\n */\nconst BYTES_TO_UNICODE = (() => {\n    // Returns list of utf-8 byte and a mapping to unicode strings.\n    // We specifically avoids mapping to whitespace/control characters\n    // the bpe code barfs on.\n\n    const bs = [\n        ...Array.from({ length: \"~\".charCodeAt(0) - \"!\".charCodeAt(0) + 1 }, (_, i) => i + \"!\".charCodeAt(0)),\n        ...Array.from({ length: \"¬\".charCodeAt(0) - \"¡\".charCodeAt(0) + 1 }, (_, i) => i + \"¡\".charCodeAt(0)),\n        ...Array.from({ length: \"ÿ\".charCodeAt(0) - \"®\".charCodeAt(0) + 1 }, (_, i) => i + \"®\".charCodeAt(0)),\n    ];\n    let cs = bs.slice();\n    let n = 0;\n    for (let b = 0; b < 256; ++b) {\n        if (!bs.includes(b)) {\n            bs.push(b);\n            cs.push(256 + n);\n            n += 1;\n        }\n    }\n    let ccs = cs.map(n => String.fromCharCode(n));\n    return Object.fromEntries(bs.map((b, i) => [b, ccs[i]]));\n})();\n\nconst UNICODE_TO_BYTES = reverseDictionary(BYTES_TO_UNICODE);\n\n\n/**\n * @typedef {Object} BPENode\n * @property {string} token The token associated with the node\n * @property {number} bias A positional bias for the node.\n * @property {number} [score] The score of the node.\n * @property {BPENode} [prev] The previous node in the linked list.\n * @property {BPENode} [next] The next node in the linked list.\n */\n\n/**\n * BPE class for encoding text into Byte-Pair-Encoding (BPE) tokens.\n * @extends TokenizerModel\n */\nclass BPE extends TokenizerModel {\n    /**\n     * Create a BPE instance.\n     * @param {Object} config The configuration object for BPE.\n     * @param {Object} config.vocab A mapping of tokens to ids.\n     * @param {string} config.unk_token The unknown token used for out of vocabulary words.\n     * @param {string} config.end_of_word_suffix The suffix to place at the end of each word.\n     * @param {string} [config.continuing_subword_suffix] The suffix to insert between words.\n     * @param {Array} config.merges An array of BPE merges as strings.\n     */\n    constructor(config) {\n        super(config);\n\n        this.BPE_SPLIT_TOKEN = ' ';\n\n        /** @type {Map<string, number>} */\n        this.tokens_to_ids = objectToMap(config.vocab);\n\n        this.unk_token_id = this.tokens_to_ids.get(config.unk_token);\n        this.unk_token = config.unk_token;\n\n        this.vocab = new Array(this.tokens_to_ids.size);\n        for (const [key, value] of this.tokens_to_ids) {\n            this.vocab[value] = key;\n        }\n\n        this.bpe_ranks = new Map(config.merges.map((x, i) => [x, i]));\n        this.merges = config.merges.map(x => x.split(this.BPE_SPLIT_TOKEN));\n\n        this.end_of_word_suffix = config.end_of_word_suffix;\n\n        // NOTE: `continuing_subword_suffix` is custom (to support `BlenderbotSmallTokenizer`)\n        this.continuing_subword_suffix = config.continuing_subword_suffix ?? null;\n\n        this.byte_fallback = this.config.byte_fallback ?? false;\n\n        if (this.byte_fallback) {\n            this.text_encoder = new TextEncoder();\n        }\n\n        /** @type {Map<string, string[]>} */\n        this.cache = new Map();\n    }\n\n    /**\n     * Apply Byte-Pair-Encoding (BPE) to a given token. Efficient heap-based priority\n     * queue implementation adapted from https://github.com/belladoreai/llama-tokenizer-js.\n     * @param {string} token The token to encode.\n     * @returns {string[]} The BPE encoded tokens.\n     */\n    bpe(token) {\n        if (token.length === 0) {\n            return [];\n        }\n\n        const cached = this.cache.get(token);\n        if (cached !== undefined) {\n            return cached;\n        }\n\n        const word = Array.from(token);\n        if (this.end_of_word_suffix) {\n            word[word.length - 1] += this.end_of_word_suffix;\n        }\n\n        let result = [];\n        if (word.length > 1) {\n            // Create a priority queue to store the nodes that will be merged.\n            // The comparator function compares the scores of the nodes.\n            const queue = new PriorityQueue((a, b) => a.score < b.score);\n\n            // Construct a doubly-linked list of nodes that will be inserted into the priority queue,\n            // starting with the individual characters. We also populate each node with a positional\n            // bias to break ties in the priority queue.\n            let startingNode = {\n                token: word[0],\n                bias: 0,\n                prev: null,\n                next: null,\n            }\n\n            let previousNode = startingNode\n            for (let i = 1; i < word.length; ++i) {\n                const currentNode = {\n                    bias: i / word.length, // Add fractional component to break ties\n                    token: word[i],\n                    prev: previousNode,\n                    next: null,\n                }\n                previousNode.next = currentNode\n                this._add_node(queue, previousNode)\n                previousNode = currentNode\n            }\n\n            while (!queue.isEmpty()) {\n                // Get the next node with the highest priority\n                const node = queue.pop();\n\n                // Check that this merge is still possible\n                if (node.deleted || !node.next || node.next.deleted) continue;\n\n                // Here, we mark the current node (left side of the merge) and the next node (right side of the merge) as deleted.\n                // This is because they will both be replaced by a new node representing the merge result.\n                node.deleted = true;\n                node.next.deleted = true;\n\n                // Next, we fix the node that comes before the current node (i.e., left side of the merge).\n                if (node.prev) {\n\n                    // Make a shallow copy of the previous node\n                    const newPreviousNode = { ...node.prev };\n\n                    // Mark the old previous node as deleted. This avoids erroneous merges later,\n                    // because there may still be references to this node in the priority queue.\n                    node.prev.deleted = true;\n                    node.prev = newPreviousNode;\n\n                    // Update the reference of the previous node, by pointing its previous node to this new previous node.\n                    if (newPreviousNode.prev) {\n                        newPreviousNode.prev.next = newPreviousNode;\n                    } else {\n                        // If the previous of the previous node does not exist, it means that\n                        // `newPreviousNode` must be the new `startingNode`.\n                        startingNode = newPreviousNode;\n                    }\n                }\n\n                // Create a new node which represents the result of the merge.\n                const merged = {\n                    token: node.token + node.next.token,\n                    bias: node.bias,\n                    prev: node.prev,\n                    next: node.next.next,\n                }\n\n                // We now consider where we can add the new merged node to the priority queue:\n                // 1. prev <-> merged\n                if (merged.prev) {\n                    merged.prev.next = merged;\n                    this._add_node(queue, merged.prev);\n                } else {\n                    // If `merged.prev` does not exist, then `merged` must be the new `startingNode`.\n                    startingNode = merged;\n                }\n\n                // 2. merged <-> next\n                if (merged.next) {\n                    merged.next.prev = merged;\n                    this._add_node(queue, merged);\n                }\n            }\n\n            // Traverse the linked list, starting from the `startingNode`, and collect the tokens.\n            for (let currentNode = startingNode; currentNode !== null; currentNode = currentNode.next) {\n                result.push(currentNode.token);\n            }\n        } else {\n            result = word;\n        }\n\n        // Possibly append suffix\n        if (this.continuing_subword_suffix) {\n            // Do not append suffix to the last token\n            for (let i = 0; i < result.length - 1; ++i) {\n                result[i] += this.continuing_subword_suffix;\n            }\n        }\n\n        // Save the result to the cache\n        this.cache.set(token, result);\n\n        return result;\n    }\n\n\n    /**\n     * Helper function to add a node to the priority queue.\n     * @param {PriorityQueue} queue \n     * @param {BPENode} node\n     * @private\n     */\n    _add_node(queue, node) {\n        // `score` is a measure of the merge priority: lower means higher priority\n        // We use the BPE rank as a measure of priority (i.e., the local of the merge in the merges list)\n        // We also add a fractional component to the score to break ties (with the earlier character having higher priority)\n        const rank = this.bpe_ranks.get(node.token + this.BPE_SPLIT_TOKEN + node.next.token);\n        if (rank !== undefined) {\n            node.score = rank + node.bias;\n            queue.push(node);\n        }\n    }\n\n    /**\n     * Encodes the input sequence of tokens using the BPE algorithm and returns the resulting subword tokens.\n     * @param {string[]} tokens The input sequence of tokens to encode.\n     * @returns {string[]} The resulting subword tokens after applying the BPE algorithm to the input sequence of tokens.\n     */\n    encode(tokens) {\n        let outputTokens = [];\n\n        for (let token of tokens) {\n            let bpe_token_list = this.bpe(token);\n\n            for (let t of bpe_token_list) {\n                if (this.tokens_to_ids.has(t)) {\n                    outputTokens.push(t);\n                } else {\n                    if (this.byte_fallback) {\n                        outputTokens.push(\n                            ...Array.from(this.text_encoder.encode(t))\n                                .map(x => `<0x${x.toString(16).toUpperCase().padStart(2, '0')}>`)\n                        );\n                    } else {\n                        outputTokens.push(this.unk_token);\n                    }\n                }\n            }\n        }\n\n        return outputTokens;\n    }\n\n}\n\n/**\n * Legacy tokenizer class for tokenizers with only a vocabulary.\n */\nclass LegacyTokenizerModel extends TokenizerModel {\n    /**\n     * Create a LegacyTokenizerModel instance.\n     * @param {Object} config The configuration object for LegacyTokenizerModel.\n     * @param {Object} config.vocab A (possibly nested) mapping of tokens to ids.\n     * @param {Object} moreConfig Additional configuration object for the LegacyTokenizerModel model.\n     */\n    constructor(config, moreConfig) {\n        super(config);\n\n        /**@type {Map<string, number>} */\n        this.tokens_to_ids = objectToMap(\n            moreConfig.target_lang\n                ? config.vocab[moreConfig.target_lang]\n                : config.vocab\n        );\n\n        this.bos_token = moreConfig.bos_token;\n        this.bos_token_id = this.tokens_to_ids.get(this.bos_token);\n\n        this.eos_token = moreConfig.eos_token;\n        this.eos_token_id = this.tokens_to_ids.get(this.eos_token);\n\n        this.pad_token = moreConfig.pad_token;\n        this.pad_token_id = this.tokens_to_ids.get(this.pad_token);\n\n        this.unk_token = moreConfig.unk_token;\n        this.unk_token_id = this.tokens_to_ids.get(this.unk_token);\n\n        this.vocab = new Array(this.tokens_to_ids.size);\n        for (const [key, value] of this.tokens_to_ids) {\n            this.vocab[value] = key;\n        }\n    }\n\n    encode(tokens) {\n        return tokens;\n    }\n}\n\n\n/**\n * A base class for text normalization.\n * @abstract\n */\nclass Normalizer extends Callable {\n    /**\n     * @param {Object} config The configuration object for the normalizer.\n     */\n    constructor(config) {\n        super();\n        this.config = config;\n    }\n\n    /**\n     * Factory method for creating normalizers from config objects.\n     * @static\n     * @param {Object} config The configuration object for the normalizer.\n     * @returns {Normalizer} A Normalizer object.\n     * @throws {Error} If an unknown Normalizer type is specified in the config.\n     */\n    static fromConfig(config) {\n        if (config === null) return null;\n        switch (config.type) {\n            case 'BertNormalizer':\n                return new BertNormalizer(config);\n            case 'Precompiled':\n                return new Precompiled(config);\n            case 'Sequence':\n                return new NormalizerSequence(config);\n            case 'Replace':\n                return new Replace(config);\n            case 'NFC':\n                return new NFC(config);\n            case 'NFKC':\n                return new NFKC(config);\n            case 'NFKD':\n                return new NFKD(config);\n            case 'Strip':\n                return new StripNormalizer(config);\n            case 'StripAccents':\n                return new StripAccents(config);\n            case 'Lowercase':\n                return new Lowercase(config);\n            case 'Prepend':\n                return new Prepend(config);\n            default:\n                throw new Error(`Unknown Normalizer type: ${config.type}`);\n        }\n    }\n\n    /**\n     * Normalize the input text.\n     * @abstract\n     * @param {string} text The text to normalize.\n     * @returns {string} The normalized text.\n     * @throws {Error} If this method is not implemented in a subclass.\n     */\n    normalize(text) {\n        throw Error(\"normalize should be implemented in subclass.\")\n    }\n\n    /**\n     * Alias for {@link Normalizer#normalize}.\n     * @param {string} text The text to normalize.\n     * @returns {string} The normalized text.\n     */\n    _call(text) {\n        return this.normalize(text);\n    }\n\n}\n\n/**\n * Replace normalizer that replaces occurrences of a pattern with a given string or regular expression.\n * @extends Normalizer\n */\nclass Replace extends Normalizer {\n    /**\n     * Normalize the input text by replacing the pattern with the content.\n     * @param {string} text The input text to be normalized.\n     * @returns {string} The normalized text after replacing the pattern with the content.\n     */\n    normalize(text) {\n        let pattern = createPattern(this.config.pattern);\n        if (pattern === null) {\n            return text;\n        }\n\n        text = text.replaceAll(pattern, this.config.content)\n\n        return text;\n    }\n}\n\n/**\n * A normalizer that applies Unicode normalization form C (NFC) to the input text.\n * @extends Normalizer\n */\nclass NFC extends Normalizer {\n    /**\n     * Normalize the input text by applying Unicode normalization form C (NFC).\n     * @param {string} text The input text to be normalized.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        text = text.normalize('NFC')\n        return text;\n    }\n}\n\n/**\n * NFKC Normalizer.\n * @extends Normalizer\n */\nclass NFKC extends Normalizer {\n    /**\n     * Normalize text using NFKC normalization.\n     * @param {string} text The text to be normalized.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        text = text.normalize('NFKC')\n        return text;\n    }\n}\n/**\n * NFKD Normalizer.\n * @extends Normalizer\n */\nclass NFKD extends Normalizer {\n    /**\n     * Normalize text using NFKD normalization.\n     * @param {string} text The text to be normalized.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        text = text.normalize('NFKD')\n        return text;\n    }\n}\n\n/**\n * A normalizer that strips leading and/or trailing whitespace from the input text.\n */\nclass StripNormalizer extends Normalizer {\n    /**\n     * Strip leading and/or trailing whitespace from the input text.\n     * @param {string} text The input text.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        if (this.config.strip_left && this.config.strip_right) {\n            // Fast path to avoid an extra trim call\n            text = text.trim();\n        } else {\n            if (this.config.strip_left) {\n                text = text.trimStart();\n            }\n            if (this.config.strip_right) {\n                text = text.trimEnd();\n            }\n        }\n        return text;\n    }\n}\n\n/**\n * StripAccents normalizer removes all accents from the text.\n * @extends Normalizer\n */\nclass StripAccents extends Normalizer {\n    /**\n     * Remove all accents from the text.\n     * @param {string} text The input text.\n     * @returns {string} The normalized text without accents.\n     */\n    normalize(text) {\n        text = remove_accents(text);\n        return text;\n    }\n}\n\n/**\n * A Normalizer that lowercases the input string.\n * @extends Normalizer\n */\nclass Lowercase extends Normalizer {\n    /**\n     * Lowercases the input string.\n     * @param {string} text The text to normalize.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        text = text.toLowerCase();\n        return text;\n    }\n}\n\n/**\n * A Normalizer that prepends a string to the input string.\n * @extends Normalizer\n */\nclass Prepend extends Normalizer {\n    /**\n     * Prepends the input string.\n     * @param {string} text The text to normalize.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        text = this.config.prepend + text;\n        return text;\n    }\n}\n\n/**\n * A Normalizer that applies a sequence of Normalizers.\n * @extends Normalizer\n */\nclass NormalizerSequence extends Normalizer {\n    /**\n   * Create a new instance of NormalizerSequence.\n   * @param {Object} config The configuration object.\n   * @param {Object[]} config.normalizers An array of Normalizer configuration objects.\n   */\n    constructor(config) {\n        super(config);\n        this.normalizers = config.normalizers.map(x => Normalizer.fromConfig(x));\n    }\n    /**\n    * Apply a sequence of Normalizers to the input text.\n    * @param {string} text The text to normalize.\n    * @returns {string} The normalized text.\n    */\n    normalize(text) {\n        return this.normalizers.reduce((t, normalizer) => {\n            return normalizer.normalize(t);\n        }, text);\n    }\n}\n\n/**\n * A class representing a normalizer used in BERT tokenization.\n * @extends Normalizer\n */\nclass BertNormalizer extends Normalizer {\n    /**\n     * Adds whitespace around any CJK (Chinese, Japanese, or Korean) character in the input text.\n     *\n     * @param {string} text The input text to tokenize.\n     * @returns {string} The tokenized text with whitespace added around CJK characters.\n     */\n    _tokenize_chinese_chars(text) {\n        /* Adds whitespace around any CJK character. */\n        let output = [];\n        for (let i = 0; i < text.length; ++i) {\n            let char = text[i];\n            let cp = char.charCodeAt(0);\n            if (this._is_chinese_char(cp)) {\n                output.push(\" \");\n                output.push(char);\n                output.push(\" \");\n            } else {\n                output.push(char);\n            }\n        }\n        return output.join(\"\");\n    }\n\n    /**\n     * Checks whether the given Unicode codepoint represents a CJK (Chinese, Japanese, or Korean) character.\n     *\n     * A \"chinese character\" is defined as anything in the CJK Unicode block:\n     * https://en.wikipedia.org/wiki/CJK_Unified_Ideographs_(Unicode_block)\n     *\n     * Note that the CJK Unicode block is NOT all Japanese and Korean characters, despite its name.\n     * The modern Korean Hangul alphabet is a different block, as is Japanese Hiragana and Katakana.\n     * Those alphabets are used to write space-separated words, so they are not treated specially\n     * and are handled like all other languages.\n     *\n     * @param {number} cp The Unicode codepoint to check.\n     * @returns {boolean} True if the codepoint represents a CJK character, false otherwise.\n     */\n    _is_chinese_char(cp) {\n        return (\n            (cp >= 0x4E00 && cp <= 0x9FFF)\n            || (cp >= 0x3400 && cp <= 0x4DBF)\n            || (cp >= 0x20000 && cp <= 0x2A6DF)\n            || (cp >= 0x2A700 && cp <= 0x2B73F)\n            || (cp >= 0x2B740 && cp <= 0x2B81F)\n            || (cp >= 0x2B820 && cp <= 0x2CEAF)\n            || (cp >= 0xF900 && cp <= 0xFAFF)\n            || (cp >= 0x2F800 && cp <= 0x2FA1F)\n        )\n    }\n    /**\n     * Strips accents from the given text.\n     * @param {string} text The text to strip accents from.\n     * @returns {string} The text with accents removed.\n     */\n    stripAccents(text) {\n        return text.normalize('NFD').replace(/[\\u0300-\\u036f]/g, '');\n    }\n\n    /**\n     * Normalizes the given text based on the configuration.\n     * @param {string} text The text to normalize.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        // TODO use rest of config\n        // config.clean_text,\n        // config.handle_chinese_chars,\n        // config.strip_accents,\n        // config.lowercase,\n\n        if (this.config.handle_chinese_chars) {\n            text = this._tokenize_chinese_chars(text);\n        }\n\n        if (this.config.lowercase) {\n            text = text.toLowerCase();\n\n            if (this.config.strip_accents !== false) {\n                text = this.stripAccents(text);\n            }\n        } else if (this.config.strip_accents) {\n            text = this.stripAccents(text);\n        }\n\n        return text;\n    }\n}\n\n/**\n * A callable class representing a pre-tokenizer used in tokenization. Subclasses\n * should implement the `pre_tokenize_text` method to define the specific pre-tokenization logic.\n * @extends Callable\n */\nclass PreTokenizer extends Callable {\n    /**\n   * Factory method that returns an instance of a subclass of `PreTokenizer` based on the provided configuration.\n   *\n   * @static\n   * @param {Object} config A configuration object for the pre-tokenizer.\n   * @returns {PreTokenizer} An instance of a subclass of `PreTokenizer`.\n   * @throws {Error} If the provided configuration object does not correspond to any known pre-tokenizer.\n   */\n    static fromConfig(config) {\n        if (config === null) return null;\n\n        switch (config.type) {\n            case 'BertPreTokenizer':\n                return new BertPreTokenizer(config);\n            case 'Sequence':\n                return new PreTokenizerSequence(config);\n            case 'WhitespaceSplit':\n                return new WhitespaceSplit(config);\n            case 'Metaspace':\n                return new MetaspacePreTokenizer(config);\n\n            case 'ByteLevel':\n                return new ByteLevelPreTokenizer(config);\n            case 'Split':\n                return new SplitPreTokenizer(config);\n            case 'Punctuation':\n                return new PunctuationPreTokenizer(config);\n            case 'Digits':\n                return new DigitsPreTokenizer(config);\n            case 'Replace':\n                return new ReplacePreTokenizer(config);\n            default:\n                throw new Error(`Unknown PreTokenizer type: ${config.type}`);\n        }\n    }\n\n    /**\n     * Method that should be implemented by subclasses to define the specific pre-tokenization logic.\n     *\n     * @abstract\n     * @param {string} text The text to pre-tokenize.\n     * @param {Object} [options] Additional options for the pre-tokenization logic.\n     * @returns {string[]} The pre-tokenized text.\n     * @throws {Error} If the method is not implemented in the subclass.\n     */\n    pre_tokenize_text(text, options) {\n        throw Error(\"pre_tokenize_text should be implemented in subclass.\")\n    }\n\n    /**\n     * Tokenizes the given text into pre-tokens.\n     * @param {string|string[]} text The text or array of texts to pre-tokenize.\n     * @param {Object} [options] Additional options for the pre-tokenization logic.\n     * @returns {string[]} An array of pre-tokens.\n     */\n    pre_tokenize(text, options) {\n        let result = [];\n        if (Array.isArray(text)) {\n            result = text.map(x => this.pre_tokenize_text(x, options))\n        } else {\n            result = this.pre_tokenize_text(text, options);\n        }\n        return result.flat();\n    }\n\n    /**\n     * Alias for {@link PreTokenizer#pre_tokenize}.\n     * @param {string|string[]} text The text or array of texts to pre-tokenize.\n     * @param {Object} [options] Additional options for the pre-tokenization logic.\n     * @returns {string[]} An array of pre-tokens.\n     */\n    _call(text, options) {\n        return this.pre_tokenize(text, options);\n    }\n}\n\n/**\n * @extends PreTokenizer\n */\nclass BertPreTokenizer extends PreTokenizer {\n    /**\n     * A PreTokenizer that splits text into wordpieces using a basic tokenization scheme\n     * similar to that used in the original implementation of BERT.\n     * \n     * @param {Object} config The configuration object.\n     */\n    constructor(config) {\n        super();\n        // Construct a pattern which matches the rust implementation:\n        // https://github.com/huggingface/tokenizers/blob/b4fcc9ce6e4ad5806e82826f816acfdfdc4fcc67/tokenizers/src/pre_tokenizers/bert.rs#L11\n        // Equivalent to removing whitespace and splitting on punctuation (both \\p{P} and other ascii characters)\n        this.pattern = new RegExp(`[^\\\\s${PUNCTUATION_REGEX}]+|[${PUNCTUATION_REGEX}]`, 'gu');\n    }\n    /**\n     * Tokenizes a single text using the BERT pre-tokenization scheme.\n     * \n     * @param {string} text The text to tokenize.\n     * @param {Object} [options] Additional options for the pre-tokenization logic.\n     * @returns {string[]} An array of tokens.\n     */\n    pre_tokenize_text(text, options) {\n        return text.trim().match(this.pattern) || [];\n    }\n}\n\n/**\n * A pre-tokenizer that splits text into Byte-Pair-Encoding (BPE) subwords.\n * @extends PreTokenizer\n */\nclass ByteLevelPreTokenizer extends PreTokenizer {\n    /**\n     * Creates a new instance of the `ByteLevelPreTokenizer` class.\n     * @param {Object} config The configuration object.\n     */\n    constructor(config) {\n        super();\n        this.config = config;\n\n        /**\n         * @type {boolean} Whether to add a leading space to the first word.\n         * This allows to treat the leading word just as any other word.\n         */\n        this.add_prefix_space = this.config.add_prefix_space;\n\n        /**\n         * @type {boolean} Whether the post processing step should trim offsets\n         * to avoid including whitespaces.\n         * @todo Use this in the pretokenization step.\n         */\n        this.trim_offsets = this.config.trim_offsets;\n\n        /**\n         * @type {boolean} Whether to use the standard GPT2 regex for whitespace splitting.\n         * Set it to False if you want to use your own splitting. Defaults to true.\n         */\n        this.use_regex = this.config.use_regex ?? true;\n        this.pattern = /'s|'t|'re|'ve|'m|'ll|'d| ?\\p{L}+| ?\\p{N}+| ?[^\\s\\p{L}\\p{N}]+|\\s+(?!\\S)|\\s+/gu;\n\n        this.byte_encoder = BYTES_TO_UNICODE;\n        this.text_encoder = new TextEncoder();\n    }\n\n    /**\n     * Tokenizes a single piece of text using byte-level tokenization.\n     * @param {string} text The text to tokenize.\n     * @param {Object} [options] Additional options for the pre-tokenization logic.\n     * @returns {string[]} An array of tokens.\n     */\n    pre_tokenize_text(text, options) {\n        // Add a leading space if the option is enabled\n        if (this.add_prefix_space && !text.startsWith(' ')) {\n            text = ' ' + text;\n        }\n\n        // Split on whitespace and punctuation\n        let tokens = this.use_regex ? (text.match(this.pattern) || []) : [text];\n\n        // Maps all our bytes to unicode strings, avoiding control tokens of the BPE (spaces in our case)\n        return tokens.map(\n            token => Array.from(this.text_encoder.encode(token), byte => this.byte_encoder[byte]).join('')\n        );\n    }\n}\n\n/**\n * @typedef {'removed'|'isolated'|'mergedWithPrevious'|'mergedWithNext'|'contiguous'} SplitDelimiterBehavior\n */\n\n/**\n * Splits text using a given pattern.\n * @extends PreTokenizer\n */\nclass SplitPreTokenizer extends PreTokenizer {\n    /**\n     * @param {Object} config The configuration options for the pre-tokenizer.\n     * @param {Object} config.pattern The pattern used to split the text. Can be a string or a regex object.\n     * @param {string|undefined} config.pattern.String The string to use for splitting. Only defined if the pattern is a string.\n     * @param {string|undefined} config.pattern.Regex The regex to use for splitting. Only defined if the pattern is a regex.\n     * @param {SplitDelimiterBehavior} config.behavior The behavior to use when splitting.\n     * @param {boolean} config.invert Whether to split (invert=false) or match (invert=true) the pattern.\n     */\n    constructor(config) {\n        super();\n        this.config = config;\n        // TODO support all behaviours (config.behavior)\n\n        this.pattern = createPattern(this.config.pattern, this.config.invert);\n    }\n\n    /**\n     * Tokenizes text by splitting it using the given pattern.\n     * @param {string} text The text to tokenize.\n     * @param {Object} [options] Additional options for the pre-tokenization logic.\n     * @returns {string[]} An array of tokens.\n     */\n    pre_tokenize_text(text, options) {\n        if (this.pattern === null) {\n            return [];\n        }\n\n        if (this.config.invert) {\n            return text.match(this.pattern) || [];\n        } else {\n            return regexSplit(text, this.pattern);\n        }\n    }\n}\n\n/**\n * Splits text based on punctuation.\n * @extends PreTokenizer\n */\nclass PunctuationPreTokenizer extends PreTokenizer {\n    /**\n     * @param {Object} config The configuration options for the pre-tokenizer.\n     * @param {SplitDelimiterBehavior} config.behavior The behavior to use when splitting.\n     */\n    constructor(config) {\n        super();\n        this.config = config;\n        this.pattern = new RegExp(`[^${PUNCTUATION_REGEX}]+|[${PUNCTUATION_REGEX}]+`, 'gu');\n    }\n\n    /**\n     * Tokenizes text by splitting it using the given pattern.\n     * @param {string} text The text to tokenize.\n     * @param {Object} [options] Additional options for the pre-tokenization logic.\n     * @returns {string[]} An array of tokens.\n     */\n    pre_tokenize_text(text, options) {\n        return text.match(this.pattern) || [];\n    }\n}\n\n\n/**\n * Splits text based on digits.\n * @extends PreTokenizer\n */\nclass DigitsPreTokenizer extends PreTokenizer {\n    /**\n     * @param {Object} config The configuration options for the pre-tokenizer.\n     * @param {boolean} config.individual_digits Whether to split on individual digits.\n     */\n    constructor(config) {\n        super();\n        this.config = config;\n\n        // Construct a pattern which matches the rust implementation:\n        const digit_pattern = `[^\\\\d]+|\\\\d${this.config.individual_digits ? '' : '+'}`;\n        this.pattern = new RegExp(digit_pattern, 'gu');\n    }\n\n    /**\n     * Tokenizes text by splitting it using the given pattern.\n     * @param {string} text The text to tokenize.\n     * @param {Object} [options] Additional options for the pre-tokenization logic.\n     * @returns {string[]} An array of tokens.\n     */\n    pre_tokenize_text(text, options) {\n        return text.match(this.pattern) || [];\n    }\n}\n\n/**\n * @extends Callable\n */\nclass PostProcessor extends Callable {\n\n    /**\n     * @param {Object} config The configuration for the post-processor.\n     */\n    constructor(config) {\n        super();\n        this.config = config;\n    }\n\n    /**\n     * Factory method to create a PostProcessor object from a configuration object.\n     *\n     * @param {Object} config Configuration object representing a PostProcessor.\n     * @returns {PostProcessor} A PostProcessor object created from the given configuration.\n     * @throws {Error} If an unknown PostProcessor type is encountered.\n     */\n    static fromConfig(config) {\n        if (config === null) return null;\n        switch (config.type) {\n            case 'TemplateProcessing':\n                return new TemplateProcessing(config);\n\n            case 'ByteLevel':\n                return new ByteLevelPostProcessor(config);\n\n            case 'RobertaProcessing':\n                return new RobertaProcessing(config);\n            case 'BertProcessing':\n                return new BertProcessing(config);\n\n            default:\n                throw new Error(`Unknown PostProcessor type: ${config.type}`);\n        }\n    }\n\n    /**\n     * Method to be implemented in subclass to apply post-processing on the given tokens.\n     *\n     * @param {Array} tokens The input tokens to be post-processed.\n     * @param {...*} args Additional arguments required by the post-processing logic.\n     * @returns {Array} The post-processed tokens.\n     * @throws {Error} If the method is not implemented in subclass.\n     */\n    post_process(tokens, ...args) {\n        throw Error(\"post_process should be implemented in subclass.\")\n    }\n\n    /**\n     * Alias for {@link PostProcessor#post_process}.\n     * @param {Array} tokens The text or array of texts to post-process.\n     * @param {...*} args Additional arguments required by the post-processing logic.\n     * @returns {Array} An array of post-processed tokens.\n     */\n    _call(tokens, ...args) {\n        return this.post_process(tokens, ...args);\n    }\n}\n\n/**\n * A post-processor that adds special tokens to the beginning and end of the input.\n */\nclass BertProcessing extends PostProcessor {\n    /**\n     * @param {Object} config The configuration for the post-processor.\n     * @param {string[]} config.cls The special tokens to add to the beginning of the input.\n     * @param {string[]} config.sep The special tokens to add to the end of the input.\n     */\n    constructor(config) {\n        super(config);\n        // TODO use all of config: add_prefix_space, trim_offsets\n\n        this.cls = config.cls[0];\n        this.sep = config.sep[0];\n    }\n\n    /**\n     * Adds the special tokens to the beginning and end of the input.\n     * @param {string[]} tokens The input tokens.\n     * @param {string[]|null} tokens_pair An optional second set of input tokens.\n     * @returns {string[]} The input tokens with the special tokens added to the beginning and end.\n     */\n    post_process(tokens, tokens_pair = null) {\n        tokens = mergeArrays([this.cls], tokens, [this.sep]);\n\n        // NOTE: It is intended to add 2 EOS tokens after the first set of tokens\n        // https://github.com/huggingface/tokenizers/issues/983\n        if (tokens_pair !== null) {\n            tokens = mergeArrays(tokens, [this.sep], tokens_pair, [this.sep]);\n        }\n        return tokens;\n    }\n}\nclass RobertaProcessing extends BertProcessing { } // NOTE: extends BertProcessing\n\n/**\n * Post processor that replaces special tokens in a template with actual tokens.\n * @extends PostProcessor\n */\nclass TemplateProcessing extends PostProcessor {\n    /**\n     * Creates a new instance of `TemplateProcessing`.\n     * @param {Object} config The configuration options for the post processor.\n     * @param {Array} config.single The template for a single sequence of tokens.\n     * @param {Array} config.pair The template for a pair of sequences of tokens.\n     */\n    constructor(config) {\n        super(config);\n\n        this.single = config.single;\n        this.pair = config.pair;\n    }\n\n    /**\n     * Replaces special tokens in the template with actual tokens.\n     * @param {Array} tokens The list of tokens for the first sequence.\n     * @param {Array} [tokens_pair=null] The list of tokens for the second sequence (optional).\n     * @returns {Array} The list of tokens with the special tokens replaced with actual tokens.\n     */\n    post_process(tokens, tokens_pair = null) {\n        let type = tokens_pair === null ? this.single : this.pair\n\n        let toReturn = [];\n        for (let item of type) {\n            if ('SpecialToken' in item) {\n                toReturn.push(item.SpecialToken.id);\n\n            } else if ('Sequence' in item) {\n                if (item.Sequence.id === 'A') {\n                    toReturn = mergeArrays(toReturn, tokens);\n\n                } else if (item.Sequence.id === 'B') {\n                    toReturn = mergeArrays(toReturn, tokens_pair);\n                }\n            }\n        }\n        return toReturn;\n    }\n}\n\n/**\n * A PostProcessor that returns the given tokens as is.\n * @extends PostProcessor\n */\nclass ByteLevelPostProcessor extends PostProcessor {\n    /**\n     * Post process the given tokens.\n     * @param {string[]} tokens The tokens to be post processed.\n     * @returns {string[]} The post processed tokens.\n     */\n    post_process(tokens) {\n        return tokens;\n    }\n}\n\n/**\n * The base class for token decoders.\n * @extends Callable\n */\nclass Decoder extends Callable {\n\n    /**\n    * Creates an instance of `Decoder`.\n    *\n    * @param {Object} config The configuration object.\n    */\n    constructor(config) {\n        super();\n        this.config = config;\n\n        /** @type {AddedToken[]} */\n        this.added_tokens = [];\n        this.end_of_word_suffix = null;\n        this.trim_offsets = config.trim_offsets;\n    }\n\n    /**\n   * Creates a decoder instance based on the provided configuration.\n   *\n   * @param {Object} config The configuration object.\n   * @returns {Decoder} A decoder instance.\n   * @throws {Error} If an unknown decoder type is provided.\n   */\n    static fromConfig(config) {\n        if (config === null) return null;\n        switch (config.type) {\n            case 'WordPiece':\n                return new WordPieceDecoder(config);\n            case 'Metaspace':\n                return new MetaspaceDecoder(config);\n            case 'ByteLevel':\n                return new ByteLevelDecoder(config);\n\n            case 'Replace':\n                return new ReplaceDecoder(config);\n            case 'ByteFallback':\n                return new ByteFallback(config);\n            case 'Fuse':\n                return new FuseDecoder(config);\n            case 'Strip':\n                return new StripDecoder(config);\n\n            case 'Sequence':\n                return new DecoderSequence(config);\n\n            case 'CTC':\n                return new CTCDecoder(config);\n            case 'BPEDecoder':\n                return new BPEDecoder(config);\n            default:\n                throw new Error(`Unknown Decoder type: ${config.type}`);\n        }\n    }\n\n    /**\n    * Calls the `decode` method.\n    *\n    * @param {string[]} tokens The list of tokens.\n    * @returns {string} The decoded string.\n    */\n    _call(tokens) {\n        return this.decode(tokens);\n    }\n\n    /**\n    * Decodes a list of tokens.\n    * @param {string[]} tokens The list of tokens.\n    * @returns {string} The decoded string.\n    */\n    decode(tokens) {\n        return this.decode_chain(tokens).join('');\n    }\n\n    /**\n     * Apply the decoder to a list of tokens.\n     * \n     * @param {string[]} tokens The list of tokens.\n     * @returns {string[]} The decoded list of tokens.\n     * @throws {Error} If the `decode_chain` method is not implemented in the subclass.\n     */\n    decode_chain(tokens) {\n        throw Error(\"`decode_chain` should be implemented in subclass.\")\n    }\n\n}\n\nclass ReplaceDecoder extends Decoder {\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        let pattern = createPattern(this.config.pattern);\n        if (pattern === null) {\n            return tokens;\n        }\n\n        return tokens.map(token => token.replaceAll(pattern, this.config.content))\n    }\n}\n\n\nclass ByteFallback extends Decoder {\n    constructor(config) {\n        super(config);\n\n        this.text_decoder = new TextDecoder();\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n\n        let new_tokens = [];\n        let previous_byte_tokens = [];\n\n        for (let token of tokens) {\n            let bytes = null;\n            if (token.length === 6 && token.startsWith('<0x') && token.endsWith('>')) {\n                let byte = parseInt(token.slice(3, 5), 16);\n                if (!isNaN(byte)) {\n                    bytes = byte;\n                }\n            }\n            if (bytes !== null) {\n                previous_byte_tokens.push(bytes);\n            } else {\n                if (previous_byte_tokens.length > 0) {\n                    let string = this.text_decoder.decode(Uint8Array.from(previous_byte_tokens));\n                    new_tokens.push(string);\n                    previous_byte_tokens = [];\n                }\n                new_tokens.push(token);\n            }\n        }\n        if (previous_byte_tokens.length > 0) {\n            let string = this.text_decoder.decode(Uint8Array.from(previous_byte_tokens));\n            new_tokens.push(string);\n            previous_byte_tokens = [];\n        }\n\n        return new_tokens;\n    }\n}\n\n/**\n * Fuse simply fuses all tokens into one big string.\n * It's usually the last decoding step anyway, but this decoder\n * exists incase some decoders need to happen after that step\n */\nclass FuseDecoder extends Decoder {\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        return [tokens.join('')];\n    }\n}\n\n\nclass StripDecoder extends Decoder {\n    constructor(config) {\n        super(config);\n\n        this.content = this.config.content;\n        this.start = this.config.start;\n        this.stop = this.config.stop;\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        return tokens.map(token => {\n            let start_cut = 0;\n            for (let i = 0; i < this.start; ++i) {\n                if (token[i] === this.content) {\n                    start_cut = i + 1;\n                    continue;\n                } else {\n                    break;\n                }\n            }\n\n            let stop_cut = token.length;\n            for (let i = 0; i < this.stop; ++i) {\n                const index = token.length - i - 1;\n                if (token[index] === this.content) {\n                    stop_cut = index;\n                    continue;\n                } else {\n                    break;\n                }\n            }\n\n            return token.slice(start_cut, stop_cut)\n        });\n    }\n}\n\n/**\n * A decoder that decodes a list of WordPiece tokens into a single string.\n * @extends Decoder\n */\nclass WordPieceDecoder extends Decoder {\n\n    /**\n     * Creates a new instance of WordPieceDecoder.\n     * @param {Object} config The configuration object.\n     * @param {string} config.prefix The prefix used for WordPiece encoding.\n     * @param {boolean} config.cleanup Whether to cleanup the decoded string.\n     */\n    constructor(config) {\n        super(config);\n        this.cleanup = config.cleanup;\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        return tokens.map((token, i) => {\n            if (i !== 0) {\n                if (token.startsWith(this.config.prefix)) {\n                    // NOTE: .replace() is intended; only replace first occurrence\n                    token = token.replace(this.config.prefix, '');\n                } else {\n                    token = ' ' + token;\n                }\n            }\n            if (this.cleanup) {\n                token = clean_up_tokenization(token)\n            }\n\n            return token;\n        });\n    }\n}\n\n/**\n * Byte-level decoder for tokenization output. Inherits from the `Decoder` class.\n * @extends Decoder\n */\nclass ByteLevelDecoder extends Decoder {\n\n    /**\n     * Create a `ByteLevelDecoder` object.\n     * @param {Object} config Configuration object.\n     */\n    constructor(config) {\n        super(config);\n\n        this.byte_decoder = UNICODE_TO_BYTES;\n        this.text_decoder = new TextDecoder(\"utf-8\", {\n            fatal: false,\n            ignoreBOM: true,\n        });\n\n        this.end_of_word_suffix = null;\n    }\n\n    /**\n     * Convert an array of tokens to string by decoding each byte.\n     * @param {string[]} tokens Array of tokens to be decoded.\n     * @returns {string} The decoded string.\n     */\n    convert_tokens_to_string(tokens) {\n        let text = tokens.join('');\n\n        let byteArray = new Uint8Array([...text].map(c => this.byte_decoder[c]));\n        let decoded_text = this.text_decoder.decode(byteArray);\n        return decoded_text;\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        // TODO move to base class (like HF)\n        // tokens === filtered_tokens\n\n        // To avoid mixing byte-level and unicode for byte-level BPT\n        // we need to build string separately for added tokens and byte-level tokens\n        // cf. https://github.com/huggingface/transformers/issues/1133\n        let sub_texts = [];\n        let current_sub_text = [];\n        for (let token of tokens) {\n            // tokens sent here are already filtered, so we don't need to do this\n            // if (skip_special_tokens && this.all_special_ids.includes(token)) {\n            //     continue;\n            // }\n\n            if (this.added_tokens.find(x => x.content === token) !== undefined) {\n                if (current_sub_text.length > 0) {\n                    sub_texts.push(this.convert_tokens_to_string(current_sub_text));\n                    current_sub_text = [];\n                }\n                sub_texts.push(token);\n            } else {\n                current_sub_text.push(token);\n            }\n        }\n        if (current_sub_text.length > 0) {\n            sub_texts.push(this.convert_tokens_to_string(current_sub_text));\n        }\n\n        // TODO add spaces_between_special_tokens and clean_up_tokenization_spaces options\n\n        return sub_texts;\n    }\n}\n\n/**\n * The CTC (Connectionist Temporal Classification) decoder.\n * See https://github.com/huggingface/tokenizers/blob/bb38f390a61883fc2f29d659af696f428d1cda6b/tokenizers/src/decoders/ctc.rs\n */\nclass CTCDecoder extends Decoder {\n\n    constructor(config) {\n        super(config);\n\n        this.pad_token = this.config.pad_token;\n        this.word_delimiter_token = this.config.word_delimiter_token;\n        this.cleanup = this.config.cleanup;\n    }\n    /**\n     * Converts a connectionist-temporal-classification (CTC) output tokens into a single string.\n     * @param {string[]} tokens Array of tokens to be decoded.\n     * @returns {string} The decoded string.\n     */\n    convert_tokens_to_string(tokens) {\n        if (tokens.length === 0) return '';\n\n        // group same tokens into non-repeating tokens in CTC style decoding\n        let grouped_tokens = [tokens[0]];\n        for (let i = 1; i < tokens.length; ++i) {\n            if (tokens[i] !== grouped_tokens.at(-1)) {\n                grouped_tokens.push(tokens[i]);\n            }\n        }\n\n        // filter self.pad_token which is used as CTC-blank token\n        let filtered_tokens = grouped_tokens.filter(token => token !== this.pad_token);\n\n        let text = filtered_tokens.join('');\n        if (this.cleanup) {\n            // cleanup and replace delimiter token\n            text = clean_up_tokenization(text)\n                .replaceAll(this.word_delimiter_token, ' ')\n                .trim();\n        }\n        return text;\n    }\n\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        return [this.convert_tokens_to_string(tokens)];\n    }\n}\n\n/**\n * Apply a sequence of decoders.\n * @extends Decoder\n */\nclass DecoderSequence extends Decoder {\n\n    /**\n     * Creates a new instance of DecoderSequence.\n     * @param {Object} config The configuration object.\n     * @param {Decoder[]} config.decoders The list of decoders to apply.\n     */\n    constructor(config) {\n        super(config);\n        this.decoders = config.decoders.map(x => Decoder.fromConfig(x));\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        // Use reduce to apply each decoder to the tokens\n        return this.decoders.reduce((toks, decoder) => {\n            return decoder.decode_chain(toks);\n        }, tokens);\n    }\n\n}\n\nclass BPEDecoder extends Decoder {\n    constructor(config) {\n        super(config);\n\n        this.suffix = this.config.suffix;\n    }\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        return tokens.map((token, i) => {\n            return token.replaceAll(this.suffix, (i === tokens.length - 1) ? '' : ' ')\n        });\n    }\n}\n\n\n/**\n * This PreTokenizer replaces spaces with the given replacement character, adds a prefix space if requested,\n * and returns a list of tokens.\n * @extends PreTokenizer\n */\nclass MetaspacePreTokenizer extends PreTokenizer {\n    /**\n     * @param {Object} config The configuration object for the MetaspacePreTokenizer.\n     * @param {boolean} config.add_prefix_space Whether to add a prefix space to the first token.\n     * @param {string} config.replacement The character to replace spaces with.\n     * @param {string} [config.str_rep=config.replacement] An optional string representation of the replacement character.\n     * @param {'first'|'never'|'always'} [config.prepend_scheme='always'] The metaspace prepending scheme.\n     */\n    constructor(config) {\n        super();\n\n        this.addPrefixSpace = config.add_prefix_space;\n        this.replacement = config.replacement;\n        this.strRep = config.str_rep || this.replacement;\n        this.prepend_scheme = config.prepend_scheme ?? 'always';\n    }\n\n    /**\n     * This method takes a string, replaces spaces with the replacement character,\n     * adds a prefix space if requested, and returns a new list of tokens.\n     * @param {string} text The text to pre-tokenize.\n     * @param {Object} [options] The options for the pre-tokenization.\n     * @param {number} [options.section_index] The index of the section to pre-tokenize.\n     * @returns {string[]} A new list of pre-tokenized tokens.\n     */\n    pre_tokenize_text(text, {\n        section_index = undefined,\n    } = {}) {\n\n        let normalized = text.replaceAll(' ', this.strRep);\n\n        if (\n            // We add a prefix space if:\n            //  (1) The addPrefixSpace option is enabled and the normalized\n            //      token does not already start with the replacement character.\n            (this.addPrefixSpace && !normalized.startsWith(this.replacement))\n\n            // and (2) either:\n            //  (a) prepend_scheme is 'always'\n            //  (b) prepend_scheme is 'first' and this is the first section\n            && (\n                this.prepend_scheme === 'always' ||\n                (this.prepend_scheme === 'first' && section_index === 0)\n            )\n        ) {\n            normalized = this.strRep + normalized;\n        }\n        return [normalized];\n    }\n}\n\n/**\n * MetaspaceDecoder class extends the Decoder class and decodes Metaspace tokenization.\n * @extends Decoder\n */\nclass MetaspaceDecoder extends Decoder {\n    /**\n     * Constructs a new MetaspaceDecoder object.\n     * @param {Object} config The configuration object for the MetaspaceDecoder.\n     * @param {boolean} config.add_prefix_space Whether to add a prefix space to the decoded string.\n     * @param {string} config.replacement The string to replace spaces with.\n     */\n    constructor(config) {\n        super(config);\n\n        this.addPrefixSpace = config.add_prefix_space;\n        this.replacement = config.replacement;\n    }\n\n    /** @type {Decoder['decode_chain']} */\n    decode_chain(tokens) {\n        let result = [];\n        for (let i = 0; i < tokens.length; ++i) {\n            let normalized = tokens[i].replaceAll(this.replacement, ' ');\n            if (this.addPrefixSpace && i == 0 && normalized.startsWith(' ')) {\n                normalized = normalized.substring(1);\n            }\n            result.push(normalized);\n        }\n        return result;\n    }\n}\n\n/**\n * A normalizer that applies a precompiled charsmap.\n * This is useful for applying complex normalizations in C++ and exposing them to JavaScript.\n * @extends Normalizer\n * @param {Object} config The configuration object for the Precompiled normalizer.\n * @param {Object} config.precompiled_charsmap The precompiled charsmap object.\n */\nclass Precompiled extends Normalizer {\n    /**\n     * Create a new instance of Precompiled normalizer.\n     * @param {Object} config The configuration object.\n     * @param {any} config.precompiled_charsmap Precompiled chars mapping.\n     */\n    constructor(config) {\n        super(config);\n        this.charsmap = config.precompiled_charsmap;\n    }\n\n    /**\n     * Normalizes the given text by applying the precompiled charsmap.\n     * @param {string} text The text to normalize.\n     * @returns {string} The normalized text.\n     */\n    normalize(text) {\n        // As stated in the sentencepiece normalization docs (https://github.com/google/sentencepiece/blob/master/doc/normalization.md#use-pre-defined-normalization-rule),\n        // there are 5 pre-defined normalization rules:\n        //  1. nmt_nfkc: NFKC normalization with some additional normalization around spaces. (default)\n        //  2. nfkc: original NFKC normalization.\n        //  3. nmt_nfkc_cf: nmt_nfkc + Unicode case folding (mostly lower casing)\n        //  4. nfkc_cf: nfkc + Unicode case folding.\n        //  5. identity: no normalization\n        // \n        // For now, we only implement the default (nmt_nfkc).\n        // See https://raw.githubusercontent.com/google/sentencepiece/master/data/nmt_nfkc.tsv for the full list of rules.\n        // TODO: detect when a different `this.charsmap` is used.\n\n        text = text.replace(/[\\u0001-\\u0008\\u000B\\u000E-\\u001F\\u007F\\u008F\\u009F]/gm, ''); // Remove control characters\n        text = text.replace(/[\\u0009\\u000A\\u000C\\u000D\\u1680\\u200B\\u200C\\u200E\\u200F\\u2028\\u2029\\u2581\\uFEFF\\uFFFD]/gm, '\\u0020'); // Replace certain characters with a space\n\n        if (text.includes('\\uFF5E')) {\n            // To match the sentencepiece implementation 100%, we must handle a very strange edge-case.\n            // For some reason, the \"Fullwidth Tilde\" character (\\uFF5E) should not be converted to the standard Tilde character (\\u007E).\n            // However, NFKC normalization does do this conversion. As a result, we split the string on the Fullwidth Tilde character,\n            // perform NFKC normalization on each substring, and then join them back together with the Fullwidth Tilde character.\n            const parts = text.split('\\uFF5E');\n            text = parts.map(part => part.normalize('NFKC')).join('\\uFF5E');\n        } else {\n            text = text.normalize('NFKC');\n        }\n\n        return text;\n    }\n}\n\n/**\n * A pre-tokenizer that applies a sequence of pre-tokenizers to the input text.\n * @extends PreTokenizer\n */\nclass PreTokenizerSequence extends PreTokenizer {\n    /**\n     * Creates an instance of PreTokenizerSequence.\n     * @param {Object} config The configuration object for the pre-tokenizer sequence.\n     * @param {Object[]} config.pretokenizers An array of pre-tokenizer configurations.\n     */\n    constructor(config) {\n        super();\n        this.tokenizers = config.pretokenizers.map(x => PreTokenizer.fromConfig(x));\n    }\n\n    /**\n     * Applies each pre-tokenizer in the sequence to the input text in turn.\n     * @param {string} text The text to pre-tokenize.\n     * @param {Object} [options] Additional options for the pre-tokenization logic.\n     * @returns {string[]} The pre-tokenized text.\n     */\n    pre_tokenize_text(text, options) {\n        // Use reduce to apply each tokenizer to the text\n        return this.tokenizers.reduce((preTokenizedText, tokenizer) => {\n            return tokenizer.pre_tokenize(preTokenizedText, options);\n        }, [text]);\n    }\n}\n\n/**\n * Splits a string of text by whitespace characters into individual tokens.\n * @extends PreTokenizer\n */\nclass WhitespaceSplit extends PreTokenizer {\n    /**\n     * Creates an instance of WhitespaceSplit.\n     * @param {Object} config The configuration object for the pre-tokenizer sequence.\n     */\n    constructor(config) {\n        super();\n    }\n    /**\n     * Pre-tokenizes the input text by splitting it on whitespace characters.\n     * @param {string} text The text to be pre-tokenized.\n     * @param {Object} [options] Additional options for the pre-tokenization logic.\n     * @returns {string[]} An array of tokens produced by splitting the input text on whitespace.\n     */\n    pre_tokenize_text(text, options) {\n        return whitespace_split(text);\n    }\n}\n\n// NOTE: `ReplacePreTokenizer` is custom (to support `BlenderbotSmallTokenizer`)\nclass ReplacePreTokenizer extends PreTokenizer {\n    /**\n     * @param {Object} config The configuration options for the pre-tokenizer.\n     * @param {Object} config.pattern The pattern used to split the text. Can be a string or a regex object.\n     * @param {string} config.content What to replace the pattern with.\n     */\n    constructor(config) {\n        super();\n        this.config = config;\n        this.pattern = createPattern(this.config.pattern);\n        this.content = this.config.content;\n    }\n\n    /**\n     * Pre-tokenizes the input text by replacing certain characters.\n     * @param {string} text The text to be pre-tokenized.\n     * @param {Object} [options] Additional options for the pre-tokenization logic.\n     * @returns {string[]} An array of tokens produced by replacing certain characters.\n     */\n    pre_tokenize_text(text, options) {\n        if (this.pattern === null) {\n            return [text];\n        }\n        return [text.replaceAll(this.pattern, this.config.content)];\n    }\n}\n\nconst SPECIAL_TOKEN_ATTRIBUTES = [\n    'bos_token',\n    'eos_token',\n    'unk_token',\n    'sep_token',\n    'pad_token',\n    'cls_token',\n    'mask_token',\n    // additional_special_tokens (TODO)\n]\n\nexport class PreTrainedTokenizer extends Callable {\n    _default_chat_template = `{% for message in messages %}{{'<|im_start|>' + message['role'] + '\\n' + message['content'] + '<|im_end|>' + '\\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\\n' }}{% endif %}`;\n\n    /**\n     * Create a new PreTrainedTokenizer instance.\n     * @param {Object} tokenizerJSON The JSON of the tokenizer.\n     * @param {Object} tokenizerConfig The config of the tokenizer.\n     */\n    constructor(tokenizerJSON, tokenizerConfig) {\n        super();\n\n        this._tokenizer_config = tokenizerConfig;\n\n        // Construct parts of the tokenizer from the JSON\n        this.normalizer = Normalizer.fromConfig(tokenizerJSON.normalizer);\n        this.pre_tokenizer = PreTokenizer.fromConfig(tokenizerJSON.pre_tokenizer);\n\n        this.model = TokenizerModel.fromConfig(tokenizerJSON.model, tokenizerConfig);\n        this.post_processor = PostProcessor.fromConfig(tokenizerJSON.post_processor);\n\n        // TODO: maybe, allow this to be null; in which case, we use model as decoder too?\n        this.decoder = Decoder.fromConfig(tokenizerJSON.decoder);\n\n        // Add added_tokens to model\n        this.special_tokens = [];\n        this.all_special_ids = [];\n\n        /** @type {AddedToken[]} */\n        this.added_tokens = [];\n        for (let addedToken of tokenizerJSON.added_tokens) {\n            const token = new AddedToken(addedToken);\n            this.added_tokens.push(token);\n\n            this.model.tokens_to_ids.set(token.content, token.id);\n            this.model.vocab[token.id] = token.content;\n\n            if (token.special) {\n                this.special_tokens.push(token.content);\n                this.all_special_ids.push(token.id);\n            }\n        }\n\n        // Update additional_special_tokens\n        this.additional_special_tokens = tokenizerConfig.additional_special_tokens ?? [];\n        this.special_tokens.push(...this.additional_special_tokens);\n        this.special_tokens = [...new Set(this.special_tokens)]; // Remove duplicates\n\n        if (this.decoder) {\n            // Slight hack, but it prevents code duplication:\n            this.decoder.added_tokens = this.added_tokens;\n\n            // Another slight hack to add `end_of_word_suffix` (if present) to the decoder\n            // This is needed for cases where BPE model and ByteLevel decoder are used\n            // For more information, see https://github.com/xenova/transformers.js/issues/74\n            // TODO: save this to the decoder when exporting?\n            this.decoder.end_of_word_suffix = this.model.end_of_word_suffix;\n        }\n\n\n        this.added_tokens_regex = this.added_tokens.length > 0 ? new RegExp(\n            this.added_tokens.map(x => `${x.lstrip ? '\\\\s*' : ''}(${escapeRegExp(x.content)})${x.rstrip ? '\\\\s*' : ''}`).join('|')\n        ) : null;\n\n        // Set mask token if present (otherwise will be undefined, which is fine)\n        this.mask_token = this.getToken('mask_token');\n        this.mask_token_id = this.model.tokens_to_ids.get(this.mask_token);\n\n        this.pad_token = this.getToken('pad_token', 'eos_token');\n        this.pad_token_id = this.model.tokens_to_ids.get(this.pad_token);\n\n        this.sep_token = this.getToken('sep_token');\n        this.sep_token_id = this.model.tokens_to_ids.get(this.sep_token);\n\n        this.unk_token = this.getToken(tokenizerConfig, 'unk_token');\n        this.unk_token_id = this.model.tokens_to_ids.get(this.unk_token);\n\n        this.model_max_length = tokenizerConfig.model_max_length;\n\n        /** @type {boolean} Whether or not to strip the text when tokenizing (removing excess spaces before and after the string). */\n        this.remove_space = tokenizerConfig.remove_space;\n\n        this.clean_up_tokenization_spaces = tokenizerConfig.clean_up_tokenization_spaces ?? true;\n        this.do_lowercase_and_remove_accent = tokenizerConfig.do_lowercase_and_remove_accent ?? false;\n\n        // TODO allow user to change this\n        this.padding_side = 'right';\n\n        this.legacy = false;\n\n        this.chat_template = tokenizerConfig.chat_template ?? null;\n        this._compiled_template_cache = new Map();\n    }\n\n    /**\n     * Returns the value of the first matching key in the tokenizer config object.\n     * @param {...string} keys One or more keys to search for in the tokenizer config object.\n     * @returns {string|null} The value associated with the first matching key, or null if no match is found.\n     * @throws {Error} If an object is found for a matching key and its __type property is not \"AddedToken\".\n     */\n    getToken(...keys) {\n        for (let key of keys) {\n            let item = this._tokenizer_config[key];\n\n            if (!item) continue;\n\n            if (typeof item === 'object') {\n                if (item.__type === 'AddedToken') {\n                    return item.content;\n                } else {\n                    throw Error(`Unknown token: ${item}`);\n                }\n            } else {\n                return item;\n            }\n        }\n        return null;\n    }\n\n    /**\n     * Loads a pre-trained tokenizer from the given `pretrained_model_name_or_path`. \n     * \n     * @param {string} pretrained_model_name_or_path The path to the pre-trained tokenizer.\n     * @param {PretrainedTokenizerOptions} options Additional options for loading the tokenizer.\n     * \n     * @throws {Error} Throws an error if the tokenizer.json or tokenizer_config.json files are not found in the `pretrained_model_name_or_path`.\n     * @returns {Promise<PreTrainedTokenizer>} A new instance of the `PreTrainedTokenizer` class.\n     */\n    static async from_pretrained(pretrained_model_name_or_path, {\n        progress_callback = null,\n        config = null,\n        cache_dir = null,\n        local_files_only = false,\n        revision = 'main',\n        legacy = null,\n    } = {}) {\n\n        let info = await loadTokenizer(pretrained_model_name_or_path, {\n            progress_callback,\n            config,\n            cache_dir,\n            local_files_only,\n            revision,\n            legacy,\n        })\n\n        // @ts-ignore\n        return new this(...info);\n    }\n\n    /**\n     * This function can be overridden by a subclass to apply additional preprocessing\n     * to a model's input data.\n     * @param {Object} inputs An object containing input data as properties.\n     * @returns {Object} The modified inputs object.\n     */\n    prepare_model_inputs(inputs) {\n        return inputs;\n    }\n\n    /**\n     * Encode/tokenize the given text(s).\n     * @param {string|string[]} text The text to tokenize.\n     * @param {Object} options An optional object containing the following properties:\n     * @param {string|string[]} [options.text_pair=null] Optional second sequence to be encoded. If set, must be the same type as text.\n     * @param {boolean} [options.padding=false] Whether to pad the input sequences.\n     * @param {boolean} [options.add_special_tokens=true] Whether or not to add the special tokens associated with the corresponding model.\n     * @param {boolean} [options.truncation=null] Whether to truncate the input sequences.\n     * @param {number} [options.max_length=null] Maximum length of the returned list and optionally padding length.\n     * @param {boolean} [options.return_tensor=true] Whether to return the results as Tensors or arrays.\n     * @returns {{ input_ids: number[]|number[][]|Tensor, attention_mask: any[]|Tensor }} Object to be passed to the model.\n     */\n    _call(\n        // Required positional arguments\n        text,\n\n        // Optional keyword arguments\n        {\n            text_pair = null,\n            add_special_tokens = true,\n            padding = false,\n            truncation = null,\n            max_length = null,\n            return_tensor = true, // Different to HF\n        } = {},\n    ) {\n\n        /** @type {number[]|number[][]|Tensor} */\n        let tokens;\n\n        if (Array.isArray(text)) {\n            if (text.length === 0) {\n                throw Error('text array must be non-empty')\n            }\n\n            if (text_pair !== null) {\n                if (!Array.isArray(text_pair)) {\n                    throw Error('text_pair must also be an array')\n\n                } else if (text.length !== text_pair.length) {\n                    throw Error('text and text_pair must have the same length')\n                }\n\n                tokens = text.map(\n                    (t, i) => this.encode(t, text_pair[i], { add_special_tokens })\n                )\n\n            } else {\n                tokens = text.map(x => this.encode(x, null, { add_special_tokens }));\n            }\n\n        } else {\n            if (text === null) {\n                throw Error('text may not be null')\n            }\n\n            if (Array.isArray(text_pair)) {\n                throw Error('When specifying `text_pair`, since `text` is a string, `text_pair` must also be a string (i.e., not an array).')\n            }\n\n            // For single input, we just wrap in an array, and then unwrap later.\n            tokens = [this.encode(text, text_pair, { add_special_tokens })];\n        }\n        // At this point, tokens is batched: [batch_size, tokens]\n        // However, array may be jagged. So, we pad to max_length\n\n        let maxLengthOfBatch = max(tokens.map(x => x.length))[0];\n\n        // If null, we calculate max length from sequences\n        if (max_length === null) {\n            max_length = maxLengthOfBatch;\n        }\n\n        // Ensure it is less than model max length\n        max_length = Math.min(max_length, this.model_max_length)\n\n        /** @type {any[]|Tensor} */\n        let attention_mask = [];\n        if (padding || truncation) {\n            // Perform padding and/or truncation\n            for (let i = 0; i < tokens.length; ++i) {\n                if (tokens[i].length === max_length) {\n                    attention_mask.push(new Array(tokens[i].length).fill(1))\n                    continue;\n\n                } else if (tokens[i].length > max_length) {\n                    // possibly truncate\n                    if (truncation) {\n                        tokens[i] = tokens[i].slice(0, max_length);\n                    }\n                    attention_mask.push(new Array(tokens[i].length).fill(1))\n\n                } else { // t.length < max_length\n                    if (padding) {\n                        let diff = max_length - tokens[i].length;\n\n                        if (this.padding_side === 'right') {\n                            attention_mask.push(\n                                (new Array(tokens[i].length).fill(1)).concat(new Array(diff).fill(0))\n                            )\n                            tokens[i].push(...new Array(diff).fill(this.pad_token_id))\n                        } else { // left\n                            attention_mask.push(\n                                (new Array(diff).fill(0)).concat(new Array(tokens[i].length).fill(1))\n                            )\n                            tokens[i].unshift(...new Array(diff).fill(this.pad_token_id))\n                        }\n\n                    } else {\n                        attention_mask.push(new Array(tokens[i].length).fill(1))\n                    }\n                }\n            }\n        } else {\n            attention_mask = tokens.map(x => new Array(x.length).fill(1))\n        }\n\n        if (return_tensor) {\n            if (!(padding && truncation)) {\n                // Not, guaranteed that all items have same length, so\n                // we perform additional check\n\n                if (tokens.some(x => x.length !== tokens[0].length)) {\n                    throw Error(\n                        \"Unable to create tensor, you should probably activate truncation and/or padding \" +\n                        \"with 'padding=true' and 'truncation=true' to have batched tensors with the same length.\"\n                    )\n                }\n            }\n\n            // Now we actually convert to tensor\n            // NOTE: In the same way as the python library, we return a batched tensor, regardless of\n            // whether we have a single input or multiple inputs.\n            let dims = [tokens.length, tokens[0].length];\n\n            tokens = new Tensor('int64',\n                BigInt64Array.from(tokens.flat().map(BigInt)),\n                dims\n            );\n\n            attention_mask = new Tensor(\n                'int64',\n                BigInt64Array.from(attention_mask.flat().map(BigInt)),\n                dims\n            )\n        } else {\n            // If not returning a tensor, we match the input type\n            if (!Array.isArray(text)) {\n                // Input was not batched, so we unwrap\n                tokens = tokens[0];\n                attention_mask = attention_mask[0];\n            }\n        }\n\n\n        // Finally, add attention mask, and possibly model-specific parameters\n        let modelInputs = {\n            input_ids: tokens,\n            attention_mask: attention_mask\n        }\n\n        // Optional post-processing\n        modelInputs = this.prepare_model_inputs(modelInputs);\n\n        return modelInputs\n    }\n\n    /**\n     * Encodes a single text using the preprocessor pipeline of the tokenizer.\n     *\n     * @param {string|null} text The text to encode.\n     * @returns {string[]|null} The encoded tokens.\n     */\n    _encode_text(text) {\n        if (text === null) return null;\n\n        // Actual function which does encoding, for a single text\n        // First, we take care of special tokens. Needed to avoid issues arising from\n        // normalization and/or pretokenization (which may not preserve special tokens)\n        const sections = this.added_tokens_regex ? text.split(this.added_tokens_regex).filter(x => x) : [text];\n\n        const tokens = sections.map((x, section_index) => {\n            const addedToken = this.added_tokens.find(t => t.content === x);\n            if (addedToken !== undefined) {\n                // Ignore added tokens\n                return x\n            } else {\n                if (this.remove_space === true) {\n                    x = x.trim().split(/\\s+/).join(' ');\n                }\n                if (this.do_lowercase_and_remove_accent) {\n                    x = lowercase_and_remove_accent(x);\n                }\n\n                if (this.normalizer !== null) {\n                    x = this.normalizer(x);\n                }\n\n                const sectionTokens = (this.pre_tokenizer !== null) ? this.pre_tokenizer(x, {\n                    section_index,\n                }) : [x];\n\n                const tokens = this.model(sectionTokens);\n\n                return tokens;\n            }\n        }).flat();\n\n        return tokens;\n    }\n\n    /**\n     * Encodes a single text or a pair of texts using the model's tokenizer.\n     *\n     * @param {string} text The text to encode.\n     * @param {string|null} text_pair The optional second text to encode.\n     * @param {Object} options An optional object containing the following properties:\n     * @param {boolean} [options.add_special_tokens=true] Whether or not to add the special tokens associated with the corresponding model.\n     * @returns {number[]} An array of token IDs representing the encoded text(s).\n     */\n    encode(text, text_pair = null, {\n        add_special_tokens = true,\n    } = {}) {\n        // Function called by users to encode possibly multiple texts\n        let tokens = this._encode_text(text);\n        let tokens2 = this._encode_text(text_pair);\n\n        // TODO improve `add_special_tokens` and ensure correctness\n        let combinedTokens = (this.post_processor !== null && add_special_tokens)\n            ? this.post_processor(tokens, tokens2)\n            : mergeArrays(tokens ?? [], tokens2 ?? []);\n\n        let ids = this.model.convert_tokens_to_ids(combinedTokens);\n        return ids;\n    }\n\n    /**\n     * Decode a batch of tokenized sequences.\n     * @param {number[][]|Tensor} batch List/Tensor of tokenized input sequences.\n     * @param {Object} decode_args (Optional) Object with decoding arguments.\n     * @returns {string[]} List of decoded sequences.\n     */\n    batch_decode(batch, decode_args = {}) {\n        if (batch instanceof Tensor) {\n            batch = batch.tolist();\n        }\n        return batch.map(x => this.decode(x, decode_args));\n    }\n\n    /**\n     * Decodes a sequence of token IDs back to a string.\n     *\n     * @param {number[]|Tensor} token_ids List/Tensor of token IDs to decode.\n     * @param {Object} [decode_args={}]\n     * @param {boolean} [decode_args.skip_special_tokens=false] If true, special tokens are removed from the output string.\n     * @param {boolean} [decode_args.clean_up_tokenization_spaces=true] If true, spaces before punctuations and abbreviated forms are removed.\n     *\n     * @returns {string} The decoded string.\n     * @throws {Error} If `token_ids` is not a non-empty array of integers.\n     */\n    decode(\n        token_ids,\n        decode_args = {},\n    ) {\n        if (token_ids instanceof Tensor) {\n            token_ids = prepareTensorForDecode(token_ids);\n        }\n\n        if (!Array.isArray(token_ids) || token_ids.length === 0 || !isIntegralNumber(token_ids[0])) {\n            throw Error(\"token_ids must be a non-empty array of integers.\");\n        }\n\n        return this.decode_single(token_ids, decode_args)\n    }\n\n    /**\n     * Decode a single list of token ids to a string.\n     * @param {number[]} token_ids List of token ids to decode\n     * @param {Object} decode_args Optional arguments for decoding\n     * @param {boolean} [decode_args.skip_special_tokens=false] Whether to skip special tokens during decoding\n     * @param {boolean} [decode_args.clean_up_tokenization_spaces=null] Whether to clean up tokenization spaces during decoding.\n     * If null, the value is set to `this.decoder.cleanup` if it exists, falling back to `this.clean_up_tokenization_spaces` if it exists, falling back to `true`.\n     * @returns {string} The decoded string\n     */\n    decode_single(\n        token_ids,\n        {\n            skip_special_tokens = false,\n            clean_up_tokenization_spaces = null,\n        }\n    ) {\n        let tokens = this.model.convert_ids_to_tokens(token_ids);\n        if (skip_special_tokens) {\n            tokens = tokens.filter(x => !this.special_tokens.includes(x));\n        }\n\n        // If `this.decoder` is null, we just join tokens with a space:\n        // https://github.com/huggingface/tokenizers/blob/8edec536a737cb04494b454805be16c020abb14f/tokenizers/src/tokenizer/mod.rs#L835\n        /** @type {string} */\n        let decoded = this.decoder ? this.decoder(tokens) : tokens.join(' ');\n\n        // Slight hack, but prevents having to pass `skip_special_tokens` to\n        // each call to `decode`, which would lead to code duplication.\n        if (this.decoder && this.decoder.end_of_word_suffix) {\n            decoded = decoded.replaceAll(this.decoder.end_of_word_suffix, ' ');\n            if (skip_special_tokens) {\n                decoded = decoded.trim();\n            }\n        }\n\n        if (clean_up_tokenization_spaces ?? this.clean_up_tokenization_spaces) {\n            decoded = clean_up_tokenization(decoded);\n        }\n\n        return decoded;\n    }\n\n    get default_chat_template() {\n        if (!this._warned_about_chat_template) {\n            console.warn(\n                \"No chat template is defined for this tokenizer - using a default chat template \" +\n                \"that implements the ChatML format. If the default is not appropriate for \" +\n                \"your model, please set `tokenizer.chat_template` to an appropriate template. \" +\n                \"See https://huggingface.co/docs/transformers/main/chat_templating for more information.\"\n            )\n            this._warned_about_chat_template = true; // TODO move to logger.warning_once()\n        }\n\n        return this._default_chat_template;\n    }\n\n    /**\n     * @typedef {Object} Message\n     * @property {string} role The role of the message (e.g., \"user\" or \"assistant\" or \"system\").\n     * @property {string} content The content of the message.\n     */\n\n    /**\n     * Converts a list of message objects with `\"role\"` and `\"content\"` keys to a list of token\n     * ids. This method is intended for use with chat models, and will read the tokenizer's chat_template attribute to\n     * determine the format and control tokens to use when converting. When chat_template is None, it will fall back\n     * to the default_chat_template specified at the class level.\n     * \n     * See [here](https://huggingface.co/docs/transformers/chat_templating) for more information.\n     * \n     * **Example:** Applying a chat template to a conversation.\n     * \n     * ```javascript\n     * import { AutoTokenizer } from \"@xenova/transformers\";\n     * \n     * const tokenizer = await AutoTokenizer.from_pretrained(\"mistralai/Mistral-7B-Instruct-v0.1\");\n     * \n     * const chat = [\n     *   { \"role\": \"user\", \"content\": \"Hello, how are you?\" },\n     *   { \"role\": \"assistant\", \"content\": \"I'm doing great. How can I help you today?\" },\n     *   { \"role\": \"user\", \"content\": \"I'd like to show off how chat templating works!\" },\n     * ]\n     * \n     * const text = tokenizer.apply_chat_template(chat, { tokenize: false });\n     * // \"<s>[INST] Hello, how are you? [/INST]I'm doing great. How can I help you today?</s> [INST] I'd like to show off how chat templating works! [/INST]\"\n     * \n     * const input_ids = tokenizer.apply_chat_template(chat, { tokenize: true, return_tensor: false });\n     * // [1, 733, 16289, 28793, 22557, 28725, 910, 460, 368, 28804, 733, 28748, 16289, 28793, 28737, 28742, 28719, 2548, 1598, 28723, 1602, 541, 315, 1316, 368, 3154, 28804, 2, 28705, 733, 16289, 28793, 315, 28742, 28715, 737, 298, 1347, 805, 910, 10706, 5752, 1077, 3791, 28808, 733, 28748, 16289, 28793]\n     * ```\n     * \n     * @param {Message[]} conversation A list of message objects with `\"role\"` and `\"content\"` keys.\n     * @param {Object} options An optional object containing the following properties:\n     * @param {string} [options.chat_template=null] A Jinja template to use for this conversion. If\n     * this is not passed, the model's default chat template will be used instead.\n     * @param {boolean} [options.add_generation_prompt=false] Whether to end the prompt with the token(s) that indicate\n     * the start of an assistant message. This is useful when you want to generate a response from the model.\n     * Note that this argument will be passed to the chat template, and so it must be supported in the\n     * template for this argument to have any effect.\n     * @param {boolean} [options.tokenize=true] Whether to tokenize the output. If false, the output will be a string.\n     * @param {boolean} [options.padding=false] Whether to pad sequences to the maximum length. Has no effect if tokenize is false.\n     * @param {boolean} [options.truncation=false] Whether to truncate sequences to the maximum length. Has no effect if tokenize is false.\n     * @param {number} [options.max_length=null] Maximum length (in tokens) to use for padding or truncation. Has no effect if tokenize is false.\n     * If not specified, the tokenizer's `max_length` attribute will be used as a default.\n     * @param {boolean} [options.return_tensor=true] Whether to return the output as a Tensor or an Array. Has no effect if tokenize is false.\n     * @returns {string | Tensor | number[]| number[][]} The tokenized output.\n     */\n    apply_chat_template(conversation, {\n        chat_template = null,\n        add_generation_prompt = false,\n        tokenize = true,\n        padding = false,\n        truncation = false,\n        max_length = null,\n        return_tensor = true,\n    } = {}) {\n\n        chat_template ??= this.chat_template ?? this.default_chat_template;\n\n        // Compilation function uses a cache to avoid recompiling the same template\n        let compiledTemplate = this._compiled_template_cache.get(chat_template);\n        if (compiledTemplate === undefined) {\n            compiledTemplate = new Template(chat_template);\n            this._compiled_template_cache.set(chat_template, compiledTemplate);\n        }\n\n        const special_tokens_map = Object.create(null);\n        for (const key of SPECIAL_TOKEN_ATTRIBUTES) {\n            const value = this.getToken(key);\n            if (value) {\n                special_tokens_map[key] = value;\n            }\n        }\n\n        const rendered = compiledTemplate.render({\n            messages: conversation,\n            add_generation_prompt: add_generation_prompt,\n\n            ...special_tokens_map,\n        });\n\n        if (tokenize) {\n            return this._call(rendered, {\n                add_special_tokens: false,\n                padding,\n                truncation,\n                max_length,\n                return_tensor,\n            }).input_ids;\n        }\n\n        return rendered;\n    }\n}\n\n/**\n* Helper method for adding `token_type_ids` to model inputs\n* @param {Object} inputs An object containing the input ids and attention mask.\n* @returns {Object} The prepared inputs object.\n*/\nexport function add_token_types(inputs) {\n    // TODO ensure correctness when token pair is present\n    if (inputs.input_ids instanceof Tensor) {\n        inputs.token_type_ids = new Tensor(\n            'int64',\n            new BigInt64Array(inputs.input_ids.data.length),\n            inputs.input_ids.dims\n        )\n    } else if (Array.isArray(inputs.input_ids)) {\n\n        if (Array.isArray(inputs.input_ids[0])) {\n            // This means input is batched, so we need to batch the token_type_ids as well\n            inputs.token_type_ids = inputs.input_ids.map(\n                x => new Array(x.length).fill(0)\n            )\n        } else {\n            inputs.token_type_ids = new Array(inputs.input_ids.length).fill(0);\n        }\n    } else {\n        throw new Error('Input ids must be a Tensor or an Array')\n    }\n\n    return inputs;\n}\n\n/**\n * BertTokenizer is a class used to tokenize text for BERT models.\n * @extends PreTrainedTokenizer\n */\nexport class BertTokenizer extends PreTrainedTokenizer {\n    /** @type {add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\n/**\n * Albert tokenizer\n * @extends PreTrainedTokenizer\n */\nexport class AlbertTokenizer extends PreTrainedTokenizer {\n    /** @type {add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\nexport class MobileBertTokenizer extends PreTrainedTokenizer {\n    /** @type {add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\nexport class SqueezeBertTokenizer extends PreTrainedTokenizer {\n    /** @type {add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\nexport class DebertaTokenizer extends PreTrainedTokenizer {\n    /** @type {add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\nexport class DebertaV2Tokenizer extends PreTrainedTokenizer {\n    /** @type {add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\nexport class HerbertTokenizer extends PreTrainedTokenizer {\n    /** @type {add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\nexport class ConvBertTokenizer extends PreTrainedTokenizer {\n    /** @type {add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\nexport class DistilBertTokenizer extends PreTrainedTokenizer { }\nexport class CamembertTokenizer extends PreTrainedTokenizer { }\nexport class XLMTokenizer extends PreTrainedTokenizer {\n    constructor(tokenizerJSON, tokenizerConfig) {\n        super(tokenizerJSON, tokenizerConfig);\n        console.warn('WARNING: `XLMTokenizer` is not yet supported by Hugging Face\\'s \"fast\" tokenizers library. Therefore, you may experience slightly inaccurate results.')\n    }\n\n    /** @type {add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\nexport class ElectraTokenizer extends PreTrainedTokenizer {\n    /** @type {add_token_types} */\n    prepare_model_inputs(inputs) {\n        return add_token_types(inputs);\n    }\n}\n\nexport class T5Tokenizer extends PreTrainedTokenizer { }\nexport class GPT2Tokenizer extends PreTrainedTokenizer {\n    _default_chat_template = `{% for message in messages %}\" \"{{ message.content }}{{ eos_token }}\" \"{% endfor %}`\n}\nexport class BartTokenizer extends PreTrainedTokenizer { }\nexport class MBartTokenizer extends PreTrainedTokenizer {\n    constructor(tokenizerJSON, tokenizerConfig) {\n        super(tokenizerJSON, tokenizerConfig);\n\n        this.languageRegex = /^[a-z]{2}_[A-Z]{2}$/;\n        this.language_codes = this.special_tokens.filter(x => this.languageRegex.test(x));\n        this.lang_to_token = x => x; // Identity function\n    }\n\n    /**\n     * Helper function to build translation inputs for an `MBartTokenizer`.\n     * @param {string|string[]} raw_inputs The text to tokenize.\n     * @param {Object} tokenizer_options Options to be sent to the tokenizer\n     * @param {Object} generate_kwargs Generation options.\n     * @returns {Object} Object to be passed to the model.\n     */\n    _build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs) {\n        return _build_translation_inputs(this, raw_inputs, tokenizer_options, generate_kwargs);\n    }\n}\nexport class MBart50Tokenizer extends MBartTokenizer { } // NOTE: extends MBartTokenizer\n\nexport class RobertaTokenizer extends PreTrainedTokenizer { }\n\nexport class BloomTokenizer extends GPT2Tokenizer { // NOTE: `GPT2Tokenizer` to get the correct chat template\n\n    constructor(tokenizerJSON, tokenizerConfig) {\n        // Override the default (invalid) regex of the pretokenizer.\n        // For more information, see https://github.com/xenova/transformers.js/issues/94\n        const splitChars = '.,!?\\u2026\\u3002\\uff0c\\u3001\\u0964\\u06d4\\u060c';\n        const patternObject = tokenizerJSON.pre_tokenizer?.pretokenizers[0]?.pattern;\n        if (patternObject && patternObject.Regex === ` ?[^(\\\\s|[${splitChars}])]+`) {\n            patternObject.Regex = ` ?[^\\\\s${splitChars}]+`;\n        }\n        super(tokenizerJSON, tokenizerConfig);\n    }\n}\n\nconst SPIECE_UNDERLINE = \"▁\";\n\nexport class LlamaTokenizer extends PreTrainedTokenizer {\n    _default_chat_template = `{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif USE_DEFAULT_PROMPT == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'DEFAULT_SYSTEM_MESSAGE' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\\n' + system_message + '\\n<</SYS>>\\n\\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\\n' + content.strip() + '\\n<</SYS>>\\n\\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}`\n\n    DEFAULT_SYSTEM_PROMPT =\n        \"You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your \" +\n        \"answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure \" +\n        \"that your responses are socially unbiased and positive in nature.\\n\\n\" +\n        \"If a question does not make any sense, or is not factually coherent, explain why instead of answering something not \" +\n        \"correct. If you don't know the answer to a question, please don't share false information.\"\n\n    constructor(tokenizerJSON, tokenizerConfig) {\n        super(tokenizerJSON, tokenizerConfig);\n        this.use_default_system_prompt = tokenizerConfig.use_default_system_prompt ?? false;\n\n        this.legacy = tokenizerConfig.legacy ?? true;\n        if (!this.legacy) {\n            // See https://github.com/huggingface/transformers/pull/24565 for more information\n            this.normalizer = null;\n            this.pre_tokenizer = new MetaspacePreTokenizer({\n                replacement: SPIECE_UNDERLINE,\n                add_prefix_space: true,\n                prepend_scheme: \"first\",\n            });\n        }\n    }\n\n    /**\n     * Helper function to handle legacy encoding of SPM tokenizers.\n     * Adapted from https://github.com/huggingface/transformers/blob/e6dcf8abd6f65bb4b6dfc1831b20d9ba49ce00e2/src/transformers/models/t5/tokenization_t5.py#L374-L387\n     * @param {string} text The text to encode.\n     * @returns {string[]} The encoded tokens.\n     */\n    _encode_text(text) {\n        if (text === null) return null;\n\n        if (this.legacy || text.length === 0) {\n            return super._encode_text(text);\n        }\n\n        let tokens = super._encode_text(SPIECE_UNDERLINE + text.replaceAll(SPIECE_UNDERLINE, \" \"));\n        if (tokens.length > 1 && tokens[0] === SPIECE_UNDERLINE && this.special_tokens.includes(tokens[1])) {\n            tokens = tokens.slice(1);\n        }\n        return tokens;\n    }\n\n    get default_chat_template() {\n        return super.default_chat_template\n            .replaceAll('USE_DEFAULT_PROMPT', this.use_default_system_prompt ? 'true' : 'false')\n            .replaceAll('DEFAULT_SYSTEM_MESSAGE', this.DEFAULT_SYSTEM_PROMPT.replaceAll(\"\\n\", \"\\\\n\").replaceAll(\"'\", \"\\\\'\"));\n    }\n}\nexport class CodeLlamaTokenizer extends LlamaTokenizer { } // NOTE: `LlamaTokenizer` to get the correct chat template\n\nexport class XLMRobertaTokenizer extends PreTrainedTokenizer { }\nexport class MPNetTokenizer extends PreTrainedTokenizer { }\n\nexport class FalconTokenizer extends PreTrainedTokenizer { }\n\nexport class GPTNeoXTokenizer extends PreTrainedTokenizer { }\n\nexport class EsmTokenizer extends PreTrainedTokenizer { }\n\n/**\n * Helper function to build translation inputs for an `NllbTokenizer` or `M2M100Tokenizer`.\n * @param {PreTrainedTokenizer} self The tokenizer instance.\n * @param {string|string[]} raw_inputs The text to tokenize.\n * @param {Object} tokenizer_options Options to be sent to the tokenizer\n * @param {Object} generate_kwargs Generation options.\n * @returns {Object} Object to be passed to the model.\n * @private\n */\nfunction _build_translation_inputs(self, raw_inputs, tokenizer_options, generate_kwargs) {\n    if (!('language_codes' in self) || !Array.isArray(self.language_codes)) {\n        throw new Error('Tokenizer must have `language_codes` attribute set and it should be an array of language ids.')\n    }\n    if (!('languageRegex' in self) || !(self.languageRegex instanceof RegExp)) {\n        throw new Error('Tokenizer must have `languageRegex` attribute set and it should be a regular expression.')\n    }\n    if (!('lang_to_token' in self) || typeof self.lang_to_token !== 'function') {\n        throw new Error('Tokenizer must have `lang_to_token` attribute set and it should be a function.')\n    }\n    const src_lang_token = generate_kwargs.src_lang;\n    const tgt_lang_token = generate_kwargs.tgt_lang;\n\n    // Check that the target language is valid:\n    if (!self.language_codes.includes(tgt_lang_token)) {\n        throw new Error(`Target language code \"${tgt_lang_token}\" is not valid. Must be one of: {${self.language_codes.join(', ')}}`);\n    }\n\n    // Allow `src_lang` to be optional. If not set, we'll use the tokenizer's default.\n    if (src_lang_token !== undefined) {\n        // Check that the source language is valid:\n        if (!self.language_codes.includes(src_lang_token)) {\n            throw new Error(`Source language code \"${src_lang_token}\" is not valid. Must be one of: {${self.language_codes.join(', ')}}`);\n        }\n\n        // In the same way as the Python library, we override the post-processor\n        // to force the source language to be first:\n        for (let item of self.post_processor.config.single) {\n            if ('SpecialToken' in item && self.languageRegex.test(item.SpecialToken.id)) {\n                item.SpecialToken.id = self.lang_to_token(src_lang_token);\n                break;\n            }\n        }\n        // TODO: Do the same for pair?\n    }\n\n    // Override the `forced_bos_token_id` to force the correct language\n    generate_kwargs.forced_bos_token_id = self.model.convert_tokens_to_ids([self.lang_to_token(tgt_lang_token)])[0];\n\n    return self._call(raw_inputs, tokenizer_options);\n}\n\n/**\n * The NllbTokenizer class is used to tokenize text for NLLB (\"No Language Left Behind\") models.\n * \n * No Language Left Behind (NLLB) is a first-of-its-kind, AI breakthrough project\n * that open-sources models capable of delivering high-quality translations directly\n * between any pair of 200+ languages — including low-resource languages like Asturian,\n * Luganda, Urdu and more. It aims to help people communicate with anyone, anywhere,\n * regardless of their language preferences. For more information, check out their\n * [paper](https://arxiv.org/abs/2207.04672).\n * \n * For a list of supported languages (along with their language codes),\n * @see {@link https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200}\n */\nexport class NllbTokenizer extends PreTrainedTokenizer {\n\n    constructor(tokenizerJSON, tokenizerConfig) {\n        super(tokenizerJSON, tokenizerConfig);\n\n        this.languageRegex = /^[a-z]{3}_[A-Z][a-z]{3}$/;\n        this.language_codes = this.special_tokens.filter(x => this.languageRegex.test(x));\n        this.lang_to_token = x => x; // Identity function\n    }\n\n    /**\n     * Helper function to build translation inputs for an `NllbTokenizer`.\n     * @param {string|string[]} raw_inputs The text to tokenize.\n     * @param {Object} tokenizer_options Options to be sent to the tokenizer\n     * @param {Object} generate_kwargs Generation options.\n     * @returns {Object} Object to be passed to the model.\n     */\n    _build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs) {\n        return _build_translation_inputs(this, raw_inputs, tokenizer_options, generate_kwargs);\n    }\n}\n\n/**\n * The M2M100Tokenizer class is used to tokenize text for M2M100 (\"Many-to-Many\") models.\n * \n * M2M100 is a multilingual encoder-decoder (seq-to-seq) model trained for Many-to-Many\n * multilingual translation. It was introduced in this [paper](https://arxiv.org/abs/2010.11125)\n * and first released in [this](https://github.com/pytorch/fairseq/tree/master/examples/m2m_100) repository.\n * \n * For a list of supported languages (along with their language codes),\n * @see {@link https://huggingface.co/facebook/m2m100_418M#languages-covered}\n */\nexport class M2M100Tokenizer extends PreTrainedTokenizer {\n    constructor(tokenizerJSON, tokenizerConfig) {\n        super(tokenizerJSON, tokenizerConfig);\n\n        this.languageRegex = /^__[a-z]{2,3}__$/;\n        this.language_codes = this.special_tokens\n            .filter(x => this.languageRegex.test(x))\n            .map(x => x.slice(2, -2));\n        this.lang_to_token = x => `__${x}__`;\n    }\n\n    /**\n     * Helper function to build translation inputs for an `M2M100Tokenizer`.\n     * @param {string|string[]} raw_inputs The text to tokenize.\n     * @param {Object} tokenizer_options Options to be sent to the tokenizer\n     * @param {Object} generate_kwargs Generation options.\n     * @returns {Object} Object to be passed to the model.\n     */\n    _build_translation_inputs(raw_inputs, tokenizer_options, generate_kwargs) {\n        return _build_translation_inputs(this, raw_inputs, tokenizer_options, generate_kwargs);\n    }\n}\n\n\nconst WHISPER_LANGUAGES = [\n    [\"en\", \"english\"],\n    [\"zh\", \"chinese\"],\n    [\"de\", \"german\"],\n    [\"es\", \"spanish\"],\n    [\"ru\", \"russian\"],\n    [\"ko\", \"korean\"],\n    [\"fr\", \"french\"],\n    [\"ja\", \"japanese\"],\n    [\"pt\", \"portuguese\"],\n    [\"tr\", \"turkish\"],\n    [\"pl\", \"polish\"],\n    [\"ca\", \"catalan\"],\n    [\"nl\", \"dutch\"],\n    [\"ar\", \"arabic\"],\n    [\"sv\", \"swedish\"],\n    [\"it\", \"italian\"],\n    [\"id\", \"indonesian\"],\n    [\"hi\", \"hindi\"],\n    [\"fi\", \"finnish\"],\n    [\"vi\", \"vietnamese\"],\n    [\"he\", \"hebrew\"],\n    [\"uk\", \"ukrainian\"],\n    [\"el\", \"greek\"],\n    [\"ms\", \"malay\"],\n    [\"cs\", \"czech\"],\n    [\"ro\", \"romanian\"],\n    [\"da\", \"danish\"],\n    [\"hu\", \"hungarian\"],\n    [\"ta\", \"tamil\"],\n    [\"no\", \"norwegian\"],\n    [\"th\", \"thai\"],\n    [\"ur\", \"urdu\"],\n    [\"hr\", \"croatian\"],\n    [\"bg\", \"bulgarian\"],\n    [\"lt\", \"lithuanian\"],\n    [\"la\", \"latin\"],\n    [\"mi\", \"maori\"],\n    [\"ml\", \"malayalam\"],\n    [\"cy\", \"welsh\"],\n    [\"sk\", \"slovak\"],\n    [\"te\", \"telugu\"],\n    [\"fa\", \"persian\"],\n    [\"lv\", \"latvian\"],\n    [\"bn\", \"bengali\"],\n    [\"sr\", \"serbian\"],\n    [\"az\", \"azerbaijani\"],\n    [\"sl\", \"slovenian\"],\n    [\"kn\", \"kannada\"],\n    [\"et\", \"estonian\"],\n    [\"mk\", \"macedonian\"],\n    [\"br\", \"breton\"],\n    [\"eu\", \"basque\"],\n    [\"is\", \"icelandic\"],\n    [\"hy\", \"armenian\"],\n    [\"ne\", \"nepali\"],\n    [\"mn\", \"mongolian\"],\n    [\"bs\", \"bosnian\"],\n    [\"kk\", \"kazakh\"],\n    [\"sq\", \"albanian\"],\n    [\"sw\", \"swahili\"],\n    [\"gl\", \"galician\"],\n    [\"mr\", \"marathi\"],\n    [\"pa\", \"punjabi\"],\n    [\"si\", \"sinhala\"],\n    [\"km\", \"khmer\"],\n    [\"sn\", \"shona\"],\n    [\"yo\", \"yoruba\"],\n    [\"so\", \"somali\"],\n    [\"af\", \"afrikaans\"],\n    [\"oc\", \"occitan\"],\n    [\"ka\", \"georgian\"],\n    [\"be\", \"belarusian\"],\n    [\"tg\", \"tajik\"],\n    [\"sd\", \"sindhi\"],\n    [\"gu\", \"gujarati\"],\n    [\"am\", \"amharic\"],\n    [\"yi\", \"yiddish\"],\n    [\"lo\", \"lao\"],\n    [\"uz\", \"uzbek\"],\n    [\"fo\", \"faroese\"],\n    [\"ht\", \"haitian creole\"],\n    [\"ps\", \"pashto\"],\n    [\"tk\", \"turkmen\"],\n    [\"nn\", \"nynorsk\"],\n    [\"mt\", \"maltese\"],\n    [\"sa\", \"sanskrit\"],\n    [\"lb\", \"luxembourgish\"],\n    [\"my\", \"myanmar\"],\n    [\"bo\", \"tibetan\"],\n    [\"tl\", \"tagalog\"],\n    [\"mg\", \"malagasy\"],\n    [\"as\", \"assamese\"],\n    [\"tt\", \"tatar\"],\n    [\"haw\", \"hawaiian\"],\n    [\"ln\", \"lingala\"],\n    [\"ha\", \"hausa\"],\n    [\"ba\", \"bashkir\"],\n    [\"jw\", \"javanese\"],\n    [\"su\", \"sundanese\"],\n]\n\n// @ts-ignore\nconst WHISPER_LANGUAGE_MAPPING = new Map(WHISPER_LANGUAGES);\n// @ts-ignore\nconst WHISPER_TO_LANGUAGE_CODE_MAPPING = new Map([\n    ...WHISPER_LANGUAGES.map(([k, v]) => [v, k]),\n    ...[\n        [\"burmese\", \"my\"],\n        [\"valencian\", \"ca\"],\n        [\"flemish\", \"nl\"],\n        [\"haitian\", \"ht\"],\n        [\"letzeburgesch\", \"lb\"],\n        [\"pushto\", \"ps\"],\n        [\"panjabi\", \"pa\"],\n        [\"moldavian\", \"ro\"],\n        [\"moldovan\", \"ro\"],\n        [\"sinhalese\", \"si\"],\n        [\"castilian\", \"es\"],\n    ]\n]);\n\n/**\n * WhisperTokenizer tokenizer\n * @extends PreTrainedTokenizer\n */\nexport class WhisperTokenizer extends PreTrainedTokenizer {\n    _default_chat_template = `{% for message in messages %}\" \"{{ message.content }}{{ eos_token }}\" \"{% endfor %}`;\n\n    /**\n     * Decodes automatic speech recognition (ASR) sequences.\n     * @param {Array<{tokens: number[], token_timestamps?: number[], stride: number[]}>} sequences The sequences to decode.\n     * @param {Object} options The options to use for decoding.\n     * @returns {Array<string|{chunks?: undefined|Array<{language: string|null, timestamp: Array<number|null>, text: string}>}>} The decoded sequences.\n     */\n    _decode_asr(sequences, {\n        return_timestamps = false,\n        return_language = false,\n        time_precision = null,\n        force_full_sequences = true\n    } = {}) {\n        // Set force_full_sequences=false if you want streaming\n        // TODO add support for `return_language`\n\n        // Internal method meant to only be used by asr pipeline.\n        // Handles all the little quirks specific to whisper to handle\n        // the various options not allowed in other seq2seq models\n\n        // =========== Overview ============\n        // - iterate over all outputs\n        // - all tokens within output\n        // - Each token can be\n        //   - language token\n        //   - special token\n        //   - timestamp token\n        //   - text token\n        // - We accumulate the text tokens.\n        // - We split on end timestamps\n        // - Lots of complexity comes from stride and timestamps\n\n        if (time_precision === null) {\n            throw Error(\"Must specify time_precision\")\n        }\n        let last_language = null;\n\n        const returnWordTimestamps = return_timestamps === \"word\";\n\n        function new_chunk() {\n            return { \"language\": last_language, \"timestamp\": [null, null], \"text\": \"\" };\n        }\n\n        // Welcome to the state machine!\n        const chunks = [];\n        let chunk = new_chunk();\n        let time_offset = 0.0;\n        const timestamp_begin = this.model.convert_tokens_to_ids([\"<|notimestamps|>\"])[0] + 1;\n\n        let previous_tokens = [];\n        let previous_token_timestamps = [];\n\n        let skip = false;\n        let right_stride_start = null;\n\n\n        const all_special_ids = new Set(this.all_special_ids);\n\n        for (let output of sequences) {\n            // NOTE: python version has batches, so it uses [0]\n            const token_ids = output.tokens;\n            const token_timestamps = returnWordTimestamps ? output.token_timestamps : null;\n\n            // These keep track of timestamps within strides, which need\n            // to be skipped and resolve all tokens in a single chunk.\n            let last_timestamp = null;\n            let first_timestamp = timestamp_begin;\n\n            if (\"stride\" in output) {\n                const [chunk_len, stride_left, stride_right] = output.stride;\n\n                // Offset the timings to account for the other `model_outputs`.\n                time_offset -= stride_left;\n                right_stride_start = chunk_len - stride_right;\n\n                // Keeping track of timestamps within strides\n                // We're going to NOT split on those, and delay until we're\n                // out of BOTH stride. Otherwise lots of issues occur and\n                // corner cases\n                if (stride_left) {\n                    first_timestamp = stride_left / time_precision + timestamp_begin;\n                }\n\n                if (stride_right) {\n                    for (let i = token_ids.length - 1; i >= 0; --i) {\n                        const token = token_ids[i];\n                        if (token >= timestamp_begin) {\n                            // There can be several token in the right stride\n                            // But the last one is ALWAYS going to be skipped\n                            if (last_timestamp !== null && (token - timestamp_begin) * time_precision < right_stride_start) {\n                                break;\n                            }\n                            last_timestamp = token;\n                        }\n                    }\n                }\n            }\n\n            let current_tokens = [];\n            let current_token_timestamps = [];\n\n            // - all tokens within output\n            for (let i = 0; i < token_ids.length; ++i) {\n                const token = token_ids[i];\n                // 4 possible states for each token\n                // - 1/ Language code\n                // - 2/ all other special tokens (which we ignore)\n                // - 3/ Timestamp\n                // - 4/ Regular text\n\n                if (all_special_ids.has(token)) {\n                    const text = this.decode([token]);\n                    const language = WHISPER_LANGUAGE_MAPPING.get(text.slice(2, -2));\n\n                    if (language !== undefined) {\n                        // 1/ Indeed some language\n                        // TODO Handle when language is different from the previous\n                        // one, and we cannot use timestamped tokens to create chunks\n                        if (last_language !== null && language !== last_language && !return_timestamps) {\n                            previous_tokens.push(current_tokens);\n                            const resolved_tokens = this.findLongestCommonSequence(previous_tokens)[0];\n                            const resolved_text = this.decode(resolved_tokens);\n                            chunk.text = resolved_text;\n                            chunks.push(chunk);\n\n                            // Flush all our temporary context\n                            previous_tokens = [];\n                            current_tokens = [];\n                            chunk = new_chunk();\n                        }\n\n                        last_language = chunk.language = language;\n                    } else {\n                        // 2/ This is a regular special token, ignoring it\n                    }\n                } else if (token >= timestamp_begin) {\n                    // 3/ Timestamp token\n                    const time = (token - timestamp_begin) * time_precision + time_offset;\n                    const rounded_time = round(time, 2);\n\n                    if (last_timestamp !== null && token >= last_timestamp) {\n                        // Whisper outputted a timestamp token, but it falls within\n                        // our stride, so we're going to skip it for the time being\n                        // and resolve this later\n                        // Skip is necessary because timestamp tokens always come\n                        // by pair, so we need to skip the next one too (which would mark the start of another chunk).\n                        skip = true;\n                    } else if (skip || (previous_tokens.length > 0 && token < first_timestamp)) {\n                        skip = false;\n                    } else if (chunk.timestamp[0] === null) {\n                        chunk.timestamp[0] = rounded_time;\n                    } else {\n                        // This is the end of the timestamp chunk\n                        if (rounded_time === chunk.timestamp[0]) {\n                            // This is a bug in timestamp token output\n                            // where we're taking the duplicate token\n                            // as a stop where it should be a start.\n                            // This is an issue in the underlying model output\n                            // Let's just skip it so it becomes de-factor a start agin\n                        } else {\n                            chunk.timestamp[1] = rounded_time;\n\n                            // Handling merges\n                            previous_tokens.push(current_tokens)\n\n                            if (returnWordTimestamps) {\n                                previous_token_timestamps.push(current_token_timestamps);\n                            }\n                            const [resolved_tokens, resolved_token_timestamps] = this.findLongestCommonSequence(\n                                previous_tokens, previous_token_timestamps\n                            )\n\n                            const resolved_text = this.decode(resolved_tokens)\n                            chunk.text = resolved_text\n\n                            if (returnWordTimestamps) {\n                                chunk.words = this.collateWordTimestamps(\n                                    resolved_tokens, resolved_token_timestamps, last_language,\n                                )\n                            }\n\n                            chunks.push(chunk)\n\n                            // Flush all our temporary context\n                            previous_tokens = []\n                            current_tokens = []\n                            previous_token_timestamps = []\n                            current_token_timestamps = []\n                            chunk = new_chunk()\n                        }\n                    }\n\n                } else {\n                    // 4/ Regular token\n                    // We just append to the list of all tokens so we can handle\n                    // merges later and decode into text.\n                    current_tokens.push(token)\n\n                    if (returnWordTimestamps) {\n                        let start_time = round(token_timestamps[i] + time_offset, 2);\n\n                        let end_time;\n                        if (i + 1 < token_timestamps.length) {\n                            end_time = round(token_timestamps[i + 1] + time_offset, 2);\n                        } else {\n                            // should never happen\n                            end_time = null;\n                        }\n                        current_token_timestamps.push([start_time, end_time]);\n                    }\n\n                }\n            }\n\n            if ('stride' in output) {\n                const [chunk_len, stride_left, stride_right] = output.stride;\n                time_offset += chunk_len - stride_right\n            }\n\n            // Leftover tokens\n            if (current_tokens.length > 0) {\n                previous_tokens.push(current_tokens)\n                if (returnWordTimestamps) {\n                    previous_token_timestamps.push(current_token_timestamps);\n                }\n            } else if (previous_tokens.every(p => p.length === 0)) {\n                // Flushing previous tokens (END)\"\n                chunk = new_chunk()\n                previous_tokens = []\n                current_tokens = []\n                previous_token_timestamps = [];\n                current_token_timestamps = [];\n            }\n\n        }\n\n        if (previous_tokens.length > 0) {\n            if (force_full_sequences && return_timestamps) {\n                // Last token should always be timestamps, so there shouldn't be\n                // leftover\n                throw new Error(\n                    \"Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. \" +\n                    \"Also make sure WhisperTimeStampLogitsProcessor was used during generation.\"\n                );\n            }\n\n            // Happens when we don't use timestamps\n            const [resolved_tokens, resolved_token_timestamps] = this.findLongestCommonSequence(previous_tokens, previous_token_timestamps);\n\n            // Flushing previous tokens (FINAL)\n            const resolved_text = this.decode(resolved_tokens);\n            chunk.text = resolved_text;\n            if (returnWordTimestamps) {\n                chunk.words = this.collateWordTimestamps(\n                    resolved_tokens, resolved_token_timestamps, last_language,\n                )\n            }\n            chunks.push(chunk);\n        }\n\n        let optional = Object.create(null);\n\n        // Preparing and cleaning up the pipeline output\n        const full_text = chunks.map(chunk => chunk.text).join('');\n        if (return_timestamps || return_language) {\n            for (let i = 0; i < chunks.length; ++i) {\n                const chunk = chunks[i];\n                if (!return_timestamps) {\n                    delete chunk[\"timestamp\"];\n                }\n\n                if (!return_language) {\n                    delete chunk[\"language\"];\n                }\n            }\n            if (returnWordTimestamps) {\n                let new_chunks = [];\n                for (let chunk of chunks) {\n                    for (let word of chunk.words) {\n                        new_chunks.push(word);\n                    }\n                }\n                optional = { \"chunks\": new_chunks };\n            } else {\n                optional = { \"chunks\": chunks };\n            }\n        }\n        return [full_text, optional];\n\n    }\n\n    /**\n     * Finds the longest common sequence among the provided sequences.\n     * @param {number[][]} sequences An array of sequences of token ids to compare.\n     * @returns {number[][]} The longest common sequence found.\n     * @throws {Error} If there is a bug within the function.\n     * @private\n     */\n    findLongestCommonSequence(sequences, token_timestamp_sequences = null) {\n        // It would be much harder to do O(n) because of fault tolerance.\n        // We actually have a really good property which is that the total sequence\n        // MUST be those subsequences in order.\n        // If token_timestamp_sequences is provided, will split those sequences in\n        // exactly the same way.\n        let leftSequence = sequences[0];\n        let leftLength = leftSequence.length;\n        let totalSequence = [];\n\n        const use_token_timestamp_sequences = Array.isArray(token_timestamp_sequences) && token_timestamp_sequences.length > 0;\n        let total_token_timestamp_sequence = use_token_timestamp_sequences ? [] : null;\n        let left_token_timestamp_sequence = use_token_timestamp_sequences ? token_timestamp_sequences[0] : null;\n        for (let i = 1; i < sequences.length; ++i) {\n            const rightSequence = sequences[i];\n            let max = 0.0;\n            let maxIndices = [leftLength, leftLength, 0, 0];\n            // Here we're sliding matches\n            // [a, b, c, d]\n            //          [c, d, f]\n            // =        [c] == [d]\n\n            // [a, b, c, d]\n            //       [c, d, f]\n            // =     [c, d] == [c, d]\n\n\n            // [a, b, c, d]\n            //    [c, d, f]\n\n            // =  [b, c, d] == [c, d, f]\n\n            // [a, b, c, d]\n            // [c, d, f]\n\n            // [a, b, c] == [c, d, f]\n\n            // [a, b, c, d]\n            // [d, f]\n\n            // [a, b] == [d, f]\n\n            // [a, b, c, d]\n            // [f]\n\n            // [a] == [f]\n\n            const rightLength = rightSequence.length;\n            for (let j = 1; j < leftLength + rightLength; ++j) {\n                const eps = j / 10000.0;\n                const leftStart = Math.max(0, leftLength - j);\n                const leftStop = Math.min(leftLength, leftLength + rightLength - j);\n                const left = leftSequence.slice(leftStart, leftStop);\n                const rightStart = Math.max(0, j - leftLength);\n                const rightStop = Math.min(rightLength, j);\n                const right = rightSequence.slice(rightStart, rightStop);\n                if (left.length !== right.length) {\n                    throw new Error(\"There is a bug within whisper `decode_asr` function, please report it. Dropping to prevent bad inference.\");\n                }\n                const matches = left.filter((elem, idx) => elem === right[idx]).length;\n                const matching = matches / j + eps;\n                if (matches > 1 && matching > max) {\n                    max = matching;\n                    maxIndices = [leftStart, leftStop, rightStart, rightStop];\n                }\n            }\n            const [leftStart, leftStop, rightStart, rightStop] = maxIndices;\n            const leftMid = Math.floor((leftStop + leftStart) / 2);\n            const rightMid = Math.floor((rightStop + rightStart) / 2);\n            totalSequence.push(...leftSequence.slice(0, leftMid));\n            leftSequence = rightSequence.slice(rightMid);\n            leftLength = leftSequence.length;\n\n            if (use_token_timestamp_sequences) {\n                total_token_timestamp_sequence.push(...left_token_timestamp_sequence.slice(0, leftMid));\n                left_token_timestamp_sequence = token_timestamp_sequences[i].slice(rightMid);\n            }\n        }\n        totalSequence.push(...leftSequence);\n\n        if (use_token_timestamp_sequences) {\n            total_token_timestamp_sequence.push(...left_token_timestamp_sequence);\n            return [totalSequence, total_token_timestamp_sequence];\n        } else {\n            return [totalSequence, []];\n        }\n    }\n\n    /** @private */\n    collateWordTimestamps(tokens, token_timestamps, language) {\n\n        let [words, _, token_indices] = this.combineTokensIntoWords(tokens, language);\n\n        let timings = [];\n        for (let i = 0; i < words.length; ++i) {\n            const indices = token_indices[i];\n            timings.push({\n                text: words[i],\n                timestamp: [\n                    token_timestamps[indices.at(0)][0],\n                    token_timestamps[indices.at(-1)][1],\n                ],\n            });\n        }\n        return timings;\n    }\n\n    /**\n     * Groups tokens by word. Returns a tuple containing a list of strings with the words,\n     * and a list of `token_id` sequences with the tokens making up each word.\n     * @param {number[]} tokens \n     * @param {string} [language] \n     * @param {string} prepend_punctionations \n     * @param {string} append_punctuations \n     * \n     * @private\n     */\n    combineTokensIntoWords(tokens, language, prepend_punctionations = \"\\\"'“¡¿([{-\", append_punctuations = \"\\\"'.。,，!！?？:：”)]}、\") {\n        language = language ?? 'english';\n\n        let words, word_tokens, token_indices;\n\n        if ([\"chinese\", \"japanese\", \"thai\", \"lao\", \"myanmar\"].includes(language)) {\n            // These languages don't typically use spaces.\n            [words, word_tokens, token_indices] = this.splitTokensOnUnicode(tokens)\n        } else {\n            [words, word_tokens, token_indices] = this.splitTokensOnSpaces(tokens)\n        }\n\n        return this.mergePunctuations(words, word_tokens, token_indices, prepend_punctionations, append_punctuations);\n    }\n\n    /** @type {PreTrainedTokenizer['decode']} */\n    decode(\n        token_ids,\n        decode_args,\n    ) {\n        let text;\n        // @ts-ignore\n        if (decode_args && decode_args.decode_with_timestamps) {\n            if (token_ids instanceof Tensor) {\n                token_ids = prepareTensorForDecode(token_ids);\n            }\n            text = this.decodeWithTimestamps(token_ids, decode_args);\n        } else {\n            text = super.decode(token_ids, decode_args);\n        }\n        // TODO: implement offsets\n        // if (decode_args.output_offsets) {\n        //     let offsets = this.computeOffsets\n        // }\n        return text;\n    }\n\n    /**\n     * @param {number[]} token_ids List of token IDs to decode.\n     * @param {Object} decode_args Optional arguments for decoding\n     * @private\n     */\n    decodeWithTimestamps(token_ids, decode_args) {\n        const time_precision = decode_args?.time_precision ?? 0.02;\n\n        const timestamp_begin = Array.from(this.all_special_ids).at(-1) + 1;\n        /**@type {Array} */\n        let outputs = [[]];\n        for (let token of token_ids) {\n            if (token >= timestamp_begin) {\n                let timestamp = (token - timestamp_begin) * time_precision;\n                timestamp = round(timestamp, 2);\n                outputs.push(`<|${timestamp}|>`);\n                outputs.push([]);\n            } else {\n                outputs[outputs.length - 1].push(token);\n            }\n        }\n        outputs = outputs.map(\n            s => {\n                if (typeof s === 'string') {\n                    return s;\n                } else {\n                    return super.decode(s, decode_args);\n                }\n            }\n        )\n\n        return outputs.join('');\n    }\n\n    /**\n     * Combine tokens into words by splitting at any position where the tokens are decoded as valid unicode points.\n     * @param {number[]} tokens \n     * @returns {*}\n     * @private\n     */\n    splitTokensOnUnicode(tokens) {\n        const decoded_full = this.decode(tokens, {\n            // @ts-ignore\n            decode_with_timestamps: true,\n        });\n        const replacement_char = '\\uFFFD';\n\n        let words = []\n        let word_tokens = []\n        let token_indices = []\n        let current_tokens = []\n        let current_indices = []\n        let unicode_offset = 0\n\n        for (let token_idx = 0; token_idx < tokens.length; ++token_idx) {\n            const token = tokens[token_idx];\n\n            current_tokens.push(token);\n            current_indices.push(token_idx);\n\n            const decoded = this.decode(current_tokens, {\n                // @ts-ignore\n                decode_with_timestamps: true,\n            });\n\n            if (!decoded.includes(replacement_char) || decoded_full[unicode_offset + decoded.indexOf(replacement_char)] === replacement_char) {\n                words.push(decoded)\n                word_tokens.push(current_tokens)\n                token_indices.push(current_indices)\n                current_tokens = []\n                current_indices = []\n                unicode_offset += decoded.length;\n            }\n\n        }\n\n        return [words, word_tokens, token_indices]\n    }\n\n    /**\n     * Combine tokens into words by splitting at whitespace and punctuation tokens.\n     * @param {number[]} tokens \n     * @private\n     */\n    splitTokensOnSpaces(tokens) {\n\n        let [subwords, subword_tokens_list, subword_indices_list] = this.splitTokensOnUnicode(tokens);\n\n        let words = []\n        let word_tokens = []\n        let token_indices = []\n\n        const punctuationRegex = new RegExp(`^[${PUNCTUATION_REGEX}]$`, 'gu');\n\n        for (let i = 0; i < subwords.length; ++i) {\n\n            const subword = subwords[i];\n            const subword_tokens = subword_tokens_list[i];\n            const subword_indices = subword_indices_list[i];\n\n            // @ts-ignore\n            const special = subword_tokens[0] >= this.model.tokens_to_ids.get('<|endoftext|>');\n            const with_space = subword.startsWith(' ');\n            const trimmed = subword.trim();\n            const punctuation = punctuationRegex.test(trimmed);\n\n            if (special || with_space || punctuation || words.length === 0) {\n                words.push(subword);\n                word_tokens.push(subword_tokens);\n                token_indices.push(subword_indices);\n            } else {\n                const ix = words.length - 1;\n                words[ix] += subword;\n                word_tokens[ix].push(...subword_tokens);\n                token_indices[ix].push(...subword_indices);\n            }\n        }\n\n        return [words, word_tokens, token_indices];\n\n    }\n\n    /**\n     * Merges punctuation tokens with neighboring words.\n     * @param {string[]} words \n     * @param {number[][]} tokens \n     * @param {number[][]} indices \n     * @param {string} prepended \n     * @param {string} appended \n     * @private\n     */\n    mergePunctuations(words, tokens, indices, prepended, appended) {\n\n        let newWords = structuredClone(words);\n        let newTokens = structuredClone(tokens);\n        let newIndices = structuredClone(indices);\n\n\n        // prepend punctuations\n        let i = newWords.length - 2;\n        let j = newWords.length - 1;\n\n        while (i >= 0) {\n            if (newWords[i].startsWith(' ') && prepended.includes(newWords[i].trim())) {\n                newWords[j] = newWords[i] + newWords[j];\n                newTokens[j] = mergeArrays(newTokens[i], newTokens[j]);\n                newIndices[j] = mergeArrays(newIndices[i], newIndices[j]);\n                newWords[i] = '';\n                newTokens[i] = [];\n                newIndices[i] = [];\n            } else {\n                j = i;\n            }\n            --i;\n        }\n\n        // append punctuations\n        i = 0;\n        j = 1;\n        while (j < newWords.length) {\n            if (!newWords[i].endsWith(' ') && appended.includes(newWords[j])) {\n                newWords[i] += newWords[j];\n                newTokens[i] = mergeArrays(newTokens[i], newTokens[j]);\n                newIndices[i] = mergeArrays(newIndices[i], newIndices[j]);\n                newWords[j] = '';\n                newTokens[j] = [];\n                newIndices[j] = [];\n            } else {\n                i = j;\n            }\n            ++j;\n        }\n\n        return [\n            newWords.filter(x => x),\n            newTokens.filter(x => x.length > 0),\n            newIndices.filter(x => x.length > 0),\n        ]\n    }\n\n    /**\n     * Helper function to build translation inputs for a `WhisperTokenizer`,\n     * depending on the language, task, and whether to predict timestamp tokens.\n     * \n     * Used to override the prefix tokens appended to the start of the label sequence.\n     * \n     * **Example: Get ids for a language**\n     * ```javascript\n     * // instantiate the tokenizer and set the prefix token to Spanish\n     * let tokenizer = await WhisperTokenizer.from_pretrained('Xenova/whisper-tiny');\n     * let forced_decoder_ids = tokenizer.get_decoder_prompt_ids({ language: 'spanish' });\n     * // [(1, 50262), (2, 50363)]\n     * ```\n     * \n     * @param {Object} options Options to generate the decoder prompt.\n     * @param {string} [options.language] The language of the transcription text.\n     * The corresponding language id token is appended to the start of the sequence for multilingual\n     * speech recognition and speech translation tasks, e.g. for \"Spanish\" the token \"<|es|>\" is appended\n     * to the start of sequence.\n     * @param {string} [options.task] Task identifier to append at the start of sequence (if any).\n     * This should be used for mulitlingual fine-tuning, with \"transcribe\" for speech recognition and\n     * \"translate\" for speech translation.\n     * @param {boolean} [options.no_timestamps] Whether to add the <|notimestamps|> token at the start of the sequence.\n     * @returns {number[][]} The decoder prompt ids.\n     */\n    get_decoder_prompt_ids({\n        language = null,\n        task = null,\n        no_timestamps = true,\n    } = {}) {\n\n        // <|lang_id|> <|task|> <|notimestamps|>\n\n        let forced_decoder_ids = [];\n\n        if (language) {\n            // User wishes to specify the language\n            language = language.toLowerCase();\n\n            // Map to code from user-friendly name (e.g., \"english\" -> \"en\")\n            let language_code = WHISPER_TO_LANGUAGE_CODE_MAPPING.get(language);\n\n            if (language_code === undefined) {\n                // User provided something that is not a language name\n\n                if (WHISPER_LANGUAGE_MAPPING.has(language)) {\n                    // User provided the language code directly (e.g., \"en\")\n                    language_code = language;\n\n                } else {\n                    // User provided something that is not a language code or name\n                    const is_language_code = language.length === 2;\n                    const langs = is_language_code ? WHISPER_LANGUAGE_MAPPING.keys() : WHISPER_LANGUAGE_MAPPING.values();\n\n                    throw new Error(`Language \"${language}\" is not supported. Must be one of: ${JSON.stringify(langs)}`);\n                }\n            }\n\n            let language_token_id = this.model.tokens_to_ids.get(`<|${language_code}|>`);\n            if (language_token_id === undefined) {\n                throw new Error(`Unable to find language \"${language_code}\" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.`)\n            }\n\n            forced_decoder_ids.push(language_token_id);\n        } else {\n            // No token will be forced, which leaves the model to predict the language\n            forced_decoder_ids.push(null);\n        }\n\n        if (task) {\n            task = task.toLowerCase();\n            if (task !== 'transcribe' && task !== 'translate') {\n                throw new Error(`Task \"${task}\" is not supported. Must be one of: [\"transcribe\", \"translate\"]`);\n            }\n\n            let task_token_id = this.model.tokens_to_ids.get(`<|${task}|>`);\n            if (task_token_id === undefined) {\n                throw new Error(`Unable to find task \"${task}\" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.`)\n            }\n\n            forced_decoder_ids.push(task_token_id);\n        } else {\n            // No token will be forced, which leaves the model to predict the task\n            forced_decoder_ids.push(null);\n        }\n\n        if (no_timestamps) {\n            let no_timestamps_id = this.model.tokens_to_ids.get(`<|notimestamps|>`);\n            if (no_timestamps_id === undefined) {\n                throw new Error('Unable to find \"<|notimestamps|>\" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.')\n            }\n\n            forced_decoder_ids.push(no_timestamps_id);\n        }\n\n        return forced_decoder_ids.map((x, i) => [i + 1, x]).filter(x => x[1] !== null);\n\n    }\n}\nexport class CodeGenTokenizer extends PreTrainedTokenizer { }\nexport class CLIPTokenizer extends PreTrainedTokenizer { }\n\n\n/**\n * @todo This model is not yet supported by Hugging Face's \"fast\" tokenizers library (https://github.com/huggingface/tokenizers).\n * Therefore, this implementation (which is based on fast tokenizers) may produce slightly inaccurate results.\n */\nexport class MarianTokenizer extends PreTrainedTokenizer {\n    /**\n     * Create a new MarianTokenizer instance.\n     * @param {Object} tokenizerJSON The JSON of the tokenizer.\n     * @param {Object} tokenizerConfig The config of the tokenizer.\n     */\n    constructor(tokenizerJSON, tokenizerConfig) {\n        super(tokenizerJSON, tokenizerConfig);\n\n        this.languageRegex = /^(>>\\w+<<)\\s*/g;\n\n        this.supported_language_codes = this.model.vocab.filter(\n            x => this.languageRegex.test(x)\n        );\n\n        console.warn('WARNING: `MarianTokenizer` is not yet supported by Hugging Face\\'s \"fast\" tokenizers library. Therefore, you may experience slightly inaccurate results.')\n    }\n\n    /**\n     * Encodes a single text. Overriding this method is necessary since the language codes\n     * must be removed before encoding with sentencepiece model.\n     * @see https://github.com/huggingface/transformers/blob/12d51db243a00726a548a43cc333390ebae731e3/src/transformers/models/marian/tokenization_marian.py#L204-L213\n     *\n     * @param {string|null} text The text to encode.\n     * @returns {Array} The encoded tokens.\n     */\n    _encode_text(text) {\n        if (text === null) return null;\n\n        // Check if text starts with language code:\n        let [matchInfo, ...remainder] = text.trim().split(this.languageRegex);\n\n        if (remainder.length === 0) {\n            // No language code, encode normally\n            return super._encode_text(matchInfo);\n\n        } else if (remainder.length === 2) {\n            // Text starts with language code, so we do not encode it with sentencepiece.\n            let [language, text] = remainder;\n\n            if (!this.supported_language_codes.includes(language)) {\n                console.warn(`Unsupported language code \"${language}\" detected, which may lead to unexpected behavior. Should be one of: ${JSON.stringify(this.supported_language_codes)}`)\n            }\n            return mergeArrays([language], super._encode_text(text));\n        }\n    }\n\n}\n\nexport class Wav2Vec2CTCTokenizer extends PreTrainedTokenizer { }\n\nexport class BlenderbotTokenizer extends PreTrainedTokenizer {\n    _default_chat_template = `{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}`;\n}\nexport class BlenderbotSmallTokenizer extends BlenderbotTokenizer { } // NOTE `BlenderbotTokenizer` to get the correct chat template\n\nexport class SpeechT5Tokenizer extends PreTrainedTokenizer { }\n\nexport class NougatTokenizer extends PreTrainedTokenizer { }\n\n/**\n * Helper class which is used to instantiate pretrained tokenizers with the `from_pretrained` function.\n * The chosen tokenizer class is determined by the type specified in the tokenizer config.\n * \n * @example\n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/bert-base-uncased');\n */\nexport class AutoTokenizer {\n    static TOKENIZER_CLASS_MAPPING = {\n        T5Tokenizer,\n        DistilBertTokenizer,\n        CamembertTokenizer,\n        DebertaTokenizer,\n        DebertaV2Tokenizer,\n        BertTokenizer,\n        HerbertTokenizer,\n        ConvBertTokenizer,\n        XLMTokenizer,\n        ElectraTokenizer,\n        MobileBertTokenizer,\n        SqueezeBertTokenizer,\n        AlbertTokenizer,\n        GPT2Tokenizer,\n        BartTokenizer,\n        MBartTokenizer,\n        MBart50Tokenizer,\n        RobertaTokenizer,\n        WhisperTokenizer,\n        CodeGenTokenizer,\n        CLIPTokenizer,\n        MarianTokenizer,\n        BloomTokenizer,\n        NllbTokenizer,\n        M2M100Tokenizer,\n        LlamaTokenizer,\n        CodeLlamaTokenizer,\n        XLMRobertaTokenizer,\n        MPNetTokenizer,\n        FalconTokenizer,\n        GPTNeoXTokenizer,\n        EsmTokenizer,\n        Wav2Vec2CTCTokenizer,\n        BlenderbotTokenizer,\n        BlenderbotSmallTokenizer,\n        SpeechT5Tokenizer,\n        NougatTokenizer,\n\n        // Base case:\n        PreTrainedTokenizer,\n    }\n\n\n    /**\n     * Instantiate one of the tokenizer classes of the library from a pretrained model.\n     * \n     * The tokenizer class to instantiate is selected based on the `tokenizer_class` property of the config object\n     * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)\n     * \n     * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:\n     * - A string, the *model id* of a pretrained tokenizer hosted inside a model repo on huggingface.co.\n     *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n     *   user or organization name, like `dbmdz/bert-base-german-cased`.\n     * - A path to a *directory* containing tokenizer files, e.g., `./my_model_directory/`.\n     * @param {PretrainedTokenizerOptions} options Additional options for loading the tokenizer.\n     * \n     * @returns {Promise<PreTrainedTokenizer>} A new instance of the PreTrainedTokenizer class.\n     */\n    static async from_pretrained(pretrained_model_name_or_path, {\n        quantized = true,\n        progress_callback = null,\n        config = null,\n        cache_dir = null,\n        local_files_only = false,\n        revision = 'main',\n        legacy = null,\n    } = {}) {\n\n        let [tokenizerJSON, tokenizerConfig] = await loadTokenizer(pretrained_model_name_or_path, {\n            quantized,\n            progress_callback,\n            config,\n            cache_dir,\n            local_files_only,\n            revision,\n            legacy,\n        })\n\n        // Some tokenizers are saved with the \"Fast\" suffix, so we remove that if present.\n        let tokenizerName = tokenizerConfig.tokenizer_class?.replace(/Fast$/, '') ?? 'PreTrainedTokenizer';\n\n        let cls = this.TOKENIZER_CLASS_MAPPING[tokenizerName];\n        if (!cls) {\n            console.warn(`Unknown tokenizer class \"${tokenizerName}\", attempting to construct from base class.`);\n            cls = PreTrainedTokenizer;\n        }\n        return new cls(tokenizerJSON, tokenizerConfig);\n    }\n}\n","\n/**\n * @file Definitions of all models available in Transformers.js.\n * \n * **Example:** Load and run an `AutoModel`.\n * \n * ```javascript\n * import { AutoModel, AutoTokenizer } from '@xenova/transformers';\n *\n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/bert-base-uncased');\n * let model = await AutoModel.from_pretrained('Xenova/bert-base-uncased');\n *\n * let inputs = await tokenizer('I love transformers!');\n * let { logits } = await model(inputs);\n * // Tensor {\n * //     data: Float32Array(183132) [-7.117443084716797, -7.107812881469727, -7.092104911804199, ...]\n * //     dims: (3) [1, 6, 30522],\n * //     type: \"float32\",\n * //     size: 183132,\n * // }\n * ```\n * \n * We also provide other `AutoModel`s (listed below), which you can use in the same way as the Python library. For example:\n * \n * **Example:** Load and run an `AutoModelForSeq2SeqLM`.\n * ```javascript\n * import { AutoModelForSeq2SeqLM, AutoTokenizer } from '@xenova/transformers';\n * \n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/t5-small');\n * let model = await AutoModelForSeq2SeqLM.from_pretrained('Xenova/t5-small');\n *\n * let { input_ids } = await tokenizer('translate English to German: I love transformers!');\n * let outputs = await model.generate(input_ids);\n * let decoded = tokenizer.decode(outputs[0], { skip_special_tokens: true });\n * // 'Ich liebe Transformatoren!'\n * ```\n * \n * @module models\n */\n\nimport {\n    AutoConfig,\n} from './configs.js';\n\nimport {\n    add_token_types,\n} from './tokenizers.js';\n\nimport {\n    Callable,\n    isIntegralNumber,\n    isTypedArray,\n    mergeArrays,\n} from './utils/core.js';\n\nimport {\n    getModelFile,\n    getModelJSON,\n} from './utils/hub.js';\n\nimport {\n    LogitsProcessorList,\n    GenerationConfig,\n    ForceTokensLogitsProcessor,\n    ForcedBOSTokenLogitsProcessor,\n    ForcedEOSTokenLogitsProcessor,\n    SuppressTokensAtBeginLogitsProcessor,\n    WhisperTimeStampLogitsProcessor,\n    NoRepeatNGramLogitsProcessor,\n    RepetitionPenaltyLogitsProcessor,\n    NoBadWordsLogitsProcessor,\n    MinLengthLogitsProcessor,\n    MinNewTokensLengthLogitsProcessor,\n\n    Sampler,\n} from './utils/generation.js';\n\nimport {\n    cat,\n    dynamicTimeWarping,\n    mean,\n    ones_like,\n    stack,\n    std_mean,\n    Tensor,\n} from './utils/tensor.js';\n\nimport { executionProviders, ONNX } from './backends/onnx.js';\nimport { medianFilter } from './transformers.js';\nconst { InferenceSession, Tensor: ONNXTensor, env } = ONNX;\n\n/** @typedef {import('onnxruntime-web').InferenceSession} InferenceSession */\n\n//////////////////////////////////////////////////\n// Model types: used internally\nconst MODEL_TYPES = {\n    EncoderOnly: 0,\n    EncoderDecoder: 1,\n    Seq2Seq: 2,\n    Vision2Seq: 3,\n    DecoderOnly: 4,\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Helper functions\n\n// NOTE: These will be populated fully later\nconst MODEL_TYPE_MAPPING = new Map();\nconst MODEL_NAME_TO_CLASS_MAPPING = new Map();\nconst MODEL_CLASS_TO_NAME_MAPPING = new Map();\n\n\n/**\n * Constructs an InferenceSession using a model file located at the specified path.\n * @param {string} pretrained_model_name_or_path The path to the directory containing the model file.\n * @param {string} fileName The name of the model file.\n * @param {import('./utils/hub.js').PretrainedOptions} options Additional options for loading the model.\n * @returns {Promise<InferenceSession>} A Promise that resolves to an InferenceSession object.\n * @private\n */\nasync function constructSession(pretrained_model_name_or_path, fileName, options) {\n    // TODO add option for user to force specify their desired execution provider\n    let modelFileName = `onnx/${fileName}${options.quantized ? '_quantized' : ''}.onnx`;\n    let buffer = await getModelFile(pretrained_model_name_or_path, modelFileName, true, options);\n\n    try {\n        return await InferenceSession.create(buffer, {\n            executionProviders,\n        });\n    } catch (err) {\n        // If the execution provided was only wasm, throw the error\n        if (executionProviders.length === 1 && executionProviders[0] === 'wasm') {\n            throw err;\n        }\n\n        console.warn(err);\n        console.warn(\n            'Something went wrong during model construction (most likely a missing operation). ' +\n            'Using `wasm` as a fallback. '\n        )\n        return await InferenceSession.create(buffer, {\n            executionProviders: ['wasm']\n        });\n    }\n}\n\n/**\n * Validate model inputs\n * @param {InferenceSession} session The InferenceSession object that will be run.\n * @param {Record<string, Tensor>} inputs The inputs to check.\n * @returns {Record<string, Tensor>} The checked inputs.\n * @throws {Error} If any inputs are missing.\n * @private\n */\nfunction validateInputs(session, inputs) {\n    /**\n     * NOTE: Create either a shallow or deep copy based on `onnx.wasm.proxy`\n     * @type {Record<string, Tensor>}\n     */\n    const checkedInputs = Object.create(null);\n    const missingInputs = [];\n    for (const inputName of session.inputNames) {\n        const tensor = inputs[inputName];\n        // Rare case where one of the model's input names corresponds to a built-in\n        // object name (e.g., toString), which would cause a simple (!tensor) check to fail,\n        // because it's not undefined but a function.\n        if (!(tensor instanceof Tensor)) {\n            missingInputs.push(inputName);\n            continue;\n        }\n        // NOTE: When `env.wasm.proxy is true` the tensor is moved across the Worker\n        // boundary, transferring ownership to the worker and invalidating the tensor.\n        // So, in this case, we simply sacrifice a clone for it.\n        checkedInputs[inputName] = env.wasm.proxy ? tensor.clone() : tensor;\n    }\n    if (missingInputs.length > 0) {\n        throw new Error(\n            `An error occurred during model execution: \"Missing the following inputs: ${missingInputs.join(', ')}.`);\n    }\n\n    const numInputsProvided = Object.keys(inputs).length;\n    const numInputsNeeded = session.inputNames.length;\n    if (numInputsProvided > numInputsNeeded) {\n        // No missing inputs, but too many inputs were provided.\n        // Warn the user and ignore the extra inputs.\n        let ignored = Object.keys(inputs).filter(inputName => !session.inputNames.includes(inputName));\n        console.warn(`WARNING: Too many inputs were provided (${numInputsProvided} > ${numInputsNeeded}). The following inputs will be ignored: \"${ignored.join(', ')}\".`);\n    }\n\n    return checkedInputs;\n}\n\n/**\n * Executes an InferenceSession using the specified inputs.\n * NOTE: `inputs` must contain at least the input names of the model.\n *  - If additional inputs are passed, they will be ignored.\n *  - If inputs are missing, an error will be thrown.\n * \n * @param {InferenceSession} session The InferenceSession object to run.\n * @param {Object} inputs An object that maps input names to input tensors.\n * @returns {Promise<Object>} A Promise that resolves to an object that maps output names to output tensors.\n * @private\n */\nasync function sessionRun(session, inputs) {\n    const checkedInputs = validateInputs(session, inputs);\n    try {\n        // @ts-ignore\n        let output = await session.run(checkedInputs);\n        output = replaceTensors(output);\n        return output;\n    } catch (e) {\n        // This usually occurs when the inputs are of the wrong type.\n        console.error(`An error occurred during model execution: \"${e}\".`);\n        console.error('Inputs given to model:', checkedInputs);\n        throw e;\n    }\n}\n\n/**\n * Replaces ONNX Tensor objects with custom Tensor objects to support additional functions.\n * @param {Object} obj The object to replace tensor objects in.\n * @returns {Object} The object with tensor objects replaced by custom Tensor objects.\n * @private\n */\nfunction replaceTensors(obj) {\n    for (let prop in obj) {\n        if (obj[prop] instanceof ONNXTensor) {\n            obj[prop] = new Tensor(obj[prop]);\n        } else if (typeof obj[prop] === 'object') {\n            replaceTensors(obj[prop]);\n        }\n    }\n    return obj;\n}\n\n\n/**\n * Converts an array or Tensor of integers to an int64 Tensor.\n * @param {Array|Tensor} items The input integers to be converted.\n * @returns {Tensor} The int64 Tensor with the converted values.\n * @throws {Error} If the input array is empty or the input is a batched Tensor and not all sequences have the same length.\n * @private\n */\nfunction toI64Tensor(items) {\n    if (items instanceof Tensor) {\n        return items;\n    }\n    // items is an array\n    if (items.length === 0) {\n        throw Error(\"items must be non-empty\");\n    }\n\n    if (Array.isArray(items[0])) {\n        // batched\n        if (items.some(x => x.length !== items[0].length)) {\n            throw Error(\"Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' and/or 'truncation=True' to have batched tensors with the same length.\")\n        }\n\n        return new Tensor('int64',\n            BigInt64Array.from(items.flat().map(x => BigInt(x))),\n            [items.length, items[0].length]\n        );\n    } else {\n        //flat\n        return new Tensor('int64',\n            BigInt64Array.from(items.map(x => BigInt(x))),\n            [1, items.length]\n        );\n    }\n}\n\n/**\n * Prepares an attention mask for a sequence of tokens based on configuration options.\n * @param {Object} self The calling object instance.\n * @param {Tensor} tokens The input tokens.\n * @returns {Tensor} The attention mask tensor.\n * @private\n */\nfunction prepareAttentionMask(self, tokens) {\n\n    // Prepare attention mask\n    let pad_token_id = self.config.pad_token_id ?? null;\n    let eos_token_id = self.config.eos_token_id ?? null;\n    if (isIntegralNumber(eos_token_id)) {\n        eos_token_id = [eos_token_id];\n    }\n\n    let is_pad_token_in_inputs = tokens.indexOf(pad_token_id) !== -1;\n    let is_pad_token_not_equal_to_eos_token_id = (eos_token_id === null) || !eos_token_id.includes(pad_token_id)\n\n    if (is_pad_token_in_inputs && is_pad_token_not_equal_to_eos_token_id) {\n        let data = BigInt64Array.from(\n            // Note: != so that int matches bigint\n            // @ts-ignore\n            tokens.data.map(x => x != pad_token_id)\n        )\n        return new Tensor('int64', data, tokens.dims)\n    } else {\n        return ones_like(tokens);\n    }\n}\n\n/**\n * Add position IDs to the feeds object.\n * @param {Object} session The inference session.\n * @param {Object} feeds The input to the model.\n * @param {boolean} use_cache_branch Whether to use the cache branch of the model.\n * @returns {void}\n * @private\n */\nfunction preparePositionIds(session, feeds, use_cache_branch) {\n    if (!session.inputNames.includes('position_ids')) return;\n\n    const data = new BigInt64Array(feeds.attention_mask.data.length);\n\n    // Compute cumulative sum of the attention mask along the sequence length dimension\n    for (let i = 0; i < feeds.attention_mask.dims[0]; ++i) {\n        let start = i * feeds.attention_mask.dims[1];\n        let sum = BigInt(0);\n        for (let j = 0; j < feeds.attention_mask.dims[1]; ++j) {\n            const index = start + j;\n            if (feeds.attention_mask.data[index] === 0n) {\n                data[index] = BigInt(1);\n            } else { // === 1n\n                data[index] = sum;\n                sum += feeds.attention_mask.data[index];\n            }\n        }\n    }\n\n    feeds.position_ids = new Tensor('int64', data, feeds.attention_mask.dims);\n\n    if (use_cache_branch) {\n        feeds.position_ids = feeds.position_ids.slice(null, -1).unsqueeze_(-1);\n    }\n}\n\n/**\n * Creates a boolean tensor with a single value.\n * @param {boolean} value The value of the tensor.\n * @returns {Tensor} The boolean tensor.\n * @private\n */\nfunction boolTensor(value) {\n    return new Tensor('bool', [value], [1]);\n}\n\n// JS doesn't support mixins, so we define some reused functions here, and allow \"this\" to be passed in\n/**\n * Perform forward pass on the seq2seq model (both encoder and decoder).\n * @param {Object} self The seq2seq model object.\n * @param {Object} model_inputs The input object for the model containing encoder and decoder inputs.\n * @returns {Promise<Seq2SeqLMOutput>} Promise that resolves with the output of the seq2seq model.\n * @private\n */\nasync function seq2seqForward(self, model_inputs) {\n\n    let { encoder_outputs, past_key_values } = model_inputs;\n\n    if (!encoder_outputs) {\n        // Encoder outputs are not given, so we must compute them.\n        encoder_outputs = (await encoderForward(self, model_inputs)).last_hidden_state;\n    }\n    let decoderFeeds = {\n        input_ids: model_inputs.decoder_input_ids,\n        encoder_hidden_states: encoder_outputs,\n    };\n    const use_cache_branch = !!past_key_values;\n\n    if (self.decoder_merged_session.inputNames.includes('use_cache_branch')) {\n        decoderFeeds.use_cache_branch = boolTensor(use_cache_branch);\n    }\n\n    if (self.decoder_merged_session.inputNames.includes('encoder_attention_mask')) {\n        decoderFeeds.encoder_attention_mask = model_inputs.attention_mask\n    }\n\n    preparePositionIds(self.decoder_merged_session, decoderFeeds, use_cache_branch);\n    self.addPastKeyValues(decoderFeeds, past_key_values);\n\n    const decoderResults = await sessionRun(self.decoder_merged_session, decoderFeeds);\n    let logits = decoderResults.logits;\n    past_key_values = self.getPastKeyValues(decoderResults, past_key_values);\n\n    // Get cross attention and/or decoder attentions if they are present\n    const attns = self.getAttentions(decoderResults);\n\n    return new Seq2SeqLMOutput({ logits, past_key_values, encoder_outputs, ...attns });\n}\n\n/**\n * Start the beam search process for the seq2seq model.\n * @param {PreTrainedModel} self The seq2seq model object.\n * @param {Tensor} inputTokenIds Array of input token ids for each input sequence.\n * @param {Object} generation_config The generation config.\n * @param {number} numOutputTokens The maximum number of output tokens for the model.\n * @returns {Object[]} Array of beam search objects.\n * @private\n */\nfunction seq2seqStartBeams(self, inputTokenIds, generation_config, numOutputTokens) {\n    let beams = [];\n    let beamId = 0;\n\n    // @ts-ignore\n    const requires_attention_mask = self.requires_attention_mask ?? true;\n\n    // decoder_input_ids == output_token_ids\n    let decoder_input_ids =\n        generation_config.decoder_input_ids\n        ?? generation_config.decoder_start_token_id\n        ?? generation_config.bos_token_id\n        ?? generation_config.eos_token_id;\n\n    // Support input as tensor or list\n    // TODO support batched decoder_input_ids\n    if (decoder_input_ids instanceof Tensor) {\n        decoder_input_ids = decoder_input_ids.tolist().flat();\n    } else if (!Array.isArray(decoder_input_ids)) {\n        decoder_input_ids = [decoder_input_ids];\n    }\n\n    for (let tokens of inputTokenIds) {\n        // TODO: Improve\n        // Currently, just add back batch dimension.\n        // In future, allow for true parallel execution\n        tokens.dims = [1, ...tokens.dims]\n\n        // Create beam\n        let start = {\n            inputs: tokens,\n            encoder_outputs: null,\n            prev_model_outputs: null,\n\n            output_token_ids: decoder_input_ids,\n            done: false,\n            score: 0,\n            id: beamId++ // assign unique id to beams\n        }\n\n        if (requires_attention_mask) {\n            start.attention_mask = prepareAttentionMask(self, tokens);\n        }\n\n        beams.push(start);\n    }\n\n    return beams;\n}\n\n/**\n * Run beam search on the seq2seq model for a single beam.\n * @param {PreTrainedModel} self The seq2seq model object.\n * @param {Object} beam The beam search object for which to run the model.\n * @param {Object} options options\n * @param {string} [options.input_name='input_ids'] The name of the input tensor for the encoder.\n * @returns {Promise<Object>} Promise that resolves with the output of the seq2seq model for the given beam.\n * @private\n */\nasync function seq2seqRunBeam(self, beam) {\n    const input_name = self.main_input_name;\n\n    let decoder_input_ids = beam.output_token_ids;\n    if (beam.prev_model_outputs) {\n        // After the first step, `prev_model_outputs` won't be null.\n        // So, we cut decoder_input_ids if past is used\n        decoder_input_ids = decoder_input_ids.slice(-1);\n    }\n\n    // 1. Prepare\n    let model_inputs = {\n        [input_name]: beam.inputs,\n        decoder_input_ids: toI64Tensor(decoder_input_ids),\n        encoder_outputs: beam.encoder_outputs,\n        past_key_values: beam.prev_model_outputs?.past_key_values,\n    }\n    if (beam.attention_mask) {\n        model_inputs.attention_mask = beam.attention_mask\n    }\n\n    // 2. Run\n    let output = await self.forward(model_inputs);\n\n    // 3. Update\n    beam.prev_model_outputs = output;\n    beam.encoder_outputs = output.encoder_outputs;\n\n    return output;\n}\n\n/**\n * Update a beam with a new token ID.\n * @param {Object} beam The beam to update.\n * @param {number} newTokenId The new token ID to add to the beam's output.\n * @private\n */\nfunction seq2seqUpdatebeam(beam, newTokenId) {\n    beam.output_token_ids = [...beam.output_token_ids, newTokenId];\n}\n\n/**\n * Forward pass of an encoder model.\n * @param {Object} self The encoder model.\n * @param {Object} model_inputs The input data to be used for the forward pass.\n * @returns {Promise<Object>} Promise that resolves with an object containing the model's outputs.\n * @private\n */\nasync function encoderForward(self, model_inputs) {\n    const encoderFeeds = Object.create(null);\n    for (const key of self.session.inputNames) {\n        encoderFeeds[key] = model_inputs[key];\n    }\n    if (self.session.inputNames.includes('token_type_ids') && !encoderFeeds.token_type_ids) {\n        // Assign default `token_type_ids` to the `encoderFeeds` if the model expects it,\n        // but they weren't created by the tokenizer.\n        add_token_types(encoderFeeds);\n    }\n    return await sessionRun(self.session, encoderFeeds);\n}\n\n\n/**\n * Forward pass of a decoder model.\n * @param {Object} self The decoder model.\n * @param {Object} model_inputs The input data to be used for the forward pass.\n * @returns {Promise<Object>} Promise that resolves with an object containing the logits and past key values.\n * @private\n */\nasync function decoderForward(self, model_inputs) {\n    let { input_ids, past_key_values, attention_mask } = model_inputs;\n    let decoderFeeds = {\n        input_ids: input_ids,\n        attention_mask: attention_mask ?? prepareAttentionMask(self, input_ids),\n    }\n    const use_cache_branch = !!past_key_values;\n\n    if (self.session.inputNames.includes('use_cache_branch')) {\n        decoderFeeds.use_cache_branch = boolTensor(use_cache_branch);\n    }\n\n    preparePositionIds(self.session, decoderFeeds, use_cache_branch);\n\n    self.addPastKeyValues(decoderFeeds, past_key_values);\n\n    let decoderResults = await sessionRun(self.session, decoderFeeds);\n\n    let logits = decoderResults.logits;\n\n    past_key_values = self.getPastKeyValues(decoderResults, past_key_values);\n    return { logits, past_key_values };\n}\n\n/**\n * Starts the generation of text by initializing the beams for the given input token IDs.\n * @param {Object} self The text generation model object.\n * @param {Tensor} inputTokenIds An tensor of input token IDs to generate text from.\n * @param {Object} generation_config The generation config.\n * @param {number} numOutputTokens The maximum number of tokens to generate for each beam.\n * @param {Tensor} [inputs_attention_mask] The attention mask tensor for the input token IDs.\n * @returns {Object[]} An array of beams initialized with the given inputs and parameters.\n * @private\n */\nfunction decoderStartBeams(self, inputTokenIds, generation_config, numOutputTokens, inputs_attention_mask) {\n    let beams = [];\n\n    let beamId = 0;\n    for (let tokens of inputTokenIds) {\n        let output_token_ids = tokens.tolist().map(Number);\n\n        // TODO: Improve\n        // Currently, just add back batch dimension.\n        // In future, allow for true parallel execution\n        tokens.dims = [1, ...tokens.dims]\n\n        let attn_mask;\n        if (inputs_attention_mask) {\n            attn_mask = inputs_attention_mask[beamId];\n            attn_mask.dims = [1, ...attn_mask.dims]\n\n        } else {\n            attn_mask = prepareAttentionMask(self, tokens)\n        }\n\n        let start = {\n            input: tokens,\n            model_input_ids: tokens,\n            attention_mask: attn_mask,\n            prev_model_outputs: null,\n\n            output_token_ids: output_token_ids,\n            num_output_tokens: numOutputTokens,\n\n            done: false,\n            score: 0,\n            id: beamId++ // assign unique id to beams\n        }\n\n        beams.push(start);\n    }\n    return beams;\n}\n\n/**\n * Runs a single step of the text generation process for a given beam.\n *\n * @param {Object} self The decoder object.\n * @param {Object} beam The beam to run.\n * @param {Tensor} beam.input The input tensor.\n * @param {Tensor} beam.model_input_ids The input ids to the model.\n * @param {Tensor} beam.attention_mask The attention mask.\n * @param {Object} beam.prev_model_outputs The past key values.\n * @param {number[]} beam.output_token_ids The output token ids.\n * @returns {Promise<Object>} The output of the generation step.\n * @private\n */\nasync function decoderRunBeam(self, beam) {\n    let attnMaskData = new BigInt64Array(beam.output_token_ids.length).fill(1n)\n\n    // 1. Prepare\n    let model_inputs = {\n        input_ids: beam.model_input_ids,\n        attention_mask: new Tensor(\n            'int64',\n            attnMaskData,\n            [1, attnMaskData.length]\n        ),\n        past_key_values: beam.prev_model_outputs?.past_key_values,\n    }\n\n    // 2. Run\n    let output = await self.forward(model_inputs);\n\n    // 3. Update\n    beam.prev_model_outputs = output;\n\n    return output;\n}\n\n/**\n * Update a beam with a new token ID.\n * @param {Object} beam The beam to update.\n * @param {number} newTokenId The new token ID to add to the beam's output.\n * @private\n */\nfunction decoderUpdatebeam(beam, newTokenId) {\n    beam.output_token_ids = [...beam.output_token_ids, newTokenId];\n    beam.model_input_ids = new Tensor('int64', [BigInt(newTokenId)], [1, 1]);\n}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n/**\n * A base class for pre-trained models that provides the model configuration and an ONNX session.\n */\nexport class PreTrainedModel extends Callable {\n    main_input_name = 'input_ids';\n\n    /**\n     * Creates a new instance of the `PreTrainedModel` class.\n     * @param {Object} config The model configuration.\n     * @param {any} session session for the model.\n     */\n    constructor(config, session) {\n        super();\n\n        this.config = config;\n        this.session = session;\n\n        const modelName = MODEL_CLASS_TO_NAME_MAPPING.get(this.constructor);\n        const modelType = MODEL_TYPE_MAPPING.get(modelName);\n\n        this.can_generate = false;\n        this._runBeam = null;\n        this._getStartBeams = null;\n        this._updateBeam = null;\n        this._forward = null;\n        if (modelType === MODEL_TYPES.DecoderOnly) {\n            this.can_generate = true;\n\n            this._runBeam = decoderRunBeam;\n            this._getStartBeams = decoderStartBeams;\n            this._updateBeam = decoderUpdatebeam;\n            this._forward = decoderForward;\n\n        } else if (modelType === MODEL_TYPES.Seq2Seq || modelType === MODEL_TYPES.Vision2Seq) {\n            this.can_generate = true;\n\n            this._runBeam = seq2seqRunBeam;\n            this._getStartBeams = seq2seqStartBeams;\n            this._updateBeam = seq2seqUpdatebeam;\n            this._forward = seq2seqForward;\n\n        } else if (modelType === MODEL_TYPES.EncoderDecoder) {\n            this._forward = encoderForward;\n\n        } else { // should be MODEL_TYPES.EncoderOnly\n            this._forward = encoderForward;\n        }\n    }\n\n    /**\n    * Disposes of all the ONNX sessions that were created during inference.\n    * @returns {Promise<unknown[]>} An array of promises, one for each ONNX session that is being disposed.\n    * @todo Use https://developer.mozilla.org/en-US/docs/Web/JavaScript/Reference/Global_Objects/FinalizationRegistry\n    */\n    async dispose() {\n        const promises = [];\n        for (let key of Object.keys(this)) {\n            const item = this[key];\n            // @ts-ignore\n            if (item instanceof InferenceSession) {\n                promises.push(item.handler.dispose())\n            }\n        }\n        return await Promise.all(promises);\n    }\n\n    /**\n     * Instantiate one of the model classes of the library from a pretrained model.\n     * \n     * The model class to instantiate is selected based on the `model_type` property of the config object\n     * (either passed as an argument or loaded from `pretrained_model_name_or_path` if possible)\n     * \n     * @param {string} pretrained_model_name_or_path The name or path of the pretrained model. Can be either:\n     * - A string, the *model id* of a pretrained model hosted inside a model repo on huggingface.co.\n     *   Valid model ids can be located at the root-level, like `bert-base-uncased`, or namespaced under a\n     *   user or organization name, like `dbmdz/bert-base-german-cased`.\n     * - A path to a *directory* containing model weights, e.g., `./my_model_directory/`.\n     * @param {import('./utils/hub.js').PretrainedOptions} options Additional options for loading the model.\n     * \n     * @returns {Promise<PreTrainedModel>} A new instance of the `PreTrainedModel` class.\n     */\n    static async from_pretrained(pretrained_model_name_or_path, {\n        quantized = true,\n        progress_callback = null,\n        config = null,\n        cache_dir = null,\n        local_files_only = false,\n        revision = 'main',\n        model_file_name = null,\n    } = {}) {\n\n        let options = {\n            quantized,\n            progress_callback,\n            config,\n            cache_dir,\n            local_files_only,\n            revision,\n            model_file_name,\n        }\n\n        const modelName = MODEL_CLASS_TO_NAME_MAPPING.get(this);\n        const modelType = MODEL_TYPE_MAPPING.get(modelName);\n\n        let info;\n        if (modelType === MODEL_TYPES.DecoderOnly) {\n            info = await Promise.all([\n                AutoConfig.from_pretrained(pretrained_model_name_or_path, options),\n                constructSession(pretrained_model_name_or_path, options.model_file_name ?? 'decoder_model_merged', options),\n                getModelJSON(pretrained_model_name_or_path, 'generation_config.json', false, options),\n            ]);\n\n        } else if (modelType === MODEL_TYPES.Seq2Seq || modelType === MODEL_TYPES.Vision2Seq) {\n            info = await Promise.all([\n                AutoConfig.from_pretrained(pretrained_model_name_or_path, options),\n                constructSession(pretrained_model_name_or_path, 'encoder_model', options),\n                constructSession(pretrained_model_name_or_path, 'decoder_model_merged', options),\n                getModelJSON(pretrained_model_name_or_path, 'generation_config.json', false, options),\n            ]);\n\n        } else if (modelType === MODEL_TYPES.EncoderDecoder) {\n            info = await Promise.all([\n                AutoConfig.from_pretrained(pretrained_model_name_or_path, options),\n                constructSession(pretrained_model_name_or_path, 'encoder_model', options),\n                constructSession(pretrained_model_name_or_path, 'decoder_model_merged', options),\n            ]);\n\n        } else { // should be MODEL_TYPES.EncoderOnly\n            if (modelType !== MODEL_TYPES.EncoderOnly) {\n                console.warn(`Model type for '${modelName}' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose.`)\n            }\n            info = await Promise.all([\n                AutoConfig.from_pretrained(pretrained_model_name_or_path, options),\n                constructSession(pretrained_model_name_or_path, options.model_file_name ?? 'model', options)\n            ]);\n        }\n\n        // @ts-ignore\n        return new this(...info);\n    }\n\n    /**\n     * Runs the model with the provided inputs\n     * @param {Object} model_inputs Object containing input tensors\n     * @returns {Promise<Object>} Object containing output tensors\n     */\n    async _call(model_inputs) {\n        return await this.forward(model_inputs);\n    }\n\n    /**\n     * Forward method for a pretrained model. If not overridden by a subclass, the correct forward method\n     * will be chosen based on the model type.\n     * @param {Object} model_inputs The input data to the model in the format specified in the ONNX model.\n     * @returns {Promise<Object>} The output data from the model in the format specified in the ONNX model.\n     * @throws {Error} This method must be implemented in subclasses.\n     */\n    async forward(model_inputs) {\n        return await this._forward(this, model_inputs);\n    }\n\n    /**\n     * @param {GenerationConfig} generation_config \n     * @param {number} input_ids_seq_length The starting sequence length for the input ids.\n     * @returns {LogitsProcessorList}\n     * @private\n     */\n    _get_logits_processor(\n        generation_config,\n        input_ids_seq_length,\n        // encoder_input_ids, TODO\n        // prefix_allowed_tokens_fn, TODO\n        logits_processor = null\n    ) {\n        const processors = new LogitsProcessorList();\n\n        // if (generation_config.diversity_penalty !== null && generation_config.diversity_penalty > 0.0) {\n        //     processors.push(new HammingDiversityLogitsProcessor(\n        //         generation_config.diversity_penalty,\n        //         generation_config.num_beams,\n        //         generation_config.num_beam_groups\n        //     ));\n        // }\n\n        // if (generation_config.encoder_repetition_penalty !== null && generation_config.encoder_repetition_penalty !== 1.0) {\n        //     processors.push(new EncoderRepetitionPenaltyLogitsProcessor(\n        //         generation_config.encoder_repetition_penalty,\n        //         encoder_input_ids\n        //     ));\n        // }\n\n        if (generation_config.repetition_penalty !== null && generation_config.repetition_penalty !== 1.0) {\n            processors.push(new RepetitionPenaltyLogitsProcessor(generation_config.repetition_penalty));\n        }\n\n        if (generation_config.no_repeat_ngram_size !== null && generation_config.no_repeat_ngram_size > 0) {\n            processors.push(new NoRepeatNGramLogitsProcessor(generation_config.no_repeat_ngram_size));\n        }\n\n        // if (generation_config.encoder_no_repeat_ngram_size !== null && generation_config.encoder_no_repeat_ngram_size > 0) {\n        //     if (this.config.is_encoder_decoder) {\n        //         processors.push(new EncoderNoRepeatNGramLogitsProcessor(\n        //             generation_config.encoder_no_repeat_ngram_size,\n        //             encoder_input_ids\n        //         ));\n        //     } else {\n        //         throw new Error(\"It's impossible to use `encoder_no_repeat_ngram_size` with decoder-only architecture\");\n        //     }\n        // }\n\n        if (generation_config.bad_words_ids !== null) {\n            processors.push(new NoBadWordsLogitsProcessor(generation_config.bad_words_ids, generation_config.eos_token_id));\n        }\n\n        if (generation_config.min_length !== null && generation_config.eos_token_id !== null && generation_config.min_length > 0) {\n            processors.push(new MinLengthLogitsProcessor(generation_config.min_length, generation_config.eos_token_id));\n        }\n\n        if (generation_config.min_new_tokens !== null && generation_config.eos_token_id !== null && generation_config.min_new_tokens > 0) {\n            processors.push(new MinNewTokensLengthLogitsProcessor(\n                input_ids_seq_length,\n                generation_config.min_new_tokens,\n                generation_config.eos_token_id\n            ));\n        }\n\n        // if (prefix_allowed_tokens_fn !== null) {\n        //     processors.push(new PrefixConstrainedLogitsProcessor(\n        //         prefix_allowed_tokens_fn,\n        //         generation_config.num_beams / generation_config.num_beam_groups\n        //     ));\n        // }\n\n\n        if (generation_config.forced_bos_token_id !== null) {\n            processors.push(new ForcedBOSTokenLogitsProcessor(generation_config.forced_bos_token_id));\n        }\n\n        if (generation_config.forced_eos_token_id !== null) {\n            processors.push(new ForcedEOSTokenLogitsProcessor(\n                generation_config.max_length,\n                generation_config.forced_eos_token_id\n            ));\n        }\n\n        // if (generation_config.remove_invalid_values === true) {\n        //     processors.push(new InfNanRemoveLogitsProcessor());\n        // }\n\n        // if (generation_config.exponential_decay_length_penalty !== null) {\n        //     processors.push(new ExponentialDecayLengthPenalty(\n        //         generation_config.exponential_decay_length_penalty,\n        //         generation_config.eos_token_id,\n        //         input_ids_seq_length\n        //     ));\n        // }\n\n        // if (generation_config.suppress_tokens !== null) {\n        //     processors.push(new SuppressTokensLogitsProcessor(generation_config.suppress_tokens));\n        // }\n\n        if (generation_config.begin_suppress_tokens !== null) {\n            let begin_index = (input_ids_seq_length > 1 || generation_config.forced_bos_token_id === null)\n                ? input_ids_seq_length\n                : input_ids_seq_length + 1;\n\n            if (generation_config.forced_decoder_ids !== null) {\n                // generation starts after the last token that is forced\n                begin_index += generation_config.forced_decoder_ids[generation_config.forced_decoder_ids.length - 1][0];\n            }\n            processors.push(new SuppressTokensAtBeginLogitsProcessor(generation_config.begin_suppress_tokens, begin_index));\n        }\n\n        if (generation_config.forced_decoder_ids !== null) {\n            processors.push(new ForceTokensLogitsProcessor(generation_config.forced_decoder_ids));\n        }\n\n        if (logits_processor !== null) {\n            processors.extend(logits_processor)\n        }\n\n        // `LogitNormalization` should always be the last logit processor, when present\n        // if (generation_config.renormalize_logits === true) {\n        //     processors.push(new LogitNormalization());\n        // }\n\n        return processors;\n    }\n\n    /**\n     * This function merges multiple generation configs together to form a final generation config to be used by the model for text generation.\n     * It first creates an empty `GenerationConfig` object, then it applies the model's own `generation_config` property to it. Finally, if a `generation_config` object was passed in the arguments, it overwrites the corresponding properties in the final config with those of the passed config object.\n     *\n     * @param {GenerationConfig} generation_config A `GenerationConfig` object containing generation parameters.\n     * @returns {GenerationConfig} The final generation config object to be used by the model for text generation.\n     */\n    _get_generation_config(generation_config) {\n        // Create empty generation config (contains defaults)\n        // We pass `this.config` so that if `eos_token_id` or `bos_token_id` exist in the model's config, we will use them\n        let gen_config = new GenerationConfig(this.config);\n\n        // Apply model's generation config, if it exists\n        if ('generation_config' in this) {\n            Object.assign(gen_config, this.generation_config);\n        }\n\n        // Finally, use any generation config specified by the user\n        // when calling `generate`\n        if (generation_config !== null) {\n            Object.assign(gen_config, generation_config);\n        }\n        return gen_config;\n    }\n\n    /**\n     * @typedef {import('./utils/maths.js').TypedArray} TypedArray\n     */\n\n    /**\n     * @typedef {{ sequences: Tensor, decoder_attentions: Tensor, cross_attentions: Tensor }} EncoderDecoderOutput\n     * @typedef {Object} DecoderOutput\n     * \n     * Generates text based on the given inputs and generation configuration using the model.\n     * @param {Tensor|Array|TypedArray} inputs An array of input token IDs.\n     * @param {Object|GenerationConfig|null} generation_config The generation configuration to use. If null, default configuration will be used.\n     * @param {Object|null} logits_processor An optional logits processor to use. If null, a new LogitsProcessorList instance will be created.\n     * @param {Object} options options\n     * @param {Object} [options.inputs_attention_mask=null] An optional attention mask for the inputs.\n     * @returns {Promise<number[][]|EncoderDecoderOutput|DecoderOutput>} An array of generated output sequences, where each sequence is an array of token IDs.\n     * @throws {Error} Throws an error if the inputs array is empty.\n     */\n    async generate(\n        inputs,\n        generation_config = null,\n        logits_processor = null,\n        {\n            inputs_attention_mask = null\n        } = {},\n    ) {\n        if (!this.can_generate) {\n            const modelName = MODEL_CLASS_TO_NAME_MAPPING.get(this.constructor);\n            let errorMessage = `The current model class (${modelName}) is not compatible with \\`.generate()\\`, as it doesn't have a language model head.`\n\n            const modelType = this.config.model_type;\n            const possibleInfo =\n                MODEL_WITH_LM_HEAD_MAPPING_NAMES.get(modelType)\n                ?? MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES.get(modelType)\n                ?? MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES.get(modelType)\n                // ?? MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES.get(modelType) // TODO\n                ?? MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES.get(modelType);\n\n            if (possibleInfo) {\n                // TODO: support multiple possible classes\n                errorMessage += ` Please use the following class instead: '${possibleInfo[0]}'`;\n            }\n            throw Error(errorMessage);\n        }\n\n        if (!(inputs instanceof Tensor) && !isTypedArray(inputs) && !Array.isArray(inputs)) {\n            throw Error(`\\`inputs\\` must be a Tensor, TypedArray, or Array, but is \"${inputs.constructor.name}\".`);\n        }\n\n        let input_ids_seq_length;\n\n        // Prepare `input_ids` which will be used for auto-regressive generation\n        // TODO: Update to align with HF transformers' implementation\n        if (this.config.is_encoder_decoder) {\n            // Generating from the encoder outputs\n            input_ids_seq_length = 0;\n\n        } else {\n            input_ids_seq_length = inputs instanceof Tensor ? inputs.dims.at(-1) : inputs.length;\n\n            // decoder-only\n            if (input_ids_seq_length === 0) {\n                throw Error(\"Must supply a non-empty array of input token ids.\")\n            }\n        }\n\n        // Update generation config with defaults\n        generation_config = this._get_generation_config(generation_config);\n\n        logits_processor = logits_processor ?? new LogitsProcessorList()\n\n        // Update logits processor\n        logits_processor = this._get_logits_processor(\n            generation_config,\n            input_ids_seq_length,\n            logits_processor\n        )\n\n        /** @type {number[]} */\n        let eos_token_ids = generation_config.eos_token_id;\n        if (eos_token_ids !== null && !Array.isArray(eos_token_ids)) {\n            eos_token_ids = [eos_token_ids];\n        }\n\n        // TODO implement early_stopping\n        // https://huggingface.co/blog/how-to-generate\n\n        let numOutputTokens = 1;\n        const maxOutputTokens = numOutputTokens + (generation_config.max_new_tokens ?? Infinity);\n\n        // Only use max length if max_new_tokens is not provided\n        const useMaxLength = Number.isInteger(generation_config.max_length) && (generation_config.max_new_tokens ?? null) === null;\n        let sampler = Sampler.getSampler(generation_config);\n\n        // @ts-ignore\n        let beams = this.getStartBeams(inputs, generation_config, numOutputTokens, inputs_attention_mask);\n\n        while (beams.some(x => !x.done) && numOutputTokens < maxOutputTokens) {\n            let newest_beams = [];\n            for (let beam of beams) {\n                if (beam.done) {\n                    // Add this beam back into the pool\n                    newest_beams.push(beam);\n                    continue\n                }\n                if (useMaxLength && beam.output_token_ids.length >= generation_config.max_length) {\n                    // Set this beam to done and add it back into the pool\n                    beam.done = true;\n                    newest_beams.push(beam);\n                    continue\n                }\n\n                // @ts-ignore\n                let output = await this.runBeam(beam);\n\n                // add attentions/scores to beam only if user requested\n                if (generation_config.output_attentions) {\n                    this.addAttentionsToBeam(beam, output);\n                }\n                if (generation_config.output_scores) {\n                    // TODO add\n                }\n\n                // Logits are of the form [batch_size, out_seq_length, vocab_size]\n                // In most cases, this will be [batch_size, 1, vocab_size]\n                // So, we select the last token's logits:\n                // (equivalent to `logits = outputs.logits[:, -1, :]`)\n                let logits = output.logits.slice(null, -1, null);\n\n                // Apply logits processor\n                logits_processor(beam.output_token_ids, logits);\n\n                let sampledTokens = sampler(logits);\n                for (let [newTokenId, logProb] of sampledTokens) {\n                    // use previous beam as a starting point\n                    let newBeam = { ...beam };\n\n                    // update new beam\n                    // @ts-ignore\n                    this.updateBeam(newBeam, newTokenId);\n\n                    newBeam.score += logProb;\n\n                    if (eos_token_ids && eos_token_ids.includes(newTokenId)) {\n                        newBeam.done = true;\n                    }\n\n                    newest_beams.push(newBeam);\n                }\n            }\n            ++numOutputTokens;\n\n            // Next, we get the best beams, per ID\n            newest_beams = this.groupBeams(newest_beams).map(\n                group => group\n                    .sort((a, b) => b.score - a.score)      // sort by score\n                    .slice(0, generation_config.num_beams)  // remove outside beam width\n            );\n\n            // Flatten beams\n            beams = newest_beams.flat();\n\n            // Run callback\n            if (generation_config.callback_function) {\n                generation_config.callback_function(beams);\n            }\n        }\n\n        // TODO: Ensure that we can return non-batched outputs\n\n        const groupedBeams = this.groupBeams(beams);\n\n        const getFlattened = (key) => groupedBeams.map(\n            batch => {\n                if (generation_config.num_return_sequences > 1) {\n                    return batch.slice(0, generation_config.num_return_sequences).map(x => x[key]);\n                } else {\n                    return [batch[0][key]];\n                }\n            }\n        ).flat(); // Flatten across batches (depth=1)\n\n        const sequences = getFlattened('output_token_ids'); // [1, seqLength]\n\n        if (generation_config.return_dict_in_generate) {\n            // NOTE: `decoder_attentions` and `cross_attentions` should be:\n            //    list (one element for each generated token)\n            //    of list (one element for each layer of the decoder)\n            //    of torch.FloatTensor of shape (batch_size, num_heads, generated_length, sequence_length)\n            // However, since we are only generating one batch at a time, they are of the form:\n            //   list (batches)\n            //   of list (one element for each generated token)\n            //   of list (one element for each layer of the decoder)\n            //   of torch.FloatTensor of shape (1, num_heads, generated_length, sequence_length)\n            // \n            // TODO: In future (when true parallelism, we should be able to return the correct shape)\n\n            const decoder_attentions = getFlattened('decoder_attentions');\n            const cross_attentions = getFlattened('cross_attentions');\n\n            return {\n                sequences,\n\n                decoder_attentions,\n                cross_attentions,\n            }\n        } else {\n            return sequences;\n        }\n    }\n\n    /**\n     * Helper function to add attentions to beam\n     * @param {Object} beam \n     * @param {Object} output\n     * @private \n     */\n    addAttentionsToBeam(beam, output) {\n        if (this.config.is_encoder_decoder) {\n            if (!output.cross_attentions || output.cross_attentions.length === 0) {\n                throw Error(\n                    \"`output_attentions` is true, but the model did not produce cross-attentions. \" +\n                    \"This is most likely because the model was not exported with `output_attentions=True`.\"\n                )\n            }\n            if (!beam.cross_attentions) {\n                beam.cross_attentions = [];\n            }\n            beam.cross_attentions.push(output.cross_attentions);\n        }\n\n        if (!output.decoder_attentions || output.decoder_attentions.length === 0) {\n            throw Error(\n                \"`output_attentions` is true, but the model did not produce decoder-attentions. \" +\n                \"This is most likely because the model was not exported with `output_attentions=True`.\"\n            )\n        }\n        if (!beam.decoder_attentions) {\n            beam.decoder_attentions = [];\n        }\n        beam.decoder_attentions.push(output.decoder_attentions);\n    }\n\n    /**\n     * Groups an array of beam objects by their ids.\n     *\n     * @param {Array} beams The array of beam objects to group.\n     * @returns {Array} An array of arrays, where each inner array contains beam objects with the same id.\n     */\n    groupBeams(beams) {\n        // Group beams by their ids\n        const groups = Object.create(null);\n        for (const obj of beams) {\n            if (groups[obj.id] === undefined) {\n                groups[obj.id] = [obj];\n            } else {\n                groups[obj.id].push(obj);\n            }\n        }\n\n        return Object.values(groups);\n    }\n\n    /**\n     * Returns an object containing past key values from the given decoder results object.\n     *\n     * @param {Object} decoderResults The decoder results object.\n     * @param {Object} pastKeyValues The previous past key values.\n     * @returns {Object} An object containing past key values.\n     */\n    getPastKeyValues(decoderResults, pastKeyValues) {\n\n        const pkvs = Object.create(null);\n\n        for (const name in decoderResults) {\n            if (name.startsWith('present')) {\n                let newName = name.replace('present', 'past_key_values');\n\n                if (pastKeyValues && name.includes('encoder')) {\n                    // Optimization introduced by optimum to reuse past key values. So, we just replace the constant\n                    // outputs with the previous past key values.\n                    // https://github.com/huggingface/optimum/blob/0bf2c05fb7e1182b52d21b703cfc95fd9e4ea3dc/optimum/onnxruntime/base.py#L677-L704\n                    pkvs[newName] = pastKeyValues[newName];\n                } else {\n                    pkvs[newName] = decoderResults[name];\n                }\n            }\n        }\n        return pkvs;\n    }\n\n    /**\n     * Returns an object containing attentions from the given decoder results object.\n     *\n     * @param {Object} decoderResults The decoder results object.\n     * @returns {Object} An object containing attentions.\n     */\n    getAttentions(decoderResults) {\n        const attns = Object.create(null);\n\n        for (const attnName of ['cross_attentions', 'decoder_attentions']) {\n            const result = [];\n            for (const name in decoderResults) {\n                if (name.startsWith(attnName)) {\n                    const index = name.split('.').pop()\n                    result[index] = decoderResults[name];\n                }\n            }\n            attns[attnName] = result;\n        }\n        return attns;\n    }\n\n    /**\n     * Adds past key values to the decoder feeds object. If pastKeyValues is null, creates new tensors for past key values.\n     *\n     * @param {Object} decoderFeeds The decoder feeds object to add past key values to.\n     * @param {Object} pastKeyValues An object containing past key values.\n     */\n    addPastKeyValues(decoderFeeds, pastKeyValues) {\n        if (pastKeyValues) {\n            Object.assign(decoderFeeds, pastKeyValues)\n        } else {\n            // TODO support batches (i.e., batch_size > 1)\n            const batch_size = 1;\n\n            // @ts-ignore\n            if (this.config.is_encoder_decoder && (this.add_encoder_pkv ?? true)) {\n                // @ts-ignore\n                let encoder_dims = [batch_size, this.num_encoder_heads, 0, this.encoder_dim_kv];\n                // @ts-ignore\n                let decoder_dims = [batch_size, this.num_decoder_heads, 0, this.decoder_dim_kv];\n                // @ts-ignore\n                for (let i = 0; i < this.num_decoder_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.encoder.key`] = new Tensor('float32', [], encoder_dims)\n                    decoderFeeds[`past_key_values.${i}.encoder.value`] = new Tensor('float32', [], encoder_dims)\n                    decoderFeeds[`past_key_values.${i}.decoder.key`] = new Tensor('float32', [], decoder_dims)\n                    decoderFeeds[`past_key_values.${i}.decoder.value`] = new Tensor('float32', [], decoder_dims)\n                }\n            } else if (this.config.model_type === 'falcon') {\n                // NOTE: Custom implementation for Falcon\n                // @ts-ignore\n                let dims = [batch_size * this.num_heads, 0, this.dim_kv]\n                // @ts-ignore\n                for (let i = 0; i < this.num_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.key`] = new Tensor('float32', [], dims)\n                    decoderFeeds[`past_key_values.${i}.value`] = new Tensor('float32', [], dims)\n                }\n            } else if (this.config.multi_query) { // e.g., for `gpt_bigcode`\n                // @ts-ignore\n                let dims = [batch_size * this.num_heads, 0, 2 * this.dim_kv]\n                // @ts-ignore\n                for (let i = 0; i < this.num_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.key_value`] = new Tensor('float32', [], dims)\n                }\n            } else if (this.config.model_type === 'bloom') {\n                // NOTE: Custom implementation for Bloom\n\n                // @ts-ignore\n                let keyDims = [batch_size * this.num_heads, this.dim_kv, 0] // [batch_size x num_heads,64,past_sequence_length]\n                // @ts-ignore\n                let valueDims = [batch_size * this.num_heads, 0, this.dim_kv] // [batch_size x num_heads,past_sequence_length,64]\n                // @ts-ignore\n                for (let i = 0; i < this.num_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.key`] = new Tensor('float32', [], keyDims)\n                    decoderFeeds[`past_key_values.${i}.value`] = new Tensor('float32', [], valueDims)\n                }\n            } else { // Decoder-only\n                // @ts-ignore\n                let dims = [batch_size, this.num_heads, 0, this.dim_kv]\n                // @ts-ignore\n                for (let i = 0; i < this.num_layers; ++i) {\n                    decoderFeeds[`past_key_values.${i}.key`] = new Tensor('float32', [], dims)\n                    decoderFeeds[`past_key_values.${i}.value`] = new Tensor('float32', [], dims)\n                }\n            }\n        }\n    }\n\n    /**\n     * Initializes and returns the beam for text generation task\n     * @param {Tensor} inputTokenIds The input token ids.\n     * @param {Object} generation_config The generation config.\n     * @param {number} numOutputTokens The number of tokens to be generated.\n     * @param {Tensor} inputs_attention_mask Optional input attention mask.\n     * @returns {any} A Beam object representing the initialized beam.\n     * @private\n     */\n    getStartBeams(inputTokenIds, generation_config, numOutputTokens, inputs_attention_mask) {\n        return this._getStartBeams(this, inputTokenIds, generation_config, numOutputTokens, inputs_attention_mask)\n    }\n\n    /**\n     * Runs a single step of the beam search generation algorithm.\n     * @param {any} beam The current beam being generated.\n     * @returns {Promise<any>} The updated beam after a single generation step.\n     * @private\n     */\n    async runBeam(beam) {\n        return await this._runBeam(this, beam);\n    }\n\n    /**\n     * Update a beam with a new token ID.\n     * @param {Object} beam The beam to update.\n     * @param {number} newTokenId The new token ID to add to the beam's output.\n     * @private\n     */\n    updateBeam(beam, newTokenId) {\n        return this._updateBeam(beam, newTokenId);\n    }\n}\n\n//////////////////////////////////////////////////\n// Base model output class\nexport class ModelOutput { }\n\n/**\n * Base class for model's outputs, with potential hidden states and attentions.\n */\nexport class BaseModelOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.last_hidden_state Sequence of hidden-states at the output of the last layer of the model.\n     * @param {Tensor} [output.hidden_states] Hidden-states of the model at the output of each layer plus the optional initial embedding outputs.\n     * @param {Tensor} [output.attentions] Attentions weights after the attention softmax, used to compute the weighted average in the self-attention heads.\n     */\n    constructor({ last_hidden_state, hidden_states = null, attentions = null }) {\n        super();\n        this.last_hidden_state = last_hidden_state;\n        this.hidden_states = hidden_states;\n        this.attentions = attentions;\n    }\n}\n//////////////////////////////////////////////////\n// Bert models\nexport class BertPreTrainedModel extends PreTrainedModel { }\nexport class BertModel extends BertPreTrainedModel { }\n\n/**\n * BertForMaskedLM is a class representing a BERT model for masked language modeling.\n */\nexport class BertForMaskedLM extends BertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * BertForSequenceClassification is a class representing a BERT model for sequence classification.\n */\nexport class BertForSequenceClassification extends BertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * BertForTokenClassification is a class representing a BERT model for token classification.\n */\nexport class BertForTokenClassification extends BertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * BertForQuestionAnswering is a class representing a BERT model for question answering.\n */\nexport class BertForQuestionAnswering extends BertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// ConvBert models\nexport class ConvBertPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare ConvBERT Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class ConvBertModel extends ConvBertPreTrainedModel { }\n\n/**\n * ConvBERT Model with a language modeling head on top.\n */\nexport class ConvBertForMaskedLM extends ConvBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * ConvBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class ConvBertForSequenceClassification extends ConvBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * ConvBERT Model with a token classification head on top (a linear layer on top of the hidden-states output)\n * e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class ConvBertForTokenClassification extends ConvBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * ConvBERT Model with a span classification head on top for extractive question-answering tasks like SQuAD\n * (a linear layers on top of the hidden-states output to compute `span start logits` and `span end logits`)\n */\nexport class ConvBertForQuestionAnswering extends ConvBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Electra models\nexport class ElectraPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare Electra Model transformer outputting raw hidden-states without any specific head on top.\n * Identical to the BERT model except that it uses an additional linear layer between the embedding\n * layer and the encoder if the hidden size and embedding size are different.\n */\nexport class ElectraModel extends ElectraPreTrainedModel { }\n// TODO add ElectraForPreTraining\n/**\n * Electra model with a language modeling head on top.\n */\nexport class ElectraForMaskedLM extends ElectraPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * ELECTRA Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class ElectraForSequenceClassification extends ElectraPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * Electra model with a token classification head on top.\n */\nexport class ElectraForTokenClassification extends ElectraPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * LECTRA Model with a span classification head on top for extractive question-answering tasks like SQuAD\n * (a linear layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n */\nexport class ElectraForQuestionAnswering extends ElectraPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// CamemBERT models\nexport class CamembertPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare CamemBERT Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class CamembertModel extends CamembertPreTrainedModel { }\n\n/**\n * CamemBERT Model with a `language modeling` head on top.\n */\nexport class CamembertForMaskedLM extends CamembertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * CamemBERT Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output) e.g. for GLUE tasks.\n */\nexport class CamembertForSequenceClassification extends CamembertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * CamemBERT Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class CamembertForTokenClassification extends CamembertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * CamemBERT Model with a span classification head on top for extractive question-answering tasks\n */\nexport class CamembertForQuestionAnswering extends CamembertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// DeBERTa models\nexport class DebertaPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare DeBERTa Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class DebertaModel extends DebertaPreTrainedModel { }\n\n/**\n * DeBERTa Model with a `language modeling` head on top.\n */\nexport class DebertaForMaskedLM extends DebertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class DebertaForSequenceClassification extends DebertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class DebertaForTokenClassification extends DebertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n * layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n */\nexport class DebertaForQuestionAnswering extends DebertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// DeBERTa-v2 models\nexport class DebertaV2PreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare DeBERTa-V2 Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class DebertaV2Model extends DebertaV2PreTrainedModel { }\n\n/**\n * DeBERTa-V2 Model with a `language modeling` head on top.\n */\nexport class DebertaV2ForMaskedLM extends DebertaV2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa-V2 Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class DebertaV2ForSequenceClassification extends DebertaV2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa-V2 Model with a token classification head on top (a linear layer on top of the hidden-states output) e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class DebertaV2ForTokenClassification extends DebertaV2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DeBERTa-V2 Model with a span classification head on top for extractive question-answering tasks like SQuAD (a linear\n * layers on top of the hidden-states output to compute `span start logits` and `span end logits`).\n */\nexport class DebertaV2ForQuestionAnswering extends DebertaV2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// DistilBert models\nexport class DistilBertPreTrainedModel extends PreTrainedModel { }\nexport class DistilBertModel extends DistilBertPreTrainedModel { }\n\n/**\n * DistilBertForSequenceClassification is a class representing a DistilBERT model for sequence classification.\n */\nexport class DistilBertForSequenceClassification extends DistilBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DistilBertForTokenClassification is a class representing a DistilBERT model for token classification.\n */\nexport class DistilBertForTokenClassification extends DistilBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n\n/**\n * DistilBertForQuestionAnswering is a class representing a DistilBERT model for question answering.\n */\nexport class DistilBertForQuestionAnswering extends DistilBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * DistilBertForMaskedLM is a class representing a DistilBERT model for masking task.\n */\nexport class DistilBertForMaskedLM extends DistilBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// ESM models\nexport class EsmPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare ESM Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class EsmModel extends EsmPreTrainedModel { }\n\n/**\n * ESM Model with a `language modeling` head on top.\n */\nexport class EsmForMaskedLM extends EsmPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * ESM Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class EsmForSequenceClassification extends EsmPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * ESM Model with a token classification head on top (a linear layer on top of the hidden-states output)\n * e.g. for Named-Entity-Recognition (NER) tasks.\n */\nexport class EsmForTokenClassification extends EsmPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// MobileBert models\nexport class MobileBertPreTrainedModel extends PreTrainedModel { }\nexport class MobileBertModel extends MobileBertPreTrainedModel { }\n\n/**\n * MobileBertForMaskedLM is a class representing a MobileBERT model for masking task.\n */\nexport class MobileBertForMaskedLM extends MobileBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MobileBert Model transformer with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class MobileBertForSequenceClassification extends MobileBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MobileBert Model with a span classification head on top for extractive question-answering tasks\n */\nexport class MobileBertForQuestionAnswering extends MobileBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MPNet models\nexport class MPNetPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare MPNet Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class MPNetModel extends MPNetPreTrainedModel { }\n\n/**\n * MPNetForMaskedLM is a class representing a MPNet model for masked language modeling.\n */\nexport class MPNetForMaskedLM extends MPNetPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} An object containing the model's output logits for masked language modeling.\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MPNetForSequenceClassification is a class representing a MPNet model for sequence classification.\n */\nexport class MPNetForSequenceClassification extends MPNetPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MPNetForTokenClassification is a class representing a MPNet model for token classification.\n */\nexport class MPNetForTokenClassification extends MPNetPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * MPNetForQuestionAnswering is a class representing a MPNet model for question answering.\n */\nexport class MPNetForQuestionAnswering extends MPNetPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} An object containing the model's output logits for question answering.\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// SqueezeBert models\nexport class SqueezeBertPreTrainedModel extends PreTrainedModel { }\nexport class SqueezeBertModel extends SqueezeBertPreTrainedModel { }\nexport class SqueezeBertForMaskedLM extends SqueezeBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\nexport class SqueezeBertForSequenceClassification extends SqueezeBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\nexport class SqueezeBertForQuestionAnswering extends SqueezeBertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Albert models\nexport class AlbertPreTrainedModel extends PreTrainedModel { }\nexport class AlbertModel extends AlbertPreTrainedModel { }\nexport class AlbertForSequenceClassification extends AlbertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\nexport class AlbertForQuestionAnswering extends AlbertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\nexport class AlbertForMaskedLM extends AlbertPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// T5 models\nexport class T5PreTrainedModel extends PreTrainedModel { };\n\nexport class T5Model extends T5PreTrainedModel { }\n\n/**\n * T5Model is a class representing a T5 model for conditional generation.\n */\nexport class T5ForConditionalGeneration extends T5PreTrainedModel {\n\n    /**\n     * Creates a new instance of the `T5ForConditionalGeneration` class.\n     * @param {Object} config The model configuration.\n     * @param {any} session session for the model.\n     * @param {any} decoder_merged_session session for the decoder.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.num_decoder_layers;\n        this.num_decoder_heads = this.config.num_heads;\n        this.decoder_dim_kv = this.config.d_kv;\n\n        this.num_encoder_layers = this.config.num_layers;\n        this.num_encoder_heads = this.config.num_heads;\n        this.encoder_dim_kv = this.config.d_kv;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// LONGT5 models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class LongT5PreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare LONGT5 Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class LongT5Model extends LongT5PreTrainedModel { }\n\n/**\n * LONGT5 Model with a `language modeling` head on top.\n */\nexport class LongT5ForConditionalGeneration extends LongT5PreTrainedModel {\n    /**\n     * Creates a new instance of the `LongT5ForConditionalGeneration` class.\n     * @param {Object} config The model configuration.\n     * @param {any} session session for the model.\n     * @param {any} decoder_merged_session session for the decoder.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.num_decoder_layers;\n        this.num_decoder_heads = this.config.num_heads;\n        this.decoder_dim_kv = this.config.d_kv;\n\n        this.num_encoder_layers = this.config.num_layers;\n        this.num_encoder_heads = this.config.num_heads;\n        this.encoder_dim_kv = this.config.d_kv;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// MT5 models\nexport class MT5PreTrainedModel extends PreTrainedModel { };\n\nexport class MT5Model extends MT5PreTrainedModel { }\n\n/**\n * A class representing a conditional sequence-to-sequence model based on the MT5 architecture.\n */\nexport class MT5ForConditionalGeneration extends MT5PreTrainedModel {\n\n    /**\n     * Creates a new instance of the `MT5ForConditionalGeneration` class.\n     * @param {any} config The model configuration.\n     * @param {any} session The ONNX session containing the encoder weights.\n     * @param {any} decoder_merged_session The ONNX session containing the merged decoder weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.num_decoder_layers;\n        this.num_decoder_heads = this.config.num_heads;\n        this.decoder_dim_kv = this.config.d_kv;\n\n        this.num_encoder_layers = this.config.num_layers;\n        this.num_encoder_heads = this.config.num_heads;\n        this.encoder_dim_kv = this.config.d_kv;\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Bart models\nexport class BartPretrainedModel extends PreTrainedModel { };\n\n/**\n * The bare BART Model outputting raw hidden-states without any specific head on top.\n */\nexport class BartModel extends BartPretrainedModel { }\n\n/**\n * The BART Model with a language modeling head. Can be used for summarization.\n */\nexport class BartForConditionalGeneration extends BartPretrainedModel {\n\n    /**\n     * Creates a new instance of the `BartForConditionalGeneration` class.\n     * @param {Object} config The configuration object for the Bart model.\n     * @param {Object} session The ONNX session used to execute the model.\n     * @param {Object} decoder_merged_session The ONNX session used to execute the decoder.\n     * @param {Object} generation_config The generation configuration object.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n\n}\n\n/**\n * Bart model with a sequence classification/head on top (a linear layer on top of the pooled output)\n */\nexport class BartForSequenceClassification extends BartPretrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MBart models\nexport class MBartPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare MBART Model outputting raw hidden-states without any specific head on top.\n */\nexport class MBartModel extends MBartPreTrainedModel { }\n\n/**\n * The MBART Model with a language modeling head. Can be used for summarization, after fine-tuning the pretrained models.\n */\nexport class MBartForConditionalGeneration extends MBartPreTrainedModel {\n\n    /**\n     * Creates a new instance of the `MBartForConditionalGeneration` class.\n     * @param {Object} config The configuration object for the Bart model.\n     * @param {Object} session The ONNX session used to execute the model.\n     * @param {Object} decoder_merged_session The ONNX session used to execute the decoder.\n     * @param {Object} generation_config The generation configuration object.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n\n}\n\n/**\n * MBart model with a sequence classification/head on top (a linear layer on top of the pooled output).\n */\nexport class MBartForSequenceClassification extends MBartPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n\nexport class MBartForCausalLM extends MBartPreTrainedModel {\n    /**\n     * Creates a new instance of the `MBartForCausalLM` class.\n     * @param {Object} config Configuration object for the model.\n     * @param {Object} decoder_merged_session ONNX Session object for the decoder.\n     * @param {Object} generation_config Configuration object for the generation process.\n     */\n    constructor(config, decoder_merged_session, generation_config) {\n        super(config, decoder_merged_session);\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Blenderbot models\nexport class BlenderbotPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare Blenderbot Model outputting raw hidden-states without any specific head on top.\n */\nexport class BlenderbotModel extends BlenderbotPreTrainedModel { }\n\n/**\n * The Blenderbot Model with a language modeling head. Can be used for summarization.\n */\nexport class BlenderbotForConditionalGeneration extends BlenderbotPreTrainedModel {\n\n    /**\n     * Creates a new instance of the `BlenderbotForConditionalGeneration` class.\n     * @param {any} config The model configuration.\n     * @param {any} session The ONNX session containing the encoder weights.\n     * @param {any} decoder_merged_session The ONNX session containing the merged decoder weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Blenderbot models\nexport class BlenderbotSmallPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare BlenderbotSmall Model outputting raw hidden-states without any specific head on top.\n */\nexport class BlenderbotSmallModel extends BlenderbotSmallPreTrainedModel { }\n\n/**\n * The BlenderbotSmall Model with a language modeling head. Can be used for summarization.\n */\nexport class BlenderbotSmallForConditionalGeneration extends BlenderbotSmallPreTrainedModel {\n\n    /**\n     * Creates a new instance of the `BlenderbotForConditionalGeneration` class.\n     * @param {any} config The model configuration.\n     * @param {any} session The ONNX session containing the encoder weights.\n     * @param {any} decoder_merged_session The ONNX session containing the merged decoder weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Roberta models\nexport class RobertaPreTrainedModel extends PreTrainedModel { }\nexport class RobertaModel extends RobertaPreTrainedModel { }\n\n/**\n * RobertaForMaskedLM class for performing masked language modeling on Roberta models.\n */\nexport class RobertaForMaskedLM extends RobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * RobertaForSequenceClassification class for performing sequence classification on Roberta models.\n */\nexport class RobertaForSequenceClassification extends RobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * RobertaForTokenClassification class for performing token classification on Roberta models.\n */\nexport class RobertaForTokenClassification extends RobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * RobertaForQuestionAnswering class for performing question answering on Roberta models.\n */\nexport class RobertaForQuestionAnswering extends RobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// XLM models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class XLMPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare XLM Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class XLMModel extends XLMPreTrainedModel { }\n\n/**\n * The XLM Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class XLMWithLMHeadModel extends XLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLM Model with a sequence classification/regression head on top (a linear layer on top of the pooled output)\n */\nexport class XLMForSequenceClassification extends XLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLM Model with a token classification head on top (a linear layer on top of the hidden-states output)\n */\nexport class XLMForTokenClassification extends XLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLM Model with a span classification head on top for extractive question-answering tasks\n */\nexport class XLMForQuestionAnswering extends XLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// XLMRoberta models\nexport class XLMRobertaPreTrainedModel extends PreTrainedModel { }\nexport class XLMRobertaModel extends XLMRobertaPreTrainedModel { }\n\n/**\n * XLMRobertaForMaskedLM class for performing masked language modeling on XLMRoberta models.\n */\nexport class XLMRobertaForMaskedLM extends XLMRobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<MaskedLMOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new MaskedLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLMRobertaForSequenceClassification class for performing sequence classification on XLMRoberta models.\n */\nexport class XLMRobertaForSequenceClassification extends XLMRobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLMRobertaForTokenClassification class for performing token classification on XLMRoberta models.\n */\nexport class XLMRobertaForTokenClassification extends XLMRobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<TokenClassifierOutput>} An object containing the model's output logits for token classification.\n     */\n    async _call(model_inputs) {\n        return new TokenClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * XLMRobertaForQuestionAnswering class for performing question answering on XLMRoberta models.\n */\nexport class XLMRobertaForQuestionAnswering extends XLMRobertaPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     *\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<QuestionAnsweringModelOutput>} returned object\n     */\n    async _call(model_inputs) {\n        return new QuestionAnsweringModelOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Audio Spectrogram Transformer (AST) models\nexport class ASTPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare AST Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class ASTModel extends ASTPreTrainedModel { }\n\n/**\n * Audio Spectrogram Transformer model with an audio classification head on top\n * (a linear layer on top of the pooled output) e.g. for datasets like AudioSet, Speech Commands v2.\n */\nexport class ASTForAudioClassification extends ASTPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Whisper models\nexport class WhisperPreTrainedModel extends PreTrainedModel { };\n\n/**\n * WhisperModel class for training Whisper models without a language model head.\n */\nexport class WhisperModel extends WhisperPreTrainedModel { }\n\n/**\n * WhisperForConditionalGeneration class for generating conditional outputs from Whisper models.\n */\nexport class WhisperForConditionalGeneration extends WhisperPreTrainedModel {\n\n    requires_attention_mask = false;\n    main_input_name = 'input_features';\n\n    /**\n     * Creates a new instance of the `WhisperForConditionalGeneration` class.\n     * @param {Object} config Configuration object for the model.\n     * @param {Object} session ONNX Session object for the model.\n     * @param {Object} decoder_merged_session ONNX Session object for the decoder.\n     * @param {Object} generation_config Configuration object for the generation process.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n\n    /**\n     * @typedef {Object} WhisperGenerationConfig\n     * @extends GenerationConfig\n     * @property {boolean} [return_timestamps=null] Whether to return the timestamps with the text. This enables the `WhisperTimestampsLogitsProcessor`.\n     * @property {boolean} [return_token_timestamps=null] Whether to return token-level timestamps\n     * with the text. This can be used with or without the `return_timestamps` option. To get word-level\n     * timestamps, use the tokenizer to group the tokens into words.\n     * @property {number} [num_frames=null]  The number of audio frames available in this chunk. This is only used generating word-level timestamps.\n     */\n\n    /**\n     * Generates outputs based on input and generation configuration.\n     * @param {Object} inputs Input data for the model.\n     * @param {WhisperGenerationConfig} generation_config Configuration object for the generation process.\n     * @param {Object} logits_processor Optional logits processor object.\n     * @returns {Promise<Object>} Promise object represents the generated outputs.\n     */\n    async generate(\n        inputs,\n        generation_config = null,\n        logits_processor = null,\n        // {\n        //     return_timestamps = null,\n        //     return_token_timestamps = null,\n        //     language = null,\n        //     task = null,\n        // } = {},\n    ) {\n        // Create generation config object\n        generation_config = this._get_generation_config(generation_config);\n\n\n        // Whisper has additional options for returning timestamps\n        generation_config.return_timestamps ??= false;\n\n        // TODO add language and task\n\n        if (generation_config.return_timestamps) {\n            logits_processor = [new WhisperTimeStampLogitsProcessor(generation_config)]\n        }\n\n        if (generation_config.return_token_timestamps) {\n            generation_config.output_attentions = true;\n            generation_config.return_dict_in_generate = true;\n\n            if (generation_config.task === 'translate') {\n                console.warn(\"Token-level timestamps may not be reliable for task 'translate'.\")\n            }\n\n            if (!generation_config.alignment_heads) {\n                throw new Error(\n                    \"Model generation config has no `alignment_heads`, token-level timestamps not available. \" +\n                    \"See https://gist.github.com/hollance/42e32852f24243b748ae6bc1f985b13a on how to add this property to the generation config.\"\n                )\n            }\n        }\n\n        const outputs = await super.generate(inputs, generation_config, logits_processor);\n\n        if (generation_config.return_token_timestamps && generation_config.alignment_heads) {\n            outputs[\"token_timestamps\"] = this._extract_token_timestamps(\n                outputs,\n                generation_config.alignment_heads,\n                generation_config.num_frames,\n            )\n        }\n\n        return outputs\n    }\n\n    /**\n     * Calculates token-level timestamps using the encoder-decoder cross-attentions and\n     * dynamic time-warping (DTW) to map each output token to a position in the input audio.\n     * @param {Object} generate_outputs Outputs generated by the model\n     * @param {Tensor[][][]} generate_outputs.cross_attentions The cross attentions output by the model\n     * @param {Tensor[][][]} generate_outputs.decoder_attentions The decoder attentions output by the model\n     * @param {number[][]} generate_outputs.sequences The sequences output by the model\n     * @param {number[][]} alignment_heads Alignment heads of the model\n     * @param {number} [num_frames=null] Number of frames in the input audio.\n     * @param {number} [time_precision=0.02] Precision of the timestamps in seconds\n     * @returns {Tensor} tensor containing the timestamps in seconds for each predicted token\n     */\n    _extract_token_timestamps(generate_outputs, alignment_heads, num_frames = null, time_precision = 0.02) {\n        if (!generate_outputs.cross_attentions) {\n            throw new Error(\n                \"Model outputs must contain cross attentions to extract timestamps. \" +\n                \"This is most likely because the model was not exported with `output_attentions=True`.\"\n            )\n        }\n\n        let median_filter_width = this.config.median_filter_width;\n        if (median_filter_width === undefined) {\n            console.warn(\"Model config has no `median_filter_width`, using default value of 7.\")\n            median_filter_width = 7;\n        }\n\n        const batchedMatrices = generate_outputs.cross_attentions.map(batch => {\n            // Create a list with `decoder_layers` elements, each a tensor of shape\n            // (batch size, attention_heads, output length, input length).\n            let cross_attentions = Array.from({ length: this.config.decoder_layers },\n                (_, i) => cat(batch.map(x => x[i]), 2)\n            );\n\n            let weights = stack(alignment_heads.map(([l, h]) => {\n                return num_frames\n                    ? cross_attentions[l].slice(null, h, null, [0, num_frames])\n                    : cross_attentions[l].slice(null, h);\n            }));\n            weights = weights.transpose(1, 0, 2, 3)\n\n            let [std, calculatedMean] = std_mean(weights, -2, 0, true);\n\n            // Normalize and smoothen the weights.\n            let smoothedWeights = weights.clone(); // [1, 8, seqLength, 1500]\n\n            for (let a = 0; a < smoothedWeights.dims[0]; ++a) {\n                let aTensor = smoothedWeights[a]; // [8, seqLength, 1500]\n\n                for (let b = 0; b < aTensor.dims[0]; ++b) {\n                    let bTensor = aTensor[b]; // [seqLength, 1500]\n\n                    const stdTensor = std[a][b][0]; // [1500]\n                    const meanTensor = calculatedMean[a][b][0]; // [1500]\n\n                    for (let c = 0; c < bTensor.dims[0]; ++c) {\n\n                        let cTensor = bTensor[c]; // [1500]\n                        for (let d = 0; d < cTensor.data.length; ++d) {\n                            cTensor.data[d] = (cTensor.data[d] - meanTensor.data[d]) / stdTensor.data[d]\n                        }\n\n                        // Apply median filter.\n                        cTensor.data.set(medianFilter(cTensor.data, median_filter_width))\n                    }\n                }\n            }\n\n            // Average the different cross-attention heads.\n            const matrix = mean(smoothedWeights, 1);\n            return matrix;\n        });\n\n        const timestampsShape = [generate_outputs.sequences.length, generate_outputs.sequences[0].length];\n\n        const timestamps = new Tensor(\n            'float32',\n            new Float32Array(timestampsShape[0] * timestampsShape[1]),\n            timestampsShape\n        );\n\n        // Perform dynamic time warping on each element of the batch.\n        for (let batch_idx = 0; batch_idx < timestampsShape[0]; ++batch_idx) {\n            // NOTE: Since we run only one batch at a time, we can squeeze to get the same dimensions\n            // as the python implementation\n            const matrix = batchedMatrices[batch_idx].neg().squeeze_(0);\n            let [text_indices, time_indices] = dynamicTimeWarping(matrix);\n\n            let diffs = Array.from({ length: text_indices.length - 1 }, (v, i) => text_indices[i + 1] - text_indices[i]);\n            let jumps = mergeArrays([1], diffs).map(x => !!x); // convert to boolean\n\n            let jump_times = [];\n            for (let i = 0; i < jumps.length; ++i) {\n                if (jumps[i]) {\n                    jump_times.push(time_indices[i] * time_precision);\n                    // NOTE: No point in rounding here, since we set to Float32Array later\n                }\n            }\n            timestamps[batch_idx].data.set(jump_times, 1)\n        }\n\n        return timestamps;\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n/**\n * Vision Encoder-Decoder model based on OpenAI's GPT architecture for image captioning and other vision tasks\n */\nexport class VisionEncoderDecoderModel extends PreTrainedModel {\n    main_input_name = 'pixel_values';\n\n    /**\n     * Creates a new instance of the `VisionEncoderDecoderModel` class.\n     * @param {Object} config The configuration object specifying the hyperparameters and other model settings.\n     * @param {Object} session The ONNX session containing the encoder model.\n     * @param {any} decoder_merged_session The ONNX session containing the merged decoder model.\n     * @param {Object} generation_config Configuration object for the generation process.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        // Extract configs\n        const encoderConfig = this.config.encoder;\n        const decoderConfig = this.config.decoder;\n\n        // Validate encoder\n        const encoderModelType = encoderConfig.model_type;\n        const encoderModel =\n            MODEL_MAPPING_NAMES_ENCODER_ONLY.get(encoderModelType)\n            ?? MODEL_MAPPING_NAMES_ENCODER_DECODER.get(encoderModelType);\n        if (!encoderModel) {\n            console.warn(`Model type for encoder '${encoderModelType}' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose.`);\n        }\n\n        // Validate decoder\n        const decoderModel = MODEL_WITH_LM_HEAD_MAPPING_NAMES.get(decoderConfig.model_type);\n        if (!decoderModel) {\n            throw new Error(`Unable to construct \\`VisionEncoderDecoder\\` due to unsupported decoder: \"${this.config.decoder.model_type}\"`);\n        }\n\n        // @ts-ignore\n        const decoderModelClass = decoderModel[1];\n        // @ts-ignore\n        const decoder = new decoderModelClass(decoderConfig, decoder_merged_session, generation_config);\n\n        this.add_encoder_pkv = 'num_decoder_layers' in decoder;\n        if (this.add_encoder_pkv) {\n            // Decoder is part of an encoder-decoder model\n            this.num_decoder_layers = decoder.num_decoder_layers;\n            this.num_decoder_heads = decoder.num_decoder_heads;\n            this.decoder_dim_kv = decoder.decoder_dim_kv;\n\n            this.num_encoder_layers = decoder.num_encoder_layers;\n            this.num_encoder_heads = decoder.num_encoder_heads;\n            this.encoder_dim_kv = decoder.encoder_dim_kv;\n\n        } else {\n            // Decoder is a decoder-only model\n            this.num_layers = decoder.num_layers;\n            this.num_heads = decoder.num_heads;\n            this.dim_kv = decoder.dim_kv;\n        }\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// CLIP models\nexport class CLIPPreTrainedModel extends PreTrainedModel { }\n\n/**\n * CLIP Text and Vision Model with a projection layers on top\n * \n * **Example:** Perform zero-shot image classification with a `CLIPModel`.\n * \n * ```javascript\n * import { AutoTokenizer, AutoProcessor, CLIPModel, RawImage } from '@xenova/transformers';\n * \n * // Load tokenizer, processor, and model\n * let tokenizer = await AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');\n * let processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');\n * let model = await CLIPModel.from_pretrained('Xenova/clip-vit-base-patch16');\n * \n * // Run tokenization\n * let texts = ['a photo of a car', 'a photo of a football match']\n * let text_inputs = tokenizer(texts, { padding: true, truncation: true });\n * \n * // Read image and run processor\n * let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\n * let image_inputs = await processor(image);\n * \n * // Run model with both text and pixel inputs\n * let output = await model({ ...text_inputs, ...image_inputs });\n * // {\n * //   logits_per_image: Tensor {\n * //     dims: [ 1, 2 ],\n * //     data: Float32Array(2) [ 18.579734802246094, 24.31830596923828 ],\n * //   },\n * //   logits_per_text: Tensor {\n * //     dims: [ 2, 1 ],\n * //     data: Float32Array(2) [ 18.579734802246094, 24.31830596923828 ],\n * //   },\n * //   text_embeds: Tensor {\n * //     dims: [ 2, 512 ],\n * //     data: Float32Array(1024) [ ... ],\n * //   },\n * //   image_embeds: Tensor {\n * //     dims: [ 1, 512 ],\n * //     data: Float32Array(512) [ ... ],\n * //   }\n * // }\n * ```\n */\nexport class CLIPModel extends CLIPPreTrainedModel { }\n\n/**\n * CLIP Text Model with a projection layer on top (a linear layer on top of the pooled output)\n * \n * **Example:** Compute text embeddings with `CLIPTextModelWithProjection`.\n * \n * ```javascript\n * import { AutoTokenizer, CLIPTextModelWithProjection } from '@xenova/transformers';\n * \n * // Load tokenizer and text model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/clip-vit-base-patch16');\n * const text_model = await CLIPTextModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');\n * \n * // Run tokenization\n * let texts = ['a photo of a car', 'a photo of a football match'];\n * let text_inputs = tokenizer(texts, { padding: true, truncation: true });\n * \n * // Compute embeddings\n * const { text_embeds } = await text_model(text_inputs);\n * // Tensor {\n * //   dims: [ 2, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(1024) [ ... ],\n * //   size: 1024\n * // }\n * ```\n */\nexport class CLIPTextModelWithProjection extends CLIPPreTrainedModel {\n\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n        // Update default model file name if not provided\n        options.model_file_name ??= 'text_model';\n        return super.from_pretrained(pretrained_model_name_or_path, options);\n    }\n}\n\n/**\n * CLIP Vision Model with a projection layer on top (a linear layer on top of the pooled output)\n * \n * **Example:** Compute vision embeddings with `CLIPVisionModelWithProjection`.\n * \n * ```javascript\n * import { AutoProcessor, CLIPVisionModelWithProjection, RawImage} from '@xenova/transformers';\n * \n * // Load processor and vision model\n * const processor = await AutoProcessor.from_pretrained('Xenova/clip-vit-base-patch16');\n * const vision_model = await CLIPVisionModelWithProjection.from_pretrained('Xenova/clip-vit-base-patch16');\n * \n * // Read image and run processor\n * let image = await RawImage.read('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/football-match.jpg');\n * let image_inputs = await processor(image);\n * \n * // Compute embeddings\n * const { image_embeds } = await vision_model(image_inputs);\n * // Tensor {\n * //   dims: [ 1, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(512) [ ... ],\n * //   size: 512\n * // }\n * ```\n */\nexport class CLIPVisionModelWithProjection extends CLIPPreTrainedModel {\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n        // Update default model file name if not provided\n        options.model_file_name ??= 'vision_model';\n        return super.from_pretrained(pretrained_model_name_or_path, options);\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// ChineseCLIP models\nexport class ChineseCLIPPreTrainedModel extends PreTrainedModel { }\n\nexport class ChineseCLIPModel extends ChineseCLIPPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// GPT2 models\nexport class GPT2PreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPT2PreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.n_embd / this.num_heads;\n    }\n}\n\nexport class GPT2Model extends GPT2PreTrainedModel { }\n\n/**\n * GPT-2 language model head on top of the GPT-2 base model. This model is suitable for text generation tasks.\n */\nexport class GPT2LMHeadModel extends GPT2PreTrainedModel { }\n// export class GPT2ForSequenceClassification extends GPT2PreTrainedModel {\n// TODO\n// }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// GPTNeo models\nexport class GPTNeoPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPTNeoPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_heads;\n        this.num_layers = this.config.num_layers;\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\nexport class GPTNeoModel extends GPTNeoPreTrainedModel { }\n\nexport class GPTNeoForCausalLM extends GPTNeoPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// GPTNeoX models\nexport class GPTNeoXPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPTNeoXPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_attention_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\nexport class GPTNeoXModel extends GPTNeoXPreTrainedModel { }\n\nexport class GPTNeoXForCausalLM extends GPTNeoXPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// GPT-J models\nexport class GPTJPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPTJPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.n_embd / this.num_heads;\n    }\n}\n\nexport class GPTJModel extends GPTJPreTrainedModel { }\n\nexport class GPTJForCausalLM extends GPTJPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// GPTBigCode models\nexport class GPTBigCodePreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `GPTBigCodePreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.n_embd / this.num_heads;\n    }\n}\n\nexport class GPTBigCodeModel extends GPTBigCodePreTrainedModel { }\n\nexport class GPTBigCodeForCausalLM extends GPTBigCodePreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// CodeGen models\nexport class CodeGenPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `CodeGenPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.n_embd / this.num_heads;\n    }\n}\n/**\n * CodeGenModel is a class representing a code generation model without a language model head.\n */\nexport class CodeGenModel extends CodeGenPreTrainedModel { }\n\n/**\n * CodeGenForCausalLM is a class that represents a code generation model based on the GPT-2 architecture. It extends the `CodeGenPreTrainedModel` class.\n */\nexport class CodeGenForCausalLM extends CodeGenPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// LLama models\n\n/**\n * The bare LLama Model outputting raw hidden-states without any specific head on top.\n */\nexport class LlamaPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `LlamaPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_key_value_heads ?? this.config.num_attention_heads\n        this.num_layers = this.config.num_hidden_layers\n        this.dim_kv = this.config.hidden_size / this.config.num_attention_heads\n    }\n}\n/**\n * The bare LLaMA Model outputting raw hidden-states without any specific head on top.\n */\nexport class LlamaModel extends LlamaPreTrainedModel { }\n\nexport class LlamaForCausalLM extends LlamaPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Phi models\n\nexport class PhiPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `PhiPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id;\n\n        this.num_heads = this.config.num_attention_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\n/**\n * The bare Phi Model outputting raw hidden-states without any specific head on top.\n */\nexport class PhiModel extends PhiPreTrainedModel { }\n\nexport class PhiForCausalLM extends PhiPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Bloom models\n/**\n * The Bloom Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class BloomPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `BloomPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_head\n        this.num_layers = this.config.n_layer\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\n\n/**\n * The bare Bloom Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class BloomModel extends BloomPreTrainedModel { }\n\n/**\n * The Bloom Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class BloomForCausalLM extends BloomPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// MPT models\nexport class MptPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `MptPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.n_heads\n        this.num_layers = this.config.n_layers\n        this.dim_kv = this.config.d_model / this.num_heads;\n    }\n}\n\n/**\n * The bare Mpt Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class MptModel extends MptPreTrainedModel { }\n\n/**\n * The MPT Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class MptForCausalLM extends MptPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// OPT models\nexport class OPTPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `OPTPreTrainedModel` class.\n     * @param {Object} config The model configuration object.\n     * @param {Object} session The ONNX session object.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_attention_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.num_heads;\n    }\n}\n\n/**\n * The bare OPT Model outputting raw hidden-states without any specific head on top.\n */\nexport class OPTModel extends OPTPreTrainedModel { }\n\n/**\n * The OPT Model transformer with a language modeling head on top (linear layer with weights tied to the input embeddings).\n */\nexport class OPTForCausalLM extends OPTPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class ViTPreTrainedModel extends PreTrainedModel { }\nexport class ViTModel extends ViTPreTrainedModel { }\nexport class ViTForImageClassification extends ViTPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class VitMattePreTrainedModel extends PreTrainedModel { }\n\n/**\n * ViTMatte framework leveraging any vision backbone e.g. for ADE20k, CityScapes.\n * \n * **Example:** Perform image matting with a `VitMatteForImageMatting` model.\n * ```javascript\n * import { AutoProcessor, VitMatteForImageMatting, RawImage } from '@xenova/transformers';\n * \n * // Load processor and model\n * const processor = await AutoProcessor.from_pretrained('Xenova/vitmatte-small-distinctions-646');\n * const model = await VitMatteForImageMatting.from_pretrained('Xenova/vitmatte-small-distinctions-646');\n * \n * // Load image and trimap\n * const image = await RawImage.fromURL('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/vitmatte_image.png');\n * const trimap = await RawImage.fromURL('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/vitmatte_trimap.png');\n * \n * // Prepare image + trimap for the model\n * const inputs = await processor(image, trimap);\n * \n * // Predict alpha matte\n * const { alphas } = await model(inputs);\n * // Tensor {\n * //   dims: [ 1, 1, 640, 960 ],\n * //   type: 'float32',\n * //   size: 614400,\n * //   data: Float32Array(614400) [ 0.9894027709960938, 0.9970508813858032, ... ]\n * // }\n * ```\n * \n * You can visualize the alpha matte as follows:\n * ```javascript\n * import { Tensor, cat } from '@xenova/transformers';\n * \n * // Visualize predicted alpha matte\n * const imageTensor = new Tensor(\n *   'uint8',\n *   new Uint8Array(image.data),\n *   [image.height, image.width, image.channels]\n * ).transpose(2, 0, 1);\n * \n * // Convert float (0-1) alpha matte to uint8 (0-255)\n * const alphaChannel = alphas\n *   .squeeze(0)\n *   .mul_(255)\n *   .clamp_(0, 255)\n *   .round_()\n *   .to('uint8');\n * \n * // Concatenate original image with predicted alpha\n * const imageData = cat([imageTensor, alphaChannel], 0);\n * \n * // Save output image\n * const outputImage = RawImage.fromTensor(imageData);\n * outputImage.save('output.png');\n * ```\n */\nexport class VitMatteForImageMatting extends VitMattePreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new ImageMattingOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class MobileViTPreTrainedModel extends PreTrainedModel { }\nexport class MobileViTModel extends MobileViTPreTrainedModel { }\nexport class MobileViTForImageClassification extends MobileViTPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n// TODO: MobileViTForSemanticSegmentation\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class OwlViTPreTrainedModel extends PreTrainedModel { }\nexport class OwlViTModel extends OwlViTPreTrainedModel { }\nexport class OwlViTForObjectDetection extends OwlViTPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Beit Models\nexport class BeitPreTrainedModel extends PreTrainedModel { }\nexport class BeitModel extends BeitPreTrainedModel { }\nexport class BeitForImageClassification extends BeitPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class DetrPreTrainedModel extends PreTrainedModel { }\nexport class DetrModel extends DetrPreTrainedModel { }\nexport class DetrForObjectDetection extends DetrPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new DetrObjectDetectionOutput(await super._call(model_inputs));\n    }\n}\n\nexport class DetrForSegmentation extends DetrPreTrainedModel {\n    /**\n     * Runs the model with the provided inputs\n     * @param {Object} model_inputs Model inputs\n     * @returns {Promise<DetrSegmentationOutput>} Object containing segmentation outputs\n     */\n    async _call(model_inputs) {\n        return new DetrSegmentationOutput(await super._call(model_inputs));\n    }\n}\n\nexport class DetrObjectDetectionOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Classification logits (including no-object) for all queries.\n     * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).\n     * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).\n     */\n    constructor({ logits, pred_boxes }) {\n        super();\n        this.logits = logits;\n        this.pred_boxes = pred_boxes;\n    }\n}\n\nexport class DetrSegmentationOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits The output logits of the model.\n     * @param {Tensor} output.pred_boxes Predicted boxes.\n     * @param {Tensor} output.pred_masks Predicted masks.\n     */\n    constructor({ logits, pred_boxes, pred_masks }) {\n        super();\n        this.logits = logits;\n        this.pred_boxes = pred_boxes;\n        this.pred_masks = pred_masks;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class DeiTPreTrainedModel extends PreTrainedModel { }\nexport class DeiTModel extends DeiTPreTrainedModel { }\nexport class DeiTForImageClassification extends DeiTPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class ResNetPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare ResNet model outputting raw features without any specific head on top.\n */\nexport class ResNetModel extends ResNetPreTrainedModel { }\n\n/**\n * ResNet Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n */\nexport class ResNetForImageClassification extends ResNetPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class SwinPreTrainedModel extends PreTrainedModel { }\nexport class SwinModel extends SwinPreTrainedModel { }\nexport class SwinForImageClassification extends SwinPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class Swin2SRPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare Swin2SR Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class Swin2SRModel extends Swin2SRPreTrainedModel { }\n\n/**\n * Swin2SR Model transformer with an upsampler head on top for image super resolution and restoration.\n * \n * **Example:** Super-resolution w/ `Xenova/swin2SR-classical-sr-x2-64`.\n * \n * ```javascript\n * import { AutoProcessor, Swin2SRForImageSuperResolution, RawImage } from '@xenova/transformers';\n * \n * // Load processor and model\n * const model_id = 'Xenova/swin2SR-classical-sr-x2-64';\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * const model = await Swin2SRForImageSuperResolution.from_pretrained(model_id);\n * \n * // Prepare model inputs\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/butterfly.jpg';\n * const image = await RawImage.fromURL(url);\n * const inputs = await processor(image);\n * \n * // Run model\n * const outputs = await model(inputs);\n * \n * // Convert Tensor to RawImage\n * const output = outputs.reconstruction.squeeze().clamp_(0, 1).mul_(255).round_().to('uint8');\n * const outputImage = RawImage.fromTensor(output);\n * // RawImage {\n * //   data: Uint8Array(786432) [ 41, 31, 24, ... ],\n * //   width: 512,\n * //   height: 512,\n * //   channels: 3\n * // }\n * ```\n */\nexport class Swin2SRForImageSuperResolution extends Swin2SRPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class DPTPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare DPT Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class DPTModel extends DPTPreTrainedModel { }\n\n/**\n * DPT Model with a depth estimation head on top (consisting of 3 convolutional layers) e.g. for KITTI, NYUv2.\n * \n * **Example:** Depth estimation w/ `Xenova/dpt-hybrid-midas`.\n * ```javascript\n * import { DPTForDepthEstimation, AutoProcessor, RawImage, interpolate, max } from '@xenova/transformers';\n * \n * // Load model and processor\n * const model_id = 'Xenova/dpt-hybrid-midas';\n * const model = await DPTForDepthEstimation.from_pretrained(model_id);\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * \n * // Load image from URL\n * const url = 'http://images.cocodataset.org/val2017/000000039769.jpg';\n * const image = await RawImage.fromURL(url);\n * \n * // Prepare image for the model\n * const inputs = await processor(image);\n * \n * // Run model\n * const { predicted_depth } = await model(inputs);\n * \n * // Interpolate to original size\n * const prediction = interpolate(predicted_depth, image.size.reverse(), 'bilinear', false);\n * \n * // Visualize the prediction\n * const formatted = prediction.mul_(255 / max(prediction.data)[0]).to('uint8');\n * const depth = RawImage.fromTensor(formatted);\n * // RawImage {\n * //   data: Uint8Array(307200) [ 85, 85, 84, ... ],\n * //   width: 640,\n * //   height: 480,\n * //   channels: 1\n * // }\n * ```\n */\nexport class DPTForDepthEstimation extends DPTPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class GLPNPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare GLPN encoder (Mix-Transformer) outputting raw hidden-states without any specific head on top.\n */\nexport class GLPNModel extends GLPNPreTrainedModel { }\n\n/**\n * GLPN Model transformer with a lightweight depth estimation head on top e.g. for KITTI, NYUv2.\n * \n * **Example:** Depth estimation w/ `Xenova/glpn-kitti`.\n * ```javascript\n * import { GLPNForDepthEstimation, AutoProcessor, RawImage, interpolate, max } from '@xenova/transformers';\n * \n * // Load model and processor\n * const model_id = 'Xenova/glpn-kitti';\n * const model = await GLPNForDepthEstimation.from_pretrained(model_id);\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * \n * // Load image from URL\n * const url = 'http://images.cocodataset.org/val2017/000000039769.jpg';\n * const image = await RawImage.fromURL(url);\n * \n * // Prepare image for the model\n * const inputs = await processor(image);\n * \n * // Run model\n * const { predicted_depth } = await model(inputs);\n * \n * // Interpolate to original size\n * const prediction = interpolate(predicted_depth, image.size.reverse(), 'bilinear', false);\n * \n * // Visualize the prediction\n * const formatted = prediction.mul_(255 / max(prediction.data)[0]).to('uint8');\n * const depth = RawImage.fromTensor(formatted);\n * // RawImage {\n * //   data: Uint8Array(307200) [ 207, 169, 154, ... ],\n * //   width: 640,\n * //   height: 480,\n * //   channels: 1\n * // }\n * ```\n */\nexport class GLPNForDepthEstimation extends GLPNPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class DonutSwinPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare Donut Swin Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Step-by-step Document Parsing.\n * \n * ```javascript\n * import { AutoProcessor, AutoTokenizer, AutoModelForVision2Seq, RawImage } from '@xenova/transformers';\n * \n * // Choose model to use\n * const model_id = 'Xenova/donut-base-finetuned-cord-v2';\n * \n * // Prepare image inputs\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/receipt.png';\n * const image = await RawImage.read(url);\n * const image_inputs = await processor(image);\n * \n * // Prepare decoder inputs\n * const tokenizer = await AutoTokenizer.from_pretrained(model_id);\n * const task_prompt = '<s_cord-v2>';\n * const decoder_input_ids = tokenizer(task_prompt, {\n *   add_special_tokens: false,\n * }).input_ids;\n * \n * // Create the model\n * const model = await AutoModelForVision2Seq.from_pretrained(model_id);\n * \n * // Run inference\n * const output = await model.generate(image_inputs.pixel_values, {\n *   decoder_input_ids,\n *   max_length: model.config.decoder.max_position_embeddings,\n * });\n * \n * // Decode output\n * const decoded = tokenizer.batch_decode(output)[0];\n * // <s_cord-v2><s_menu><s_nm> CINNAMON SUGAR</s_nm><s_unitprice> 17,000</s_unitprice><s_cnt> 1 x</s_cnt><s_price> 17,000</s_price></s_menu><s_sub_total><s_subtotal_price> 17,000</s_subtotal_price></s_sub_total><s_total><s_total_price> 17,000</s_total_price><s_cashprice> 20,000</s_cashprice><s_changeprice> 3,000</s_changeprice></s_total></s>\n * ```\n * \n * **Example:** Step-by-step Document Visual Question Answering (DocVQA)\n * \n * ```javascript\n * import { AutoProcessor, AutoTokenizer, AutoModelForVision2Seq, RawImage } from '@xenova/transformers';\n * \n * // Choose model to use\n * const model_id = 'Xenova/donut-base-finetuned-docvqa';\n * \n * // Prepare image inputs\n * const processor = await AutoProcessor.from_pretrained(model_id);\n * const url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/invoice.png';\n * const image = await RawImage.read(url);\n * const image_inputs = await processor(image);\n * \n * // Prepare decoder inputs\n * const tokenizer = await AutoTokenizer.from_pretrained(model_id);\n * const question = 'What is the invoice number?';\n * const task_prompt = `<s_docvqa><s_question>${question}</s_question><s_answer>`;\n * const decoder_input_ids = tokenizer(task_prompt, {\n *   add_special_tokens: false,\n * }).input_ids;\n * \n * // Create the model\n * const model = await AutoModelForVision2Seq.from_pretrained(model_id);\n * \n * // Run inference\n * const output = await model.generate(image_inputs.pixel_values, {\n *   decoder_input_ids,\n *   max_length: model.config.decoder.max_position_embeddings,\n * });\n * \n * // Decode output\n * const decoded = tokenizer.batch_decode(output)[0];\n * // <s_docvqa><s_question> What is the invoice number?</s_question><s_answer> us-001</s_answer></s>\n * ```\n */\nexport class DonutSwinModel extends DonutSwinPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class ConvNextPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare ConvNext model outputting raw features without any specific head on top.\n */\nexport class ConvNextModel extends ConvNextPreTrainedModel { }\n\n/**\n * ConvNext Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n */\nexport class ConvNextForImageClassification extends ConvNextPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class ConvNextV2PreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare ConvNextV2 model outputting raw features without any specific head on top.\n */\nexport class ConvNextV2Model extends ConvNextV2PreTrainedModel { }\n\n/**\n * ConvNextV2 Model with an image classification head on top (a linear layer on top of the pooled features), e.g. for ImageNet.\n */\nexport class ConvNextV2ForImageClassification extends ConvNextV2PreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class Dinov2PreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare DINOv2 Model transformer outputting raw hidden-states without any specific head on top.\n */\nexport class Dinov2Model extends Dinov2PreTrainedModel { }\n\n/**\n * Dinov2 Model transformer with an image classification head on top (a linear layer on top of the final hidden state of the [CLS] token) e.g. for ImageNet.\n */\nexport class Dinov2ForImageClassification extends Dinov2PreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class YolosPreTrainedModel extends PreTrainedModel { }\nexport class YolosModel extends YolosPreTrainedModel { }\nexport class YolosForObjectDetection extends YolosPreTrainedModel {\n    /**\n     * @param {any} model_inputs\n     */\n    async _call(model_inputs) {\n        return new YolosObjectDetectionOutput(await super._call(model_inputs));\n    }\n}\n\nexport class YolosObjectDetectionOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Classification logits (including no-object) for all queries.\n     * @param {Tensor} output.pred_boxes Normalized boxes coordinates for all queries, represented as (center_x, center_y, width, height).\n     * These values are normalized in [0, 1], relative to the size of each individual image in the batch (disregarding possible padding).\n     */\n    constructor({ logits, pred_boxes }) {\n        super();\n        this.logits = logits;\n        this.pred_boxes = pred_boxes;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\nexport class SamPreTrainedModel extends PreTrainedModel { }\nexport class SamModel extends SamPreTrainedModel {\n    /**\n     * @param {Object} model_inputs\n     * @param {Tensor} model_inputs.pixel_values Pixel values as a Tensor with shape `(batch_size, num_channels, height, width)`.\n     * @param {Tensor} model_inputs.input_points Input 2D spatial points with shape `(batch_size, num_points, 2)`. This is used by the prompt encoder to encode the prompt.\n     * @todo Add support for `input_labels`, `input_boxes`, `input_masks`, and `image_embeddings`.\n     */\n    async _call(model_inputs) {\n        return new SamImageSegmentationOutput(await super._call(model_inputs));\n    }\n}\n\n\n/**\n * Base class for Segment-Anything model's output.\n */\nexport class SamImageSegmentationOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.iou_scores The output logits of the model.\n     * @param {Tensor} output.pred_masks Predicted boxes.\n     */\n    constructor({ iou_scores, pred_masks }) {\n        super();\n        this.iou_scores = iou_scores;\n        this.pred_masks = pred_masks;\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// MarianMT models\nexport class MarianPreTrainedModel extends PreTrainedModel { };\n\nexport class MarianModel extends MarianPreTrainedModel { }\n\nexport class MarianMTModel extends MarianPreTrainedModel {\n\n    /**\n     * Creates a new instance of the `MarianMTModel` class.\n    * @param {Object} config The model configuration object.\n    * @param {Object} session The ONNX session object.\n    * @param {any} decoder_merged_session \n    * @param {any} generation_config \n    */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// M2M100 models\nexport class M2M100PreTrainedModel extends PreTrainedModel { };\n\nexport class M2M100Model extends M2M100PreTrainedModel { }\n\nexport class M2M100ForConditionalGeneration extends M2M100PreTrainedModel {\n\n    /**\n     * Creates a new instance of the `M2M100ForConditionalGeneration` class.\n    * @param {Object} config The model configuration object.\n    * @param {Object} session The ONNX session object.\n    * @param {any} decoder_merged_session \n    * @param {any} generation_config \n    */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.d_model / this.num_encoder_heads;\n    }\n\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Wav2Vec2 models\nexport class Wav2Vec2PreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare Wav2Vec2 Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Load and run a `Wav2Vec2Model` for feature extraction.\n * \n * ```javascript\n * import { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/mms-300m');\n * const audio = await read_audio('https://huggingface.co/datasets/Narsil/asr_dummy/resolve/main/mlk.flac', 16000);\n * const inputs = await processor(audio);\n * \n * // Run model with inputs\n * const model = await AutoModel.from_pretrained('Xenova/mms-300m');\n * const output = await model(inputs);\n * // {\n * //   last_hidden_state: Tensor {\n * //     dims: [ 1, 1144, 1024 ],\n * //     type: 'float32',\n * //     data: Float32Array(1171456) [ ... ],\n * //     size: 1171456\n * //   }\n * // }\n * ```\n */\nexport class Wav2Vec2Model extends Wav2Vec2PreTrainedModel { }\n\nexport class Wav2Vec2ForCTC extends Wav2Vec2PreTrainedModel {\n    /**\n     * @param {Object} model_inputs\n     * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n     * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n     */\n    async _call(model_inputs) {\n        return new CausalLMOutput(await super._call(model_inputs));\n    }\n}\n\nexport class Wav2Vec2ForSequenceClassification extends Wav2Vec2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Hubert models\nexport class HubertPreTrainedModel extends PreTrainedModel { }\n\n/**\n * The bare Hubert Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Load and run a `HubertModel` for feature extraction.\n * \n * ```javascript\n * import { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/hubert-base-ls960');\n * const audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav', 16000);\n * const inputs = await processor(audio);\n * \n * // Load and run model with inputs\n * const model = await AutoModel.from_pretrained('Xenova/hubert-base-ls960');\n * const output = await model(inputs);\n * // {\n * //   last_hidden_state: Tensor {\n * //     dims: [ 1, 549, 768 ],\n * //     type: 'float32',\n * //     data: Float32Array(421632) [0.0682469978928566, 0.08104046434164047, -0.4975186586380005, ...],\n * //     size: 421632\n * //   }\n * // }\n * ```\n */\nexport class HubertModel extends Wav2Vec2PreTrainedModel { }\n\n/**\n * Hubert Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n */\nexport class HubertForCTC extends Wav2Vec2PreTrainedModel {\n    /**\n     * @param {Object} model_inputs\n     * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n     * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n     */\n    async _call(model_inputs) {\n        return new CausalLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * Hubert Model with a sequence classification head on top (a linear layer over the pooled output) for tasks like SUPERB Keyword Spotting.\n */\nexport class HubertForSequenceClassification extends Wav2Vec2PreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// WavLM models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class WavLMPreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare WavLM Model transformer outputting raw hidden-states without any specific head on top.\n * \n * **Example:** Load and run a `WavLMModel` for feature extraction.\n * \n * ```javascript\n * import { AutoProcessor, AutoModel, read_audio } from '@xenova/transformers';\n * \n * // Read and preprocess audio\n * const processor = await AutoProcessor.from_pretrained('Xenova/wavlm-base');\n * const audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav', 16000);\n * const inputs = await processor(audio);\n * \n * // Run model with inputs\n * const model = await AutoModel.from_pretrained('Xenova/wavlm-base');\n * const output = await model(inputs);\n * // {\n * //   last_hidden_state: Tensor {\n * //     dims: [ 1, 549, 768 ],\n * //     type: 'float32',\n * //     data: Float32Array(421632) [-0.349443256855011, -0.39341306686401367,  0.022836603224277496, ...],\n * //     size: 421632\n * //   }\n * // }\n * ```\n */\nexport class WavLMModel extends WavLMPreTrainedModel { }\n\n/**\n * WavLM Model with a `language modeling` head on top for Connectionist Temporal Classification (CTC).\n */\nexport class WavLMForCTC extends WavLMPreTrainedModel {\n    /**\n     * @param {Object} model_inputs\n     * @param {Tensor} model_inputs.input_values Float values of input raw speech waveform.\n     * @param {Tensor} model_inputs.attention_mask Mask to avoid performing convolution and attention on padding token indices. Mask values selected in [0, 1]\n     */\n    async _call(model_inputs) {\n        return new CausalLMOutput(await super._call(model_inputs));\n    }\n}\n\n/**\n * WavLM Model with a sequence classification head on top (a linear layer over the pooled output).\n */\nexport class WavLMForSequenceClassification extends WavLMPreTrainedModel {\n    /**\n     * Calls the model on new inputs.\n     * @param {Object} model_inputs The inputs to the model.\n     * @returns {Promise<SequenceClassifierOutput>} An object containing the model's output logits for sequence classification.\n     */\n    async _call(model_inputs) {\n        return new SequenceClassifierOutput(await super._call(model_inputs));\n    }\n}\n\n//////////////////////////////////////////////////\n// SpeechT5 models\n/**\n * An abstract class to handle weights initialization and a simple interface for downloading and loading pretrained models.\n */\nexport class SpeechT5PreTrainedModel extends PreTrainedModel { };\n\n/**\n * The bare SpeechT5 Encoder-Decoder Model outputting raw hidden-states without any specific pre- or post-nets.\n */\nexport class SpeechT5Model extends SpeechT5PreTrainedModel { };\n\n/**\n * SpeechT5 Model with a speech encoder and a text decoder.\n * \n * **Example:** Generate speech from text with `SpeechT5ForSpeechToText`.\n * ```javascript\n * import { AutoTokenizer, AutoProcessor, SpeechT5ForTextToSpeech, SpeechT5HifiGan, Tensor } from '@xenova/transformers';\n * \n * // Load the tokenizer and processor\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/speecht5_tts');\n * const processor = await AutoProcessor.from_pretrained('Xenova/speecht5_tts');\n * \n * // Load the models\n * // NOTE: We use the unquantized versions as they are more accurate\n * const model = await SpeechT5ForTextToSpeech.from_pretrained('Xenova/speecht5_tts', { quantized: false });\n * const vocoder = await SpeechT5HifiGan.from_pretrained('Xenova/speecht5_hifigan', { quantized: false });\n * \n * // Load speaker embeddings from URL\n * const speaker_embeddings_data = new Float32Array(\n *     await (await fetch('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/speaker_embeddings.bin')).arrayBuffer()\n * );\n * const speaker_embeddings = new Tensor(\n *     'float32',\n *     speaker_embeddings_data,\n *     [1, speaker_embeddings_data.length]\n * )\n * \n * // Run tokenization\n * const { input_ids } = tokenizer('Hello, my dog is cute');\n * \n * // Generate waveform\n * const { waveform } = await model.generate_speech(input_ids, speaker_embeddings, { vocoder });\n * console.log(waveform)\n * // Tensor {\n * //   dims: [ 26112 ],\n * //   type: 'float32',\n * //   size: 26112,\n * //   data: Float32Array(26112) [ -0.00043630177970044315, -0.00018082228780258447, ... ],\n * // }\n * ```\n */\nexport class SpeechT5ForSpeechToText extends SpeechT5PreTrainedModel { }\n\n/**\n * SpeechT5 Model with a text encoder and a speech decoder.\n */\nexport class SpeechT5ForTextToSpeech extends SpeechT5PreTrainedModel {\n\n    /**\n     * Creates a new instance of the `SpeechT5ForTextToSpeech` class.\n     * @param {Object} config The model configuration.\n     * @param {any} session session for the model.\n     * @param {any} decoder_merged_session session for the decoder.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, decoder_merged_session, generation_config) {\n        super(config, session);\n        this.decoder_merged_session = decoder_merged_session;\n        this.generation_config = generation_config;\n\n        this.num_decoder_layers = this.config.decoder_layers;\n        this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.decoder_dim_kv = this.config.hidden_size / this.num_decoder_heads;\n\n        this.num_encoder_layers = this.config.encoder_layers;\n        this.num_encoder_heads = this.config.encoder_attention_heads;\n        this.encoder_dim_kv = this.config.hidden_size / this.num_encoder_heads;\n    }\n\n    /**\n     * @typedef {Object} SpeechOutput\n     * @property {Tensor} [spectrogram] The predicted log-mel spectrogram of shape\n     * `(output_sequence_length, config.num_mel_bins)`. Returned when no `vocoder` is provided\n     * @property {Tensor} [waveform] The predicted waveform of shape `(num_frames,)`. Returned when a `vocoder` is provided.\n     * @property {Tensor} [cross_attentions] The outputs of the decoder's cross-attention layers of shape\n     * `(config.decoder_layers, config.decoder_attention_heads, output_sequence_length, input_sequence_length)`. returned when `output_cross_attentions` is `true`.\n     */\n\n    /**\n     * Converts a sequence of input tokens into a sequence of mel spectrograms, which are subsequently turned into a speech waveform using a vocoder.\n     * @param {Tensor} input_values Indices of input sequence tokens in the vocabulary.\n     * @param {Tensor} speaker_embeddings Tensor containing the speaker embeddings.\n     * @param {Object} options Optional parameters for generating speech.\n     * @param {number} [options.threshold=0.5] The generated sequence ends when the predicted stop token probability exceeds this value.\n     * @param {number} [options.minlenratio=0.0] Used to calculate the minimum required length for the output sequence.\n     * @param {number} [options.maxlenratio=20.0] Used to calculate the maximum allowed length for the output sequence.\n     * @param {Object} [options.vocoder=null] The vocoder that converts the mel spectrogram into a speech waveform. If `null`, the output is the mel spectrogram.\n     * @param {boolean} [options.output_cross_attentions=false] Whether or not to return the attentions tensors of the decoder's cross-attention layers.\n     * @returns {Promise<SpeechOutput>} A promise which resolves to an object containing the spectrogram, waveform, and cross-attention tensors.\n     */\n    async generate_speech(input_values, speaker_embeddings, {\n        threshold = 0.5,\n        minlenratio = 0.0,\n        maxlenratio = 20.0,\n        vocoder = null,\n        // output_cross_attentions = false, // TODO add\n    } = {}) {\n\n        const model_inputs = {\n            input_ids: input_values\n        }\n\n        const { encoder_outputs, encoder_attention_mask } = await encoderForward(this, model_inputs);\n\n        const r = encoder_outputs.dims[1] / this.config.reduction_factor;\n        const maxlen = Math.floor(r * maxlenratio);\n        const minlen = Math.floor(r * minlenratio);\n\n        const num_mel_bins = this.config.num_mel_bins;\n\n        let spectrogramParts = [];\n        let past_key_values = null;\n        let decoder_outputs = null;\n        let idx = 0;\n\n        while (true) {\n            ++idx;\n\n            const use_cache_branch = boolTensor(!!decoder_outputs);\n            let output_sequence;\n            if (decoder_outputs) {\n                output_sequence = decoder_outputs.output_sequence_out;\n            } else {\n                output_sequence = new Tensor(\n                    'float32',\n                    new Float32Array(num_mel_bins),\n                    [1, 1, num_mel_bins],\n                )\n            }\n            let decoderFeeds = {\n                use_cache_branch,\n                output_sequence,\n                encoder_attention_mask: encoder_attention_mask,\n                speaker_embeddings: speaker_embeddings,\n                encoder_hidden_states: encoder_outputs,\n            };\n\n            this.addPastKeyValues(decoderFeeds, past_key_values);\n            decoder_outputs = await sessionRun(this.decoder_merged_session, decoderFeeds);\n            past_key_values = this.getPastKeyValues(decoder_outputs, past_key_values);\n\n            const { prob, spectrum } = decoder_outputs;\n            spectrogramParts.push(spectrum);\n\n            if (idx >= minlen && (\n                // Finished when stop token or maximum length is reached.\n                Array.from(prob.data).filter(p => p >= threshold).length > 0 || idx >= maxlen\n            )) {\n                break;\n            }\n        }\n\n        const spectrogram = cat(spectrogramParts);\n        const { waveform } = await sessionRun(vocoder.session, { spectrogram });\n\n        return {\n            spectrogram,\n            waveform,\n            // cross_attentions: null, // TODO add\n        }\n    }\n}\n\n/**\n * HiFi-GAN vocoder.\n * \n * See [SpeechT5ForSpeechToText](./models#module_models.SpeechT5ForSpeechToText) for example usage.\n */\nexport class SpeechT5HifiGan extends PreTrainedModel {\n    main_input_name = 'spectrogram';\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// TrOCR models\nexport class TrOCRPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `TrOCRPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id;\n\n        this.num_encoder_layers = this.num_decoder_layers = this.config.decoder_layers;\n        this.num_encoder_heads = this.num_decoder_heads = this.config.decoder_attention_heads;\n        this.encoder_dim_kv = this.decoder_dim_kv = this.config.d_model / this.num_decoder_heads;\n    }\n}\n\n/**\n * The TrOCR Decoder with a language modeling head.\n */\nexport class TrOCRForCausalLM extends TrOCRPreTrainedModel { }\n\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// Mistral models\n/**\n * The bare Mistral Model outputting raw hidden-states without any specific head on top.\n */\nexport class MistralPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `MistralPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_key_value_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.config.num_attention_heads;\n    }\n}\n\nexport class MistralModel extends MistralPreTrainedModel { }\n\nexport class MistralForCausalLM extends MistralPreTrainedModel { }\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\n// Falcon models\n/**\n * The bare Falcon Model outputting raw hidden-states without any specific head on top.\n */\nexport class FalconPreTrainedModel extends PreTrainedModel {\n    /**\n     * Creates a new instance of the `FalconPreTrainedModel` class.\n     * @param {Object} config The configuration of the model.\n     * @param {any} session The ONNX session containing the model weights.\n     * @param {GenerationConfig} generation_config The generation configuration.\n     */\n    constructor(config, session, generation_config) {\n        super(config, session);\n        this.generation_config = generation_config;\n\n        // config doesn't contain pad_token_id, so we assume it is the eos_token_id\n        this.config.pad_token_id = this.config.eos_token_id\n\n        this.num_heads = this.config.num_attention_heads;\n        this.num_layers = this.config.num_hidden_layers;\n        this.dim_kv = this.config.hidden_size / this.config.num_attention_heads;\n    }\n}\n\nexport class FalconModel extends FalconPreTrainedModel { }\n\nexport class FalconForCausalLM extends FalconPreTrainedModel { }\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// CLAP models\nexport class ClapPreTrainedModel extends PreTrainedModel { }\n\nexport class ClapModel extends ClapPreTrainedModel { }\n\n/**\n * CLAP Text Model with a projection layer on top (a linear layer on top of the pooled output).\n * \n * **Example:** Compute text embeddings with `ClapTextModelWithProjection`.\n * \n * ```javascript\n * import { AutoTokenizer, ClapTextModelWithProjection } from '@xenova/transformers';\n * \n * // Load tokenizer and text model\n * const tokenizer = await AutoTokenizer.from_pretrained('Xenova/clap-htsat-unfused');\n * const text_model = await ClapTextModelWithProjection.from_pretrained('Xenova/clap-htsat-unfused');\n * \n * // Run tokenization\n * const texts = ['a sound of a cat', 'a sound of a dog'];\n * const text_inputs = tokenizer(texts, { padding: true, truncation: true });\n * \n * // Compute embeddings\n * const { text_embeds } = await text_model(text_inputs);\n * // Tensor {\n * //   dims: [ 2, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(1024) [ ... ],\n * //   size: 1024\n * // }\n * ```\n */\nexport class ClapTextModelWithProjection extends ClapPreTrainedModel {\n\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n        // Update default model file name if not provided\n        options.model_file_name ??= 'text_model';\n        return super.from_pretrained(pretrained_model_name_or_path, options);\n    }\n}\n\n/**\n * CLAP Audio Model with a projection layer on top (a linear layer on top of the pooled output).\n * \n * **Example:** Compute audio embeddings with `ClapAudioModelWithProjection`.\n * \n * ```javascript\n * import { AutoProcessor, ClapAudioModelWithProjection, read_audio } from '@xenova/transformers';\n * \n * // Load processor and audio model\n * const processor = await AutoProcessor.from_pretrained('Xenova/clap-htsat-unfused');\n * const audio_model = await ClapAudioModelWithProjection.from_pretrained('Xenova/clap-htsat-unfused');\n * \n * // Read audio and run processor\n * const audio = await read_audio('https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/cat_meow.wav');\n * const audio_inputs = await processor(audio);\n * \n * // Compute embeddings\n * const { audio_embeds } = await audio_model(audio_inputs);\n * // Tensor {\n * //   dims: [ 1, 512 ],\n * //   type: 'float32',\n * //   data: Float32Array(512) [ ... ],\n * //   size: 512\n * // }\n * ```\n */\nexport class ClapAudioModelWithProjection extends ClapPreTrainedModel {\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, options = {}) {\n        // Update default model file name if not provided\n        options.model_file_name ??= 'audio_model';\n        return super.from_pretrained(pretrained_model_name_or_path, options);\n    }\n}\n//////////////////////////////////////////////////\n\n\n//////////////////////////////////////////////////\n// AutoModels, used to simplify construction of PreTrainedModels\n// (uses config to instantiate correct class)\n\n/**\n * Base class of all AutoModels. Contains the `from_pretrained` function\n * which is used to instantiate pretrained models.\n */\nexport class PretrainedMixin {\n    /**\n     * Mapping from model type to model class.\n     * @type {Map<string, Object>[]}\n     */\n    static MODEL_CLASS_MAPPINGS = null;\n\n    /**\n     * Whether to attempt to instantiate the base class (`PretrainedModel`) if \n     * the model type is not found in the mapping.\n     */\n    static BASE_IF_FAIL = false;\n\n\n    /** @type {PreTrainedModel.from_pretrained} */\n    static async from_pretrained(pretrained_model_name_or_path, {\n        quantized = true,\n        progress_callback = null,\n        config = null,\n        cache_dir = null,\n        local_files_only = false,\n        revision = 'main',\n        model_file_name = null,\n    } = {}) {\n\n        let options = {\n            quantized,\n            progress_callback,\n            config,\n            cache_dir,\n            local_files_only,\n            revision,\n            model_file_name,\n        }\n        config = await AutoConfig.from_pretrained(pretrained_model_name_or_path, options);\n        if (!options.config) {\n            // If no config was passed, reuse this config for future processing\n            options.config = config;\n        }\n\n        if (!this.MODEL_CLASS_MAPPINGS) {\n            throw new Error(\"`MODEL_CLASS_MAPPINGS` not implemented for this type of `AutoClass`: \" + this.name);\n        }\n\n        for (let MODEL_CLASS_MAPPING of this.MODEL_CLASS_MAPPINGS) {\n            const modelInfo = MODEL_CLASS_MAPPING.get(config.model_type);\n            if (!modelInfo) {\n                continue; // Item not found in this mapping\n            }\n            return await modelInfo[1].from_pretrained(pretrained_model_name_or_path, options);\n        }\n\n        if (this.BASE_IF_FAIL) {\n            console.warn(`Unknown model class \"${config.model_type}\", attempting to construct from base class.`);\n            return await PreTrainedModel.from_pretrained(pretrained_model_name_or_path, options);\n        } else {\n            throw Error(`Unsupported model type: ${config.model_type}`)\n        }\n    }\n}\n\nconst MODEL_MAPPING_NAMES_ENCODER_ONLY = new Map([\n    ['bert', ['BertModel', BertModel]],\n    ['electra', ['ElectraModel', ElectraModel]],\n    ['esm', ['EsmModel', EsmModel]],\n    ['convbert', ['ConvBertModel', ConvBertModel]],\n    ['camembert', ['CamembertModel', CamembertModel]],\n    ['deberta', ['DebertaModel', DebertaModel]],\n    ['deberta-v2', ['DebertaV2Model', DebertaV2Model]],\n    ['mpnet', ['MPNetModel', MPNetModel]],\n    ['albert', ['AlbertModel', AlbertModel]],\n    ['distilbert', ['DistilBertModel', DistilBertModel]],\n    ['roberta', ['RobertaModel', RobertaModel]],\n    ['xlm', ['XLMModel', XLMModel]],\n    ['xlm-roberta', ['XLMRobertaModel', XLMRobertaModel]],\n    ['clap', ['ClapModel', ClapModel]],\n    ['clip', ['CLIPModel', CLIPModel]],\n    ['chinese_clip', ['ChineseCLIPModel', ChineseCLIPModel]],\n    ['mobilebert', ['MobileBertModel', MobileBertModel]],\n    ['squeezebert', ['SqueezeBertModel', SqueezeBertModel]],\n    ['wav2vec2', ['Wav2Vec2Model', Wav2Vec2Model]],\n    ['hubert', ['HubertModel', HubertModel]],\n    ['wavlm', ['WavLMModel', WavLMModel]],\n    ['audio-spectrogram-transformer', ['ASTModel', ASTModel]],\n\n    ['detr', ['DetrModel', DetrModel]],\n    ['vit', ['ViTModel', ViTModel]],\n    ['mobilevit', ['MobileViTModel', MobileViTModel]],\n    ['owlvit', ['OwlViTModel', OwlViTModel]],\n    ['beit', ['BeitModel', BeitModel]],\n    ['deit', ['DeiTModel', DeiTModel]],\n    ['convnext', ['ConvNextModel', ConvNextModel]],\n    ['convnextv2', ['ConvNextV2Model', ConvNextV2Model]],\n    ['dinov2', ['Dinov2Model', Dinov2Model]],\n    ['resnet', ['ResNetModel', ResNetModel]],\n    ['swin', ['SwinModel', SwinModel]],\n    ['swin2sr', ['Swin2SRModel', Swin2SRModel]],\n    ['donut-swin', ['DonutSwinModel', DonutSwinModel]],\n    ['yolos', ['YolosModel', YolosModel]],\n    ['dpt', ['DPTModel', DPTModel]],\n    ['glpn', ['GLPNModel', GLPNModel]],\n\n    ['hifigan', ['SpeechT5HifiGan', SpeechT5HifiGan]],\n\n    ['sam', ['SamModel', SamModel]], // TODO change to encoder-decoder when model is split correctly\n]);\n\nconst MODEL_MAPPING_NAMES_ENCODER_DECODER = new Map([\n    ['t5', ['T5Model', T5Model]],\n    ['longt5', ['LongT5Model', LongT5Model]],\n    ['mt5', ['MT5Model', MT5Model]],\n    ['bart', ['BartModel', BartModel]],\n    ['mbart', ['MBartModel', MBartModel]],\n    ['marian', ['MarianModel', MarianModel]],\n    ['whisper', ['WhisperModel', WhisperModel]],\n    ['m2m_100', ['M2M100Model', M2M100Model]],\n    ['blenderbot', ['BlenderbotModel', BlenderbotModel]],\n    ['blenderbot-small', ['BlenderbotSmallModel', BlenderbotSmallModel]],\n]);\n\n\nconst MODEL_MAPPING_NAMES_DECODER_ONLY = new Map([\n    ['bloom', ['BloomModel', BloomModel]],\n    ['gpt2', ['GPT2Model', GPT2Model]],\n    ['gptj', ['GPTJModel', GPTJModel]],\n    ['gpt_bigcode', ['GPTBigCodeModel', GPTBigCodeModel]],\n    ['gpt_neo', ['GPTNeoModel', GPTNeoModel]],\n    ['gpt_neox', ['GPTNeoXModel', GPTNeoXModel]],\n    ['codegen', ['CodeGenModel', CodeGenModel]],\n    ['llama', ['LlamaModel', LlamaModel]],\n    ['phi', ['PhiModel', PhiModel]],\n    ['mpt', ['MptModel', MptModel]],\n    ['opt', ['OPTModel', OPTModel]],\n    ['mistral', ['MistralModel', MistralModel]],\n    ['falcon', ['FalconModel', FalconModel]],\n]);\n\nconst MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES = new Map([\n    ['speecht5', ['SpeechT5ForSpeechToText', SpeechT5ForSpeechToText]],\n    ['whisper', ['WhisperForConditionalGeneration', WhisperForConditionalGeneration]],\n])\n\nconst MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES = new Map([\n    ['speecht5', ['SpeechT5ForTextToSpeech', SpeechT5ForTextToSpeech]],\n])\n\nconst MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES = new Map([\n    ['bert', ['BertForSequenceClassification', BertForSequenceClassification]],\n    ['electra', ['ElectraForSequenceClassification', ElectraForSequenceClassification]],\n    ['esm', ['EsmForSequenceClassification', EsmForSequenceClassification]],\n    ['convbert', ['ConvBertForSequenceClassification', ConvBertForSequenceClassification]],\n    ['camembert', ['CamembertForSequenceClassification', CamembertForSequenceClassification]],\n    ['deberta', ['DebertaForSequenceClassification', DebertaForSequenceClassification]],\n    ['deberta-v2', ['DebertaV2ForSequenceClassification', DebertaV2ForSequenceClassification]],\n    ['mpnet', ['MPNetForSequenceClassification', MPNetForSequenceClassification]],\n    ['albert', ['AlbertForSequenceClassification', AlbertForSequenceClassification]],\n    ['distilbert', ['DistilBertForSequenceClassification', DistilBertForSequenceClassification]],\n    ['roberta', ['RobertaForSequenceClassification', RobertaForSequenceClassification]],\n    ['xlm', ['XLMForSequenceClassification', XLMForSequenceClassification]],\n    ['xlm-roberta', ['XLMRobertaForSequenceClassification', XLMRobertaForSequenceClassification]],\n    ['bart', ['BartForSequenceClassification', BartForSequenceClassification]],\n    ['mbart', ['MBartForSequenceClassification', MBartForSequenceClassification]],\n    ['mobilebert', ['MobileBertForSequenceClassification', MobileBertForSequenceClassification]],\n    ['squeezebert', ['SqueezeBertForSequenceClassification', SqueezeBertForSequenceClassification]],\n]);\n\nconst MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES = new Map([\n    ['bert', ['BertForTokenClassification', BertForTokenClassification]],\n    ['electra', ['ElectraForTokenClassification', ElectraForTokenClassification]],\n    ['esm', ['EsmForTokenClassification', EsmForTokenClassification]],\n    ['convbert', ['ConvBertForTokenClassification', ConvBertForTokenClassification]],\n    ['camembert', ['CamembertForTokenClassification', CamembertForTokenClassification]],\n    ['deberta', ['DebertaForTokenClassification', DebertaForTokenClassification]],\n    ['deberta-v2', ['DebertaV2ForTokenClassification', DebertaV2ForTokenClassification]],\n    ['mpnet', ['MPNetForTokenClassification', MPNetForTokenClassification]],\n    ['distilbert', ['DistilBertForTokenClassification', DistilBertForTokenClassification]],\n    ['roberta', ['RobertaForTokenClassification', RobertaForTokenClassification]],\n    ['xlm', ['XLMForTokenClassification', XLMForTokenClassification]],\n    ['xlm-roberta', ['XLMRobertaForTokenClassification', XLMRobertaForTokenClassification]],\n]);\n\nconst MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES = new Map([\n    ['t5', ['T5ForConditionalGeneration', T5ForConditionalGeneration]],\n    ['longt5', ['LongT5ForConditionalGeneration', LongT5ForConditionalGeneration]],\n    ['mt5', ['MT5ForConditionalGeneration', MT5ForConditionalGeneration]],\n    ['bart', ['BartForConditionalGeneration', BartForConditionalGeneration]],\n    ['mbart', ['MBartForConditionalGeneration', MBartForConditionalGeneration]],\n    ['marian', ['MarianMTModel', MarianMTModel]],\n    ['m2m_100', ['M2M100ForConditionalGeneration', M2M100ForConditionalGeneration]],\n    ['blenderbot', ['BlenderbotForConditionalGeneration', BlenderbotForConditionalGeneration]],\n    ['blenderbot-small', ['BlenderbotSmallForConditionalGeneration', BlenderbotSmallForConditionalGeneration]],\n]);\n\nconst MODEL_WITH_LM_HEAD_MAPPING_NAMES = new Map([\n    ['bloom', ['BloomForCausalLM', BloomForCausalLM]],\n    ['gpt2', ['GPT2LMHeadModel', GPT2LMHeadModel]],\n    ['gptj', ['GPTJForCausalLM', GPTJForCausalLM]],\n    ['gpt_bigcode', ['GPTBigCodeForCausalLM', GPTBigCodeForCausalLM]],\n    ['gpt_neo', ['GPTNeoForCausalLM', GPTNeoForCausalLM]],\n    ['gpt_neox', ['GPTNeoXForCausalLM', GPTNeoXForCausalLM]],\n    ['codegen', ['CodeGenForCausalLM', CodeGenForCausalLM]],\n    ['llama', ['LlamaForCausalLM', LlamaForCausalLM]],\n    ['phi', ['PhiForCausalLM', PhiForCausalLM]],\n    ['mpt', ['MptForCausalLM', MptForCausalLM]],\n    ['opt', ['OPTForCausalLM', OPTForCausalLM]],\n    ['mbart', ['MBartForCausalLM', MBartForCausalLM]],\n    ['mistral', ['MistralForCausalLM', MistralForCausalLM]],\n    ['falcon', ['FalconForCausalLM', FalconForCausalLM]],\n    ['trocr', ['TrOCRForCausalLM', TrOCRForCausalLM]],\n]);\n\nconst MODEL_FOR_MASKED_LM_MAPPING_NAMES = new Map([\n    ['bert', ['BertForMaskedLM', BertForMaskedLM]],\n    ['electra', ['ElectraForMaskedLM', ElectraForMaskedLM]],\n    ['esm', ['EsmForMaskedLM', EsmForMaskedLM]],\n    ['convbert', ['ConvBertForMaskedLM', ConvBertForMaskedLM]],\n    ['camembert', ['CamembertForMaskedLM', CamembertForMaskedLM]],\n    ['deberta', ['DebertaForMaskedLM', DebertaForMaskedLM]],\n    ['deberta-v2', ['DebertaV2ForMaskedLM', DebertaV2ForMaskedLM]],\n    ['mpnet', ['MPNetForMaskedLM', MPNetForMaskedLM]],\n    ['albert', ['AlbertForMaskedLM', AlbertForMaskedLM]],\n    ['distilbert', ['DistilBertForMaskedLM', DistilBertForMaskedLM]],\n    ['roberta', ['RobertaForMaskedLM', RobertaForMaskedLM]],\n    ['xlm', ['XLMWithLMHeadModel', XLMWithLMHeadModel]],\n    ['xlm-roberta', ['XLMRobertaForMaskedLM', XLMRobertaForMaskedLM]],\n    ['mobilebert', ['MobileBertForMaskedLM', MobileBertForMaskedLM]],\n    ['squeezebert', ['SqueezeBertForMaskedLM', SqueezeBertForMaskedLM]],\n]);\n\nconst MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES = new Map([\n    ['bert', ['BertForQuestionAnswering', BertForQuestionAnswering]],\n    ['electra', ['ElectraForQuestionAnswering', ElectraForQuestionAnswering]],\n    ['convbert', ['ConvBertForQuestionAnswering', ConvBertForQuestionAnswering]],\n    ['camembert', ['CamembertForQuestionAnswering', CamembertForQuestionAnswering]],\n    ['deberta', ['DebertaForQuestionAnswering', DebertaForQuestionAnswering]],\n    ['deberta-v2', ['DebertaV2ForQuestionAnswering', DebertaV2ForQuestionAnswering]],\n    ['mpnet', ['MPNetForQuestionAnswering', MPNetForQuestionAnswering]],\n    ['albert', ['AlbertForQuestionAnswering', AlbertForQuestionAnswering]],\n    ['distilbert', ['DistilBertForQuestionAnswering', DistilBertForQuestionAnswering]],\n    ['roberta', ['RobertaForQuestionAnswering', RobertaForQuestionAnswering]],\n    ['xlm', ['XLMForQuestionAnswering', XLMForQuestionAnswering]],\n    ['xlm-roberta', ['XLMRobertaForQuestionAnswering', XLMRobertaForQuestionAnswering]],\n    ['mobilebert', ['MobileBertForQuestionAnswering', MobileBertForQuestionAnswering]],\n    ['squeezebert', ['SqueezeBertForQuestionAnswering', SqueezeBertForQuestionAnswering]],\n]);\n\nconst MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES = new Map([\n    ['vision-encoder-decoder', ['VisionEncoderDecoderModel', VisionEncoderDecoderModel]],\n]);\n\nconst MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES = new Map([\n    ['vision-encoder-decoder', ['VisionEncoderDecoderModel', VisionEncoderDecoderModel]],\n]);\n\nconst MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES = new Map([\n    ['vit', ['ViTForImageClassification', ViTForImageClassification]],\n    ['mobilevit', ['MobileViTForImageClassification', MobileViTForImageClassification]],\n    ['beit', ['BeitForImageClassification', BeitForImageClassification]],\n    ['deit', ['DeiTForImageClassification', DeiTForImageClassification]],\n    ['convnext', ['ConvNextForImageClassification', ConvNextForImageClassification]],\n    ['convnextv2', ['ConvNextV2ForImageClassification', ConvNextV2ForImageClassification]],\n    ['dinov2', ['Dinov2ForImageClassification', Dinov2ForImageClassification]],\n    ['resnet', ['ResNetForImageClassification', ResNetForImageClassification]],\n    ['swin', ['SwinForImageClassification', SwinForImageClassification]],\n]);\n\nconst MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES = new Map([\n    ['detr', ['DetrForObjectDetection', DetrForObjectDetection]],\n    ['yolos', ['YolosForObjectDetection', YolosForObjectDetection]],\n]);\n\nconst MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES = new Map([\n    ['owlvit', ['OwlViTForObjectDetection', OwlViTForObjectDetection]],\n]);\n\nconst MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES = new Map([\n    ['detr', ['DetrForSegmentation', DetrForSegmentation]],\n]);\n\nconst MODEL_FOR_MASK_GENERATION_MAPPING_NAMES = new Map([\n    ['sam', ['SamModel', SamModel]],\n]);\n\nconst MODEL_FOR_CTC_MAPPING_NAMES = new Map([\n    ['wav2vec2', ['Wav2Vec2ForCTC', Wav2Vec2ForCTC]],\n    ['wavlm', ['WavLMForCTC', WavLMForCTC]],\n    ['hubert', ['HubertForCTC', HubertForCTC]],\n]);\n\nconst MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES = new Map([\n    ['wav2vec2', ['Wav2Vec2ForSequenceClassification', Wav2Vec2ForSequenceClassification]],\n    ['wavlm', ['WavLMForSequenceClassification', WavLMForSequenceClassification]],\n    ['hubert', ['HubertForSequenceClassification', HubertForSequenceClassification]],\n    ['audio-spectrogram-transformer', ['ASTForAudioClassification', ASTForAudioClassification]],\n]);\n\nconst MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES = new Map([\n    ['vitmatte', ['VitMatteForImageMatting', VitMatteForImageMatting]],\n]);\n\nconst MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES = new Map([\n    ['swin2sr', ['Swin2SRForImageSuperResolution', Swin2SRForImageSuperResolution]],\n])\n\nconst MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES = new Map([\n    ['dpt', ['DPTForDepthEstimation', DPTForDepthEstimation]],\n    ['glpn', ['GLPNForDepthEstimation', GLPNForDepthEstimation]],\n])\n\n\nconst MODEL_CLASS_TYPE_MAPPING = [\n    [MODEL_MAPPING_NAMES_ENCODER_ONLY, MODEL_TYPES.EncoderOnly],\n    [MODEL_MAPPING_NAMES_ENCODER_DECODER, MODEL_TYPES.EncoderDecoder],\n    [MODEL_MAPPING_NAMES_DECODER_ONLY, MODEL_TYPES.DecoderOnly],\n    [MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES, MODEL_TYPES.Seq2Seq],\n    [MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES, MODEL_TYPES.Seq2Seq],\n    [MODEL_WITH_LM_HEAD_MAPPING_NAMES, MODEL_TYPES.DecoderOnly],\n    [MODEL_FOR_MASKED_LM_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES, MODEL_TYPES.Vision2Seq],\n    [MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_MASK_GENERATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_CTC_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES, MODEL_TYPES.EncoderOnly],\n    [MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES, MODEL_TYPES.Seq2Seq],\n];\n\nfor (const [mappings, type] of MODEL_CLASS_TYPE_MAPPING) {\n    // @ts-ignore\n    for (const [name, model] of mappings.values()) {\n        MODEL_TYPE_MAPPING.set(name, type);\n        MODEL_CLASS_TO_NAME_MAPPING.set(model, name);\n        MODEL_NAME_TO_CLASS_MAPPING.set(name, model);\n    }\n}\n\nconst CUSTOM_MAPPING = [\n    ['CLIPTextModelWithProjection', CLIPTextModelWithProjection, MODEL_TYPES.EncoderOnly],\n    ['CLIPVisionModelWithProjection', CLIPVisionModelWithProjection, MODEL_TYPES.EncoderOnly],\n\n    ['ClapTextModelWithProjection', ClapTextModelWithProjection, MODEL_TYPES.EncoderOnly],\n    ['ClapAudioModelWithProjection', ClapAudioModelWithProjection, MODEL_TYPES.EncoderOnly],\n]\nfor (const [name, model, type] of CUSTOM_MAPPING) {\n    MODEL_TYPE_MAPPING.set(name, type);\n    MODEL_CLASS_TO_NAME_MAPPING.set(model, name);\n    MODEL_NAME_TO_CLASS_MAPPING.set(name, model);\n}\n\n\n/**\n * Helper class which is used to instantiate pretrained models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModel.from_pretrained('bert-base-uncased');\n */\nexport class AutoModel extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_MAPPING_NAMES_ENCODER_ONLY, MODEL_MAPPING_NAMES_ENCODER_DECODER, MODEL_MAPPING_NAMES_DECODER_ONLY];\n    static BASE_IF_FAIL = true;\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence classification models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSequenceClassification.from_pretrained('distilbert-base-uncased-finetuned-sst-2-english');\n */\nexport class AutoModelForSequenceClassification extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained token classification models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForTokenClassification.from_pretrained('Davlan/distilbert-base-multilingual-cased-ner-hrl');\n */\nexport class AutoModelForTokenClassification extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence-to-sequence models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSeq2SeqLM.from_pretrained('t5-small');\n */\nexport class AutoModelForSeq2SeqLM extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence-to-sequence speech-to-text models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForSpeechSeq2Seq.from_pretrained('openai/whisper-tiny.en');\n */\nexport class AutoModelForSpeechSeq2Seq extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained sequence-to-sequence text-to-spectrogram models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForTextToSpectrogram.from_pretrained('microsoft/speecht5_tts');\n */\nexport class AutoModelForTextToSpectrogram extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained causal language models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForCausalLM.from_pretrained('gpt2');\n */\nexport class AutoModelForCausalLM extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_WITH_LM_HEAD_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained masked language models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForMaskedLM.from_pretrained('bert-base-uncased');\n */\nexport class AutoModelForMaskedLM extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_MASKED_LM_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained question answering models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForQuestionAnswering.from_pretrained('distilbert-base-cased-distilled-squad');\n */\nexport class AutoModelForQuestionAnswering extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained vision-to-sequence models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForVision2Seq.from_pretrained('nlpconnect/vit-gpt2-image-captioning');\n */\nexport class AutoModelForVision2Seq extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained image classification models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForImageClassification.from_pretrained('google/vit-base-patch16-224');\n */\nexport class AutoModelForImageClassification extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained image segmentation models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForImageSegmentation.from_pretrained('facebook/detr-resnet-50-panoptic');\n */\nexport class AutoModelForImageSegmentation extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES];\n}\n\n/**\n * Helper class which is used to instantiate pretrained object detection models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForObjectDetection.from_pretrained('facebook/detr-resnet-50');\n */\nexport class AutoModelForObjectDetection extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES];\n}\n\nexport class AutoModelForZeroShotObjectDetection extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES];\n}\n\n\n/**\n * Helper class which is used to instantiate pretrained object detection models with the `from_pretrained` function.\n * The chosen model class is determined by the type specified in the model config.\n * \n * @example\n * let model = await AutoModelForMaskGeneration.from_pretrained('Xenova/sam-vit-base');\n */\nexport class AutoModelForMaskGeneration extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_MASK_GENERATION_MAPPING_NAMES];\n}\n\nexport class AutoModelForCTC extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_CTC_MAPPING_NAMES];\n}\n\nexport class AutoModelForAudioClassification extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES];\n}\n\nexport class AutoModelForDocumentQuestionAnswering extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES];\n}\n\nexport class AutoModelForImageMatting extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES];\n}\n\nexport class AutoModelForImageToImage extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES];\n}\n\nexport class AutoModelForDepthEstimation extends PretrainedMixin {\n    static MODEL_CLASS_MAPPINGS = [MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES];\n}\n\n//////////////////////////////////////////////////\n\n//////////////////////////////////////////////////\nexport class Seq2SeqLMOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits The output logits of the model.\n     * @param {Tensor} output.past_key_values An tensor of key/value pairs that represent the previous state of the model.\n     * @param {Tensor} output.encoder_outputs The output of the encoder in a sequence-to-sequence model.\n     * @param {Tensor} [output.decoder_attentions] Attentions weights of the decoder, after the attention softmax, used to compute the weighted average in the self-attention heads.\n     * @param {Tensor} [output.cross_attentions] Attentions weights of the decoder's cross-attention layer, after the attention softmax, used to compute the weighted average in the cross-attention heads.\n     */\n    constructor({ logits, past_key_values, encoder_outputs, decoder_attentions = null, cross_attentions = null }) {\n        super();\n        this.logits = logits;\n        this.past_key_values = past_key_values;\n        this.encoder_outputs = encoder_outputs;\n        this.decoder_attentions = decoder_attentions;\n        this.cross_attentions = cross_attentions;\n    }\n}\n\n/**\n * Base class for outputs of sentence classification models.\n */\nexport class SequenceClassifierOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits classification (or regression if config.num_labels==1) scores (before SoftMax).\n     */\n    constructor({ logits }) {\n        super();\n        this.logits = logits;\n    }\n}\n\n/**\n * Base class for outputs of token classification models.\n */\nexport class TokenClassifierOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Classification scores (before SoftMax).\n     */\n    constructor({ logits }) {\n        super();\n        this.logits = logits;\n    }\n}\n\n/**\n * Base class for masked language models outputs.\n */\nexport class MaskedLMOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before SoftMax).\n     */\n    constructor({ logits }) {\n        super();\n        this.logits = logits;\n    }\n}\n\n/**\n * Base class for outputs of question answering models.\n */\nexport class QuestionAnsweringModelOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.start_logits Span-start scores (before SoftMax).\n     * @param {Tensor} output.end_logits Span-end scores (before SoftMax).\n     */\n    constructor({ start_logits, end_logits }) {\n        super();\n        this.start_logits = start_logits;\n        this.end_logits = end_logits;\n    }\n}\n\n\n/**\n * Base class for causal language model (or autoregressive) outputs.\n */\nexport class CausalLMOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before softmax).\n     */\n    constructor({ logits }) {\n        super();\n        this.logits = logits;\n    }\n}\n\n/**\n * Base class for causal language model (or autoregressive) outputs.\n */\nexport class CausalLMOutputWithPast extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.logits Prediction scores of the language modeling head (scores for each vocabulary token before softmax).\n     * @param {Tensor} output.past_key_values Contains pre-computed hidden-states (key and values in the self-attention blocks)\n     * that can be used (see `past_key_values` input) to speed up sequential decoding.\n     */\n    constructor({ logits, past_key_values }) {\n        super();\n        this.logits = logits;\n        this.past_key_values = past_key_values;\n    }\n}\n\nexport class ImageMattingOutput extends ModelOutput {\n    /**\n     * @param {Object} output The output of the model.\n     * @param {Tensor} output.alphas Estimated alpha values, of shape `(batch_size, num_channels, height, width)`.\n     */\n    constructor({ alphas }) {\n        super();\n        this.alphas = alphas;\n    }\n}\n","/**\n * @file Pipelines provide a high-level, easy to use, API for running machine learning models.\n * \n * **Example:** Instantiate pipeline using the `pipeline` function.\n * ```javascript\n * import { pipeline } from '@xenova/transformers';\n * \n * let classifier = await pipeline('sentiment-analysis');\n * let output = await classifier('I love transformers!');\n * // [{'label': 'POSITIVE', 'score': 0.999817686}]\n * ```\n * \n * @module pipelines\n */\n\nimport {\n    AutoTokenizer,\n    PreTrainedTokenizer,\n} from './tokenizers.js';\nimport {\n    AutoModel,\n    AutoModelForSequenceClassification,\n    AutoModelForAudioClassification,\n    AutoModelForTokenClassification,\n    AutoModelForQuestionAnswering,\n    AutoModelForMaskedLM,\n    AutoModelForSeq2SeqLM,\n    AutoModelForSpeechSeq2Seq,\n    AutoModelForTextToSpectrogram,\n    AutoModelForCTC,\n    AutoModelForCausalLM,\n    AutoModelForVision2Seq,\n    AutoModelForImageClassification,\n    AutoModelForImageSegmentation,\n    AutoModelForObjectDetection,\n    AutoModelForZeroShotObjectDetection,\n    AutoModelForDocumentQuestionAnswering,\n    AutoModelForImageToImage,\n    AutoModelForDepthEstimation,\n    // AutoModelForTextToWaveform,\n    PreTrainedModel,\n} from './models.js';\nimport {\n    AutoProcessor,\n    Processor\n} from './processors.js';\n\n\nimport {\n    Callable,\n    isString,\n    dispatchCallback,\n    pop,\n    product,\n    get_bounding_box,\n} from './utils/core.js';\nimport {\n    softmax,\n    max,\n    getTopItems,\n    round,\n} from './utils/maths.js';\nimport {\n    read_audio\n} from './utils/audio.js';\nimport {\n    Tensor,\n    mean_pooling,\n    interpolate,\n} from './utils/tensor.js';\nimport { RawImage } from './utils/image.js';\n\n/**\n * Prepare images for further tasks.\n * @param {any[]} images images to prepare.\n * @returns {Promise<any[]>} returns processed images.\n * @private\n */\nasync function prepareImages(images) {\n    if (!Array.isArray(images)) {\n        images = [images];\n    }\n\n    // Possibly convert any non-images to images\n    images = await Promise.all(images.map(x => RawImage.read(x)));\n    return images;\n}\n\n/**\n * The Pipeline class is the class from which all pipelines inherit.\n * Refer to this class for methods shared across different pipelines.\n * @extends Callable\n */\nexport class Pipeline extends Callable {\n    /**\n     * Create a new Pipeline.\n     * @param {Object} options An object containing the following properties:\n     * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.\n     * @param {PreTrainedModel} [options.model] The model to use.\n     * @param {PreTrainedTokenizer} [options.tokenizer=null] The tokenizer to use (if any).\n     * @param {Processor} [options.processor=null] The processor to use (if any).\n     */\n    constructor({ task, model, tokenizer = null, processor = null }) {\n        super();\n        this.task = task;\n        this.model = model;\n        this.tokenizer = tokenizer;\n        this.processor = processor;\n    }\n\n    /**\n     * Disposes the model.\n     * @returns {Promise<void>} A promise that resolves when the model has been disposed.\n     */\n    async dispose() {\n        await this.model.dispose();\n    }\n\n    /**\n     * Executes the task associated with the pipeline.\n     * @param {any} texts The input texts to be processed.\n     * @param {...any} args Additional arguments.\n     * @returns {Promise<any>} A promise that resolves to an array containing the inputs and outputs of the task.\n     */\n    async _call(texts, ...args) {\n        // Run tokenization\n        let model_inputs = this.tokenizer(texts, {\n            padding: true,\n            truncation: true\n        });\n\n        // Run model\n        let outputs = await this.model(model_inputs)\n\n        return [model_inputs, outputs];\n    }\n}\n\n/**\n * Text classification pipeline using any `ModelForSequenceClassification`.\n *\n * **Example:** Sentiment-analysis w/ `Xenova/distilbert-base-uncased-finetuned-sst-2-english`.\n * ```javascript\n * let classifier = await pipeline('sentiment-analysis', 'Xenova/distilbert-base-uncased-finetuned-sst-2-english');\n * let output = await classifier('I love transformers!');\n * // [{ label: 'POSITIVE', score: 0.999788761138916 }]\n * ```\n * \n * **Example:** Multilingual sentiment-analysis w/ `Xenova/bert-base-multilingual-uncased-sentiment` (and return top 5 classes).\n * ```javascript\n * let classifier = await pipeline('sentiment-analysis', 'Xenova/bert-base-multilingual-uncased-sentiment');\n * let output = await classifier('Le meilleur film de tous les temps.', { topk: 5 });\n * // [\n * //   { label: '5 stars', score: 0.9610759615898132 },\n * //   { label: '4 stars', score: 0.03323351591825485 },\n * //   { label: '3 stars', score: 0.0036155181005597115 },\n * //   { label: '1 star', score: 0.0011325967498123646 },\n * //   { label: '2 stars', score: 0.0009423971059732139 }\n * // ]\n * ```\n * \n * **Example:** Toxic comment classification w/ `Xenova/toxic-bert` (and return all classes).\n * ```javascript\n * let classifier = await pipeline('text-classification', 'Xenova/toxic-bert');\n * let output = await classifier('I hate you!', { topk: null });\n * // [\n * //   { label: 'toxic', score: 0.9593140482902527 },\n * //   { label: 'insult', score: 0.16187334060668945 },\n * //   { label: 'obscene', score: 0.03452680632472038 },\n * //   { label: 'identity_hate', score: 0.0223250575363636 },\n * //   { label: 'threat', score: 0.019197041168808937 },\n * //   { label: 'severe_toxic', score: 0.005651099607348442 }\n * // ]\n * ```\n */\nexport class TextClassificationPipeline extends Pipeline {\n    /**\n     * Executes the text classification task.\n     * @param {any} texts The input texts to be classified.\n     * @param {Object} options An optional object containing the following properties:\n     * @param {number} [options.topk=1] The number of top predictions to be returned.\n     * @returns {Promise<Object[]|Object>} A promise that resolves to an array or object containing the predicted labels and scores.\n     */\n    async _call(texts, {\n        topk = 1\n    } = {}) {\n\n        // TODO: Use softmax tensor function\n        let function_to_apply =\n            this.model.config.problem_type === 'multi_label_classification'\n                ? batch => batch.sigmoid().data\n                : batch => softmax(batch.data); // single_label_classification (default)\n\n        let [inputs, outputs] = await super._call(texts);\n\n        let id2label = this.model.config.id2label;\n        let toReturn = [];\n        for (let batch of outputs.logits) {\n            let output = function_to_apply(batch);\n            let scores = getTopItems(output, topk);\n\n            let vals = scores.map(function (x) {\n                return {\n                    label: id2label[x[0]],\n                    score: x[1],\n                }\n            });\n            if (topk === 1) {\n                toReturn.push(...vals);\n            } else {\n                toReturn.push(vals);\n            }\n        }\n\n        return Array.isArray(texts) || topk === 1 ? toReturn : toReturn[0];\n    }\n}\n\n\n/**\n * Named Entity Recognition pipeline using any `ModelForTokenClassification`.\n * \n * **Example:** Perform named entity recognition with `Xenova/bert-base-NER`.\n * ```javascript\n * let classifier = await pipeline('token-classification', 'Xenova/bert-base-NER');\n * let output = await classifier('My name is Sarah and I live in London');\n * // [\n * //   { entity: 'B-PER', score: 0.9980202913284302, index: 4, word: 'Sarah' },\n * //   { entity: 'B-LOC', score: 0.9994474053382874, index: 9, word: 'London' }\n * // ]\n * ```\n * \n * **Example:** Perform named entity recognition with `Xenova/bert-base-NER` (and return all labels).\n * ```javascript\n * let classifier = await pipeline('token-classification', 'Xenova/bert-base-NER');\n * let output = await classifier('Sarah lives in the United States of America', { ignore_labels: [] });\n * // [\n * //   { entity: 'B-PER', score: 0.9966587424278259, index: 1, word: 'Sarah' },\n * //   { entity: 'O', score: 0.9987385869026184, index: 2, word: 'lives' },\n * //   { entity: 'O', score: 0.9990072846412659, index: 3, word: 'in' },\n * //   { entity: 'O', score: 0.9988298416137695, index: 4, word: 'the' },\n * //   { entity: 'B-LOC', score: 0.9995510578155518, index: 5, word: 'United' },\n * //   { entity: 'I-LOC', score: 0.9990395307540894, index: 6, word: 'States' },\n * //   { entity: 'I-LOC', score: 0.9986724853515625, index: 7, word: 'of' },\n * //   { entity: 'I-LOC', score: 0.9975294470787048, index: 8, word: 'America' }\n * // ]\n * ```\n */\nexport class TokenClassificationPipeline extends Pipeline {\n    /**\n     * Executes the token classification task.\n     * @param {any} texts The input texts to be classified.\n     * @param {Object} options An optional object containing the following properties:\n     * @returns {Promise<Object[]|Object>} A promise that resolves to an array or object containing the predicted labels and scores.\n     */\n    async _call(texts, {\n        ignore_labels = ['O'], // TODO init param?\n    } = {}) {\n\n        let isBatched = Array.isArray(texts);\n\n        if (!isBatched) {\n            texts = [texts];\n        }\n\n        let tokenizer = this.tokenizer;\n        let [inputs, outputs] = await super._call(texts);\n\n        let logits = outputs.logits;\n        let id2label = this.model.config.id2label;\n\n        let toReturn = [];\n        for (let i = 0; i < logits.dims[0]; ++i) {\n            let ids = inputs.input_ids[i];\n            let batch = logits[i];\n\n            // List of tokens that aren't ignored\n            let tokens = [];\n            for (let j = 0; j < batch.dims[0]; ++j) {\n                let tokenData = batch[j];\n                let topScoreIndex = max(tokenData.data)[1];\n\n                let entity = id2label[topScoreIndex];\n                if (ignore_labels.includes(entity)) {\n                    // We predicted a token that should be ignored. So, we skip it.\n                    continue;\n                }\n\n                // TODO add option to keep special tokens?\n                let word = tokenizer.decode([ids[j].item()], { skip_special_tokens: true });\n                if (word === '') {\n                    // Was a special token. So, we skip it.\n                    continue;\n                }\n\n                let scores = softmax(tokenData.data);\n\n                tokens.push({\n                    entity: entity,\n                    score: scores[topScoreIndex],\n                    index: j,\n                    word: word,\n\n                    // TODO: null for now, but will add\n                    start: null,\n                    end: null,\n                });\n            }\n            toReturn.push(tokens);\n        }\n        return isBatched ? toReturn : toReturn[0];\n    }\n}\n\n/**\n * @typedef {object} QuestionAnsweringResult\n * @property {string} answer - The answer.\n * @property {number} score - The score.\n */\n\n/**\n * @typedef {Promise<QuestionAnsweringResult|QuestionAnsweringResult[]>} QuestionAnsweringReturnType\n */\n\n/**\n * Question Answering pipeline using any `ModelForQuestionAnswering`.\n * \n * **Example:** Run question answering with `Xenova/distilbert-base-uncased-distilled-squad`.\n * ```javascript\n * let question = 'Who was Jim Henson?';\n * let context = 'Jim Henson was a nice puppet.';\n * \n * let answerer = await pipeline('question-answering', 'Xenova/distilbert-base-uncased-distilled-squad');\n * let output = await answerer(question, context);\n * // {\n * //   \"answer\": \"a nice puppet\",\n * //   \"score\": 0.5768911502526741\n * // }\n * ```\n */\nexport class QuestionAnsweringPipeline extends Pipeline {\n    /**\n     * Executes the question answering task.\n     * @param {string|string[]} question The question(s) to be answered.\n     * @param {string|string[]} context The context(s) where the answer(s) can be found.\n     * @param {Object} options An optional object containing the following properties:\n     * @param {number} [options.topk=1] The number of top answer predictions to be returned.\n     * @returns {QuestionAnsweringReturnType} A promise that resolves to an array or object\n     * containing the predicted answers and scores.\n     */\n    async _call(question, context, {\n        topk = 1\n    } = {}) {\n\n        // Run tokenization\n        let inputs = this.tokenizer(question, {\n            text_pair: context,\n            padding: true,\n            truncation: true\n        });\n\n        let output = await this.model(inputs);\n\n        let toReturn = [];\n        for (let j = 0; j < output.start_logits.dims[0]; ++j) {\n            let ids = inputs.input_ids[j];\n            let sepIndex = ids.indexOf(this.tokenizer.sep_token_id);\n\n            let s1 = Array.from(softmax(output.start_logits[j].data))\n                .map((x, i) => [x, i])\n                .filter(x => x[1] > sepIndex);\n            let e1 = Array.from(softmax(output.end_logits[j].data))\n                .map((x, i) => [x, i])\n                .filter(x => x[1] > sepIndex);\n\n            let options = product(s1, e1)\n                .filter(x => x[0][1] <= x[1][1])\n                .map(x => [x[0][1], x[1][1], x[0][0] * x[1][0]])\n                .sort((a, b) => b[2] - a[2]);\n\n            for (let k = 0; k < Math.min(options.length, topk); ++k) {\n                let [start, end, score] = options[k];\n\n                let answer_tokens = [...ids].slice(start, end + 1)\n\n                let answer = this.tokenizer.decode(answer_tokens, {\n                    skip_special_tokens: true,\n                });\n\n                // TODO add start and end?\n                // NOTE: HF returns character index\n                toReturn.push({\n                    answer, score\n                });\n            }\n        }\n\n        // Mimic HF's return type based on topk\n        return (topk === 1) ? toReturn[0] : toReturn;\n\n    }\n}\n\n/**\n * Masked language modeling prediction pipeline using any `ModelWithLMHead`.\n * \n * **Example:** Perform masked language modelling (a.k.a. \"fill-mask\") with `Xenova/bert-base-uncased`.\n * ```javascript\n * let unmasker = await pipeline('fill-mask', 'Xenova/bert-base-cased');\n * let output = await unmasker('The goal of life is [MASK].');\n * // [\n * //   { token_str: 'survival', score: 0.06137419492006302, token: 8115, sequence: 'The goal of life is survival.' },\n * //   { token_str: 'love', score: 0.03902450203895569, token: 1567, sequence: 'The goal of life is love.' },\n * //   { token_str: 'happiness', score: 0.03253183513879776, token: 9266, sequence: 'The goal of life is happiness.' },\n * //   { token_str: 'freedom', score: 0.018736306577920914, token: 4438, sequence: 'The goal of life is freedom.' },\n * //   { token_str: 'life', score: 0.01859794743359089, token: 1297, sequence: 'The goal of life is life.' }\n * // ]\n * ```\n * \n * **Example:** Perform masked language modelling (a.k.a. \"fill-mask\") with `Xenova/bert-base-cased` (and return top result).\n * ```javascript\n * let unmasker = await pipeline('fill-mask', 'Xenova/bert-base-cased');\n * let output = await unmasker('The Milky Way is a [MASK] galaxy.', { topk: 1 });\n * // [{ token_str: 'spiral', score: 0.6299987435340881, token: 14061, sequence: 'The Milky Way is a spiral galaxy.' }]\n * ```\n */\nexport class FillMaskPipeline extends Pipeline {\n    /**\n     * Fill the masked token in the text(s) given as inputs.\n     * @param {any} texts The masked input texts.\n     * @param {Object} options An optional object containing the following properties:\n     * @param {number} [options.topk=5] The number of top predictions to be returned.\n     * @returns {Promise<Object[]|Object>} A promise that resolves to an array or object containing the predicted tokens and scores.\n     */\n    async _call(texts, {\n        topk = 5\n    } = {}) {\n        // Run tokenization\n        let [inputs, outputs] = await super._call(texts);\n\n        // Determine indices of mask tokens\n        // let mask_token_indices = inputs.input_ids.data.map(x => )\n\n        // let logits = reshape(outputs.logits.data, outputs.logits.dims);\n\n        let tokenizer = this.tokenizer;\n\n        let toReturn = [];\n\n        for (let i = 0; i < inputs.input_ids.dims[0]; ++i) {\n            let ids = inputs.input_ids[i];\n            let mask_token_index = ids.indexOf(this.tokenizer.mask_token_id)\n\n            if (mask_token_index === -1) {\n                throw Error(`Mask token (${tokenizer.mask_token}) not found in text.`)\n            }\n            let logits = outputs.logits[i];\n            let itemLogits = logits[mask_token_index];\n\n            let scores = getTopItems(softmax(itemLogits.data), topk);\n\n            toReturn.push(scores.map(x => {\n                let sequence = [...ids];\n                sequence[mask_token_index] = x[0];\n\n                return {\n                    score: x[1],\n                    token: x[0],\n                    token_str: tokenizer.model.vocab[x[0]],\n                    sequence: tokenizer.decode(sequence, { skip_special_tokens: true }),\n                }\n            }));\n        }\n        return Array.isArray(texts) ? toReturn : toReturn[0];\n    }\n}\n\n/**\n * Text2TextGenerationPipeline class for generating text using a model that performs text-to-text generation tasks.\n * \n * **Example:** Text-to-text generation w/ `Xenova/LaMini-Flan-T5-783M`.\n * ```javascript\n * let generator = await pipeline('text2text-generation', 'Xenova/LaMini-Flan-T5-783M');\n * let output = await generator('how can I become more healthy?', {\n *   max_new_tokens: 100,\n * });\n * // [{ generated_text: \"To become more healthy, you can: 1. Eat a balanced diet with plenty of fruits, vegetables, whole grains, lean proteins, and healthy fats. 2. Stay hydrated by drinking plenty of water. 3. Get enough sleep and manage stress levels. 4. Avoid smoking and excessive alcohol consumption. 5. Regularly exercise and maintain a healthy weight. 6. Practice good hygiene and sanitation. 7. Seek medical attention if you experience any health issues.\" }]\n * ```\n */\nexport class Text2TextGenerationPipeline extends Pipeline {\n    _key = 'generated_text';\n\n    /**\n     * Fill the masked token in the text(s) given as inputs.\n     * @param {string|string[]} texts The text or array of texts to be processed.\n     * @param {Object} [options={}] Options for the fill-mask pipeline.\n     * @param {number} [options.topk=5] The number of top-k predictions to return.\n     * @returns {Promise<any>} An array of objects containing the score, predicted token, predicted token string,\n     * and the sequence with the predicted token filled in, or an array of such arrays (one for each input text).\n     * If only one input text is given, the output will be an array of objects.\n     * @throws {Error} When the mask token is not found in the input text.\n     */\n    async _call(texts, generate_kwargs = {}) {\n        if (!Array.isArray(texts)) {\n            texts = [texts];\n        }\n\n        // Add global prefix, if present\n        if (this.model.config.prefix) {\n            texts = texts.map(x => this.model.config.prefix + x)\n        }\n\n        // Handle task specific params:\n        let task_specific_params = this.model.config.task_specific_params\n        if (task_specific_params && task_specific_params[this.task]) {\n            // Add prefixes, if present\n            if (task_specific_params[this.task].prefix) {\n                texts = texts.map(x => task_specific_params[this.task].prefix + x)\n            }\n\n            // TODO update generation config\n        }\n\n        let tokenizer_options = {\n            padding: true,\n            truncation: true,\n        }\n        let input_ids;\n        if (this instanceof TranslationPipeline && '_build_translation_inputs' in this.tokenizer) {\n            // TODO: move to Translation pipeline?\n            // Currently put here to avoid code duplication\n            // @ts-ignore\n            input_ids = this.tokenizer._build_translation_inputs(texts, tokenizer_options, generate_kwargs).input_ids;\n\n        } else {\n            input_ids = this.tokenizer(texts, tokenizer_options).input_ids;\n        }\n\n        let outputTokenIds = await this.model.generate(input_ids, generate_kwargs);\n\n        /**\n         * @type {any[]}\n         */\n        let toReturn = this.tokenizer.batch_decode(outputTokenIds, {\n            skip_special_tokens: true,\n        });\n        if (this._key !== null) {\n            toReturn = toReturn.map(text => {\n                return (this._key === null) ? text : { [this._key]: text }\n            })\n        }\n        return toReturn\n    }\n}\n\n\n/**\n * A pipeline for summarization tasks, inheriting from Text2TextGenerationPipeline.\n * \n * **Example:** Summarization w/ `Xenova/distilbart-cnn-6-6`.\n * ```javascript\n * let text = 'The tower is 324 metres (1,063 ft) tall, about the same height as an 81-storey building, ' +\n *   'and the tallest structure in Paris. Its base is square, measuring 125 metres (410 ft) on each side. ' +\n *   'During its construction, the Eiffel Tower surpassed the Washington Monument to become the tallest ' +\n *   'man-made structure in the world, a title it held for 41 years until the Chrysler Building in New ' +\n *   'York City was finished in 1930. It was the first structure to reach a height of 300 metres. Due to ' +\n *   'the addition of a broadcasting aerial at the top of the tower in 1957, it is now taller than the ' +\n *   'Chrysler Building by 5.2 metres (17 ft). Excluding transmitters, the Eiffel Tower is the second ' +\n *   'tallest free-standing structure in France after the Millau Viaduct.';\n * \n * let generator = await pipeline('summarization', 'Xenova/distilbart-cnn-6-6');\n * let output = await generator(text, {\n *   max_new_tokens: 100,\n * });\n * // [{ summary_text: ' The Eiffel Tower is about the same height as an 81-storey building and the tallest structure in Paris. It is the second tallest free-standing structure in France after the Millau Viaduct.' }]\n * ```\n */\nexport class SummarizationPipeline extends Text2TextGenerationPipeline {\n    _key = 'summary_text';\n}\n\n/**\n * Translates text from one language to another.\n * \n * **Example:** Multilingual translation w/ `Xenova/nllb-200-distilled-600M`.\n * \n * See [here](https://github.com/facebookresearch/flores/blob/main/flores200/README.md#languages-in-flores-200)\n * for the full list of languages and their corresponding codes.\n * \n * ```javascript\n * let translator = await pipeline('translation', 'Xenova/nllb-200-distilled-600M');\n * let output = await translator('जीवन एक चॉकलेट बॉक्स की तरह है।', {\n *   src_lang: 'hin_Deva', // Hindi\n *   tgt_lang: 'fra_Latn', // French\n * });\n * // [{ translation_text: 'La vie est comme une boîte à chocolat.' }]\n * ```\n * \n * **Example:** Multilingual translation w/ `Xenova/m2m100_418M`.\n * \n * See [here](https://huggingface.co/facebook/m2m100_418M#languages-covered)\n * for the full list of languages and their corresponding codes.\n * \n * ```javascript\n * let translator = await pipeline('translation', 'Xenova/m2m100_418M');\n * let output = await translator('生活就像一盒巧克力。', {\n *   src_lang: 'zh', // Chinese\n *   tgt_lang: 'en', // English\n * });\n * // [{ translation_text: 'Life is like a box of chocolate.' }]\n * ```\n * \n * **Example:** Multilingual translation w/ `Xenova/mbart-large-50-many-to-many-mmt`.\n * \n * See [here](https://huggingface.co/facebook/mbart-large-50-many-to-many-mmt#languages-covered)\n * for the full list of languages and their corresponding codes.\n * \n * ```javascript\n * let translator = await pipeline('translation', 'Xenova/mbart-large-50-many-to-many-mmt');\n * let output = await translator('संयुक्त राष्ट्र के प्रमुख का कहना है कि सीरिया में कोई सैन्य समाधान नहीं है', {\n *   src_lang: 'hi_IN', // Hindi\n *   tgt_lang: 'fr_XX', // French\n * });\n * // [{ translation_text: 'Le chef des Nations affirme qu 'il n 'y a military solution in Syria.' }]\n * ```\n */\nexport class TranslationPipeline extends Text2TextGenerationPipeline {\n    _key = 'translation_text';\n}\n\n/**\n * Language generation pipeline using any `ModelWithLMHead` or `ModelForCausalLM`.\n * This pipeline predicts the words that will follow a specified text prompt.\n * NOTE: For the full list of generation parameters, see [`GenerationConfig`](./utils/generation#module_utils/generation.GenerationConfig).\n * \n * **Example:** Text generation with `Xenova/distilgpt2` (default settings).\n * ```javascript\n * let text = 'I enjoy walking with my cute dog,';\n * let generator = await pipeline('text-generation', 'Xenova/distilgpt2');\n * let output = await generator(text);\n * // [{ generated_text: \"I enjoy walking with my cute dog, and I love to play with the other dogs.\" }]\n * ```\n * \n * **Example:** Text generation with `Xenova/distilgpt2` (custom settings).\n * ```javascript\n * let text = 'Once upon a time, there was';\n * let generator = await pipeline('text-generation', 'Xenova/distilgpt2');\n * let output = await generator(text, {\n *   temperature: 2,\n *   max_new_tokens: 10,\n *   repetition_penalty: 1.5,\n *   no_repeat_ngram_size: 2,\n *   num_beams: 2,\n *   num_return_sequences: 2,\n * });\n * // [{\n * //   \"generated_text\": \"Once upon a time, there was an abundance of information about the history and activities that\"\n * // }, {\n * //   \"generated_text\": \"Once upon a time, there was an abundance of information about the most important and influential\"\n * // }]\n * ```\n * \n * **Example:** Run code generation with `Xenova/codegen-350M-mono`.\n * ```javascript\n * let text = 'def fib(n):';\n * let generator = await pipeline('text-generation', 'Xenova/codegen-350M-mono');\n * let output = await generator(text, {\n *   max_new_tokens: 44,\n * });\n * // [{\n * //   generated_text: 'def fib(n):\\n' +\n * //     '    if n == 0:\\n' +\n * //     '        return 0\\n' +\n * //     '    elif n == 1:\\n' +\n * //     '        return 1\\n' +\n * //     '    else:\\n' +\n * //     '        return fib(n-1) + fib(n-2)\\n'\n * // }]\n * ```\n */\nexport class TextGenerationPipeline extends Pipeline {\n    /**\n     * Generates text based on an input prompt.\n     * @param {any} texts The input prompt or prompts to generate text from.\n     * @param {Object} [generate_kwargs={}] Additional arguments for text generation.\n     * @returns {Promise<any>} The generated text or texts.\n     */\n    async _call(texts, generate_kwargs = {}) {\n        let stringInput = typeof texts === 'string' || texts instanceof String;\n        if (stringInput) {\n            texts = [texts];\n        }\n\n        // By default, do not add special tokens\n        const add_special_tokens = generate_kwargs.add_special_tokens ?? false;\n\n        this.tokenizer.padding_side = 'left';\n        let inputs = this.tokenizer(texts, {\n            add_special_tokens,\n            padding: true,\n            truncation: true,\n        });\n\n        let input_ids = inputs.input_ids;\n        let attention_mask = inputs.attention_mask;\n\n        let outputTokenIds = await this.model.generate(input_ids, generate_kwargs, null, {\n            inputs_attention_mask: attention_mask\n        });\n\n        const decoded = this.tokenizer.batch_decode(outputTokenIds, {\n            skip_special_tokens: true,\n        });\n        const toReturn = Array.from({ length: texts.length }, _ => []);\n        for (let i = 0; i < decoded.length; ++i) {\n            const textIndex = Math.floor(i / outputTokenIds.length * texts.length);\n\n            toReturn[textIndex].push({\n                generated_text: decoded[i]\n            });\n        }\n        return (stringInput && toReturn.length === 1) ? toReturn[0] : toReturn;\n    }\n}\n\n/**\n * NLI-based zero-shot classification pipeline using a `ModelForSequenceClassification`\n * trained on NLI (natural language inference) tasks. Equivalent of `text-classification`\n * pipelines, but these models don't require a hardcoded number of potential classes, they\n * can be chosen at runtime. It usually means it's slower but it is **much** more flexible.\n * \n * **Example:** Zero shot classification with `Xenova/mobilebert-uncased-mnli`.\n * ```javascript\n * let text = 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.';\n * let labels = [ 'mobile', 'billing', 'website', 'account access' ];\n * let classifier = await pipeline('zero-shot-classification', 'Xenova/mobilebert-uncased-mnli');\n * let output = await classifier(text, labels);\n * // {\n * //   sequence: 'Last week I upgraded my iOS version and ever since then my phone has been overheating whenever I use your app.',\n * //   labels: [ 'mobile', 'website', 'billing', 'account access' ],\n * //   scores: [ 0.5562091040482018, 0.1843621307860853, 0.13942646639336376, 0.12000229877234923 ]\n * // }\n * ```\n * \n * **Example:** Zero shot classification with `Xenova/nli-deberta-v3-xsmall` (multi-label).\n * ```javascript\n * let text = 'I have a problem with my iphone that needs to be resolved asap!';\n * let labels = [ 'urgent', 'not urgent', 'phone', 'tablet', 'computer' ];\n * let classifier = await pipeline('zero-shot-classification', 'Xenova/nli-deberta-v3-xsmall');\n * let output = await classifier(text, labels, { multi_label: true });\n * // {\n * //   sequence: 'I have a problem with my iphone that needs to be resolved asap!',\n * //   labels: [ 'urgent', 'phone', 'computer', 'tablet', 'not urgent' ],\n * //   scores: [ 0.9958870956360275, 0.9923963400697035, 0.002333537946160235, 0.0015134138567598765, 0.0010699384208377163 ]\n * // }\n * ```\n */\nexport class ZeroShotClassificationPipeline extends Pipeline {\n\n    /**\n     * Create a new ZeroShotClassificationPipeline.\n     * @param {Object} options An object containing the following properties:\n     * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.\n     * @param {PreTrainedModel} [options.model] The model to use.\n     * @param {PreTrainedTokenizer} [options.tokenizer] The tokenizer to use.\n     */\n    constructor(options) {\n        super(options);\n\n        // Use model config to get label2id mapping\n        this.label2id = Object.fromEntries(\n            Object.entries(this.model.config.label2id).map(\n                ([k, v]) => [k.toLowerCase(), v]\n            )\n        );\n\n        this.entailment_id = this.label2id['entailment'];\n        if (this.entailment_id === undefined) {\n            console.warn(\"Could not find 'entailment' in label2id mapping. Using 2 as entailment_id.\");\n            this.entailment_id = 2;\n        }\n\n        this.contradiction_id = this.label2id['contradiction'] ?? this.label2id['not_entailment'];\n        if (this.contradiction_id === undefined) {\n            console.warn(\"Could not find 'contradiction' in label2id mapping. Using 0 as contradiction_id.\");\n            this.contradiction_id = 0;\n        }\n    }\n    /**\n     * @param {any[]} texts\n     * @param {string[]} candidate_labels\n     * @param {Object} options Additional options:\n     * @param {string} [options.hypothesis_template=\"This example is {}.\"] The template used to turn each\n     * candidate label into an NLI-style hypothesis. The candidate label will replace the {} placeholder.\n     * @param {boolean} [options.multi_label=false] Whether or not multiple candidate labels can be true.\n     * If `false`, the scores are normalized such that the sum of the label likelihoods for each sequence\n     * is 1. If `true`, the labels are considered independent and probabilities are normalized for each\n     * candidate by doing a softmax of the entailment score vs. the contradiction score.\n     * @return {Promise<Object|Object[]>} The prediction(s), as a map (or list of maps) from label to score.\n     */\n    async _call(texts, candidate_labels, {\n        hypothesis_template = \"This example is {}.\",\n        multi_label = false,\n    } = {}) {\n\n        let isBatched = Array.isArray(texts);\n\n        if (!isBatched) {\n            texts = [texts];\n        }\n        if (!Array.isArray(candidate_labels)) {\n            candidate_labels = [candidate_labels];\n        }\n\n        // Insert labels into hypothesis template\n        let hypotheses = candidate_labels.map(\n            x => hypothesis_template.replace('{}', x)\n        );\n\n        // How to perform the softmax over the logits:\n        //  - true:  softmax over the entailment vs. contradiction dim for each label independently\n        //  - false: softmax the \"entailment\" logits over all candidate labels\n        let softmaxEach = multi_label || candidate_labels.length === 1;\n\n        let toReturn = [];\n        for (let premise of texts) {\n            let entails_logits = [];\n\n            for (let hypothesis of hypotheses) {\n                let inputs = this.tokenizer(premise, {\n                    text_pair: hypothesis,\n                    padding: true,\n                    truncation: true,\n                })\n                let outputs = await this.model(inputs)\n\n                if (softmaxEach) {\n                    entails_logits.push([\n                        outputs.logits.data[this.contradiction_id],\n                        outputs.logits.data[this.entailment_id]\n                    ])\n                } else {\n                    entails_logits.push(outputs.logits.data[this.entailment_id])\n                }\n            }\n\n            let scores;\n            if (softmaxEach) {\n                scores = entails_logits.map(x => softmax(x)[1]);\n            } else {\n                scores = softmax(entails_logits);\n            }\n\n            // Sort by scores (desc) and return scores with indices\n            let scores_sorted = scores\n                .map((x, i) => [x, i])\n                .sort((a, b) => {\n                    return b[0] - a[0];\n                });\n\n            toReturn.push({\n                sequence: premise,\n                labels: scores_sorted.map(x => candidate_labels[x[1]]),\n                scores: scores_sorted.map(x => x[0]),\n            });\n        }\n        return isBatched ? toReturn : toReturn[0];\n    }\n}\n\n\n/**\n * Feature extraction pipeline using no model head. This pipeline extracts the hidden\n * states from the base transformer, which can be used as features in downstream tasks.\n * \n * **Example:** Run feature extraction with `bert-base-uncased` (without pooling/normalization).\n * ```javascript\n * let extractor = await pipeline('feature-extraction', 'Xenova/bert-base-uncased', { revision: 'default' });\n * let output = await extractor('This is a simple test.');\n * // Tensor {\n * //   type: 'float32',\n * //   data: Float32Array [0.05939924716949463, 0.021655935794115067, ...],\n * //   dims: [1, 8, 768]\n * // }\n * ```\n * \n * **Example:** Run feature extraction with `bert-base-uncased` (with pooling/normalization).\n * ```javascript\n * let extractor = await pipeline('feature-extraction', 'Xenova/bert-base-uncased', { revision: 'default' });\n * let output = await extractor('This is a simple test.', { pooling: 'mean', normalize: true });\n * // Tensor {\n * //   type: 'float32',\n * //   data: Float32Array [0.03373778983950615, -0.010106077417731285, ...],\n * //   dims: [1, 768]\n * // }\n * ```\n * \n * **Example:** Calculating embeddings with `sentence-transformers` models.\n * ```javascript\n * let extractor = await pipeline('feature-extraction', 'Xenova/all-MiniLM-L6-v2');\n * let output = await extractor('This is a simple test.', { pooling: 'mean', normalize: true });\n * // Tensor {\n * //   type: 'float32',\n * //   data: Float32Array [0.09094982594251633, -0.014774246141314507, ...],\n * //   dims: [1, 384]\n * // }\n * ```\n */\nexport class FeatureExtractionPipeline extends Pipeline {\n\n    /**\n     * Extract the features of the input(s).\n     * \n     * @param {string|string[]} texts The input texts\n     * @param {Object} options Additional options:\n     * @param {string} [options.pooling=\"none\"] The pooling method to use. Can be one of: \"none\", \"mean\".\n     * @param {boolean} [options.normalize=false] Whether or not to normalize the embeddings in the last dimension.\n     * @returns The features computed by the model.\n     */\n    async _call(texts, {\n        pooling = 'none',\n        normalize = false,\n    } = {}) {\n        let [inputs, outputs] = await super._call(texts);\n\n        // TODO: Provide warning to the user that they might be using model which was not exported\n        // specifically for feature extraction\n        // console.log(this.model.config)\n        // console.log(outputs)\n\n        let result = outputs.last_hidden_state ?? outputs.logits;\n        if (pooling === 'none') {\n            // Skip pooling\n        } else if (pooling === 'mean') {\n            result = mean_pooling(result, inputs.attention_mask);\n        } else if (pooling === 'cls') {\n            result = result.slice(null, 0);\n        } else {\n            throw Error(`Pooling method '${pooling}' not supported.`);\n        }\n\n        if (normalize) {\n            result = result.normalize(2, -1);\n        }\n\n        return result;\n    }\n}\n\n// TODO\n// export class SentenceSimilarityPipeline extends Pipeline {\n// }\n\n\n/**\n * Audio classification pipeline using any `AutoModelForAudioClassification`.\n * This pipeline predicts the class of a raw waveform or an audio file.\n * \n * **Example:** Perform audio classification with `Xenova/wav2vec2-large-xlsr-53-gender-recognition-librispeech`.\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav';\n * let classifier = await pipeline('audio-classification', 'Xenova/wav2vec2-large-xlsr-53-gender-recognition-librispeech');\n * let output = await classifier(url);\n * // [\n * //   { label: 'male', score: 0.9981542229652405 },\n * //   { label: 'female', score: 0.001845747814513743 }\n * // ]\n * ```\n * \n * **Example:** Perform audio classification with `Xenova/ast-finetuned-audioset-10-10-0.4593` and return top 4 results.\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/cat_meow.wav';\n * let classifier = await pipeline('audio-classification', 'Xenova/ast-finetuned-audioset-10-10-0.4593');\n * let output = await classifier(url, { topk: 4 });\n * // [\n * //   { label: 'Meow', score: 0.5617874264717102 },\n * //   { label: 'Cat', score: 0.22365376353263855 },\n * //   { label: 'Domestic animals, pets', score: 0.1141069084405899 },\n * //   { label: 'Animal', score: 0.08985692262649536 },\n * // ]\n * ```\n */\nexport class AudioClassificationPipeline extends Pipeline {\n\n    /**\n     * Create a new AudioClassificationPipeline.\n     * @param {Object} options An object containing the following properties:\n     * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.\n     * @param {PreTrainedModel} [options.model] The model to use.\n     * @param {Processor} [options.processor] The processor to use.\n     */\n    constructor(options) {\n        super(options);\n    }\n\n    /**\n     * Preprocesses the input audio for the AutomaticSpeechRecognitionPipeline.\n     * @param {any} audio The audio to be preprocessed.\n     * @param {number} sampling_rate The sampling rate of the audio.\n     * @returns {Promise<Float32Array>} A promise that resolves to the preprocessed audio data.\n     * @private\n     */\n    async _preprocess(audio, sampling_rate) {\n        if (isString(audio)) {\n            audio = await read_audio(audio, sampling_rate);\n        }\n\n        return audio;\n    }\n\n    /**\n     * Executes the audio classification task.\n     * @param {any} audio The input audio files to be classified.\n     * @param {Object} options An optional object containing the following properties:\n     * @param {number} [options.topk=5] The number of top predictions to be returned.\n     * @returns {Promise<Object[]|Object>} A promise that resolves to an array or object containing the predicted labels and scores.\n     */\n    async _call(audio, {\n        topk = 5\n    } = {}) {\n\n        let single = !Array.isArray(audio);\n        if (single) {\n            // @ts-ignore\n            audio = [audio];\n        }\n\n        const id2label = this.model.config.id2label;\n        const sampling_rate = this.processor.feature_extractor.config.sampling_rate;\n\n        let toReturn = [];\n        for (let aud of audio) {\n            aud = await this._preprocess(aud, sampling_rate)\n\n            const inputs = await this.processor(aud);\n            const output = await this.model(inputs);\n            const logits = output.logits[0];\n\n            let scores = getTopItems(softmax(logits.data), topk);\n\n            let vals = scores.map(function (x) {\n                return {\n                    label: id2label[x[0]],\n                    score: x[1],\n                }\n            });\n            if (topk === 1) {\n                toReturn.push(...vals);\n            } else {\n                toReturn.push(vals);\n            }\n        }\n        return !single || topk === 1 ? toReturn : toReturn[0];\n    }\n}\n\n/**\n * Zero shot audio classification pipeline using `ClapModel`. This pipeline predicts the class of an audio when you\n * provide an audio and a set of `candidate_labels`.\n * \n * **Example**: Perform zero-shot audio classification with `Xenova/clap-htsat-unfused`.\n * ```javascript\n * let classifier = await pipeline('zero-shot-audio-classification', 'Xenova/clap-htsat-unfused');\n * let audio = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/dog_barking.wav';\n * let candidate_labels = ['dog', 'vaccum cleaner'];\n * let scores = await classifier(audio, candidate_labels);\n * // [\n * //   { score: 0.9993992447853088, label: 'dog' },\n * //   { score: 0.0006007603369653225, label: 'vaccum cleaner' }\n * // ]\n * ```\n */\nexport class ZeroShotAudioClassificationPipeline extends Pipeline {\n\n    /**\n     * Create a new ZeroShotAudioClassificationPipeline.\n     * @param {Object} options An object containing the following properties:\n     * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.\n     * @param {PreTrainedModel} [options.model] The model to use.\n     * @param {PreTrainedTokenizer} [options.tokenizer] The tokenizer to use.\n     * @param {Processor} [options.processor] The processor to use.\n     */\n    constructor(options) {\n        super(options);\n    }\n\n    /**\n     * Preprocesses the input audio for the ZeroShotAudioClassificationPipeline.\n     * @param {any} audio The audio to be preprocessed.\n     * @param {number} sampling_rate The sampling rate of the audio.\n     * @returns {Promise<Float32Array>} A promise that resolves to the preprocessed audio data.\n     * @private\n     */\n    async _preprocess(audio, sampling_rate) {\n        if (isString(audio)) {\n            audio = await read_audio(audio, sampling_rate);\n        }\n\n        return audio;\n    }\n\n    /**\n     * Assign labels to the audio(s) passed as inputs.\n     * @param {Array} audios The input audios.\n     * @param {string[]} candidate_labels The candidate labels for this audio\n     * @param {Object} options The options for the classification.\n     * @param {string} [options.hypothesis_template] The sentence used in cunjunction with *candidate_labels* to attempt\n     * the audio classification by replacing the placeholder with the candidate_labels.\n     * Then likelihood is estimated by using logits_per_audio.\n     * @returns {Promise<any>}\n     */\n    async _call(audios, candidate_labels, {\n        hypothesis_template = \"This is a sound of {}.\"\n    } = {}) {\n        const single = !Array.isArray(audios);\n        if (single) {\n            // @ts-ignore\n            audios = [audios];\n        }\n\n        // Insert label into hypothesis template \n        const texts = candidate_labels.map(\n            x => hypothesis_template.replace('{}', x)\n        );\n\n        // Run tokenization\n        const text_inputs = this.tokenizer(texts, {\n            padding: true,\n            truncation: true,\n        });\n\n        const sampling_rate = this.processor.feature_extractor.config.sampling_rate;\n\n        const toReturn = [];\n        for (let audio of audios) {\n            audio = await this._preprocess(audio, sampling_rate)\n\n            const audio_inputs = await this.processor(audio);\n\n            // Run model with both text and audio inputs\n            const output = await this.model({ ...text_inputs, ...audio_inputs });\n\n            // Compute softmax per audio\n            const probs = softmax(output.logits_per_audio.data);\n\n            toReturn.push([...probs].map((x, i) => {\n                return {\n                    score: x,\n                    label: candidate_labels[i]\n                }\n            }));\n        }\n        return !single ? toReturn : toReturn[0];\n    }\n}\n\n/**\n * Pipeline that aims at extracting spoken text contained within some audio.\n *\n * **Example:** Transcribe English.\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav';\n * let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en');\n * let output = await transcriber(url);\n * // { text: \" And so my fellow Americans ask not what your country can do for you, ask what you can do for your country.\" }\n * ```\n * \n * **Example:** Transcribe English w/ timestamps.\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav';\n * let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en');\n * let output = await transcriber(url, { return_timestamps: true });\n * // {\n * //   text: \" And so my fellow Americans ask not what your country can do for you, ask what you can do for your country.\"\n * //   chunks: [\n * //     { timestamp: [0, 8],  text: \" And so my fellow Americans ask not what your country can do for you\" }\n * //     { timestamp: [8, 11], text: \" ask what you can do for your country.\" }\n * //   ]\n * // }\n * ```\n * \n * **Example:** Transcribe English w/ word-level timestamps.\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/jfk.wav';\n * let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en');\n * let output = await transcriber(url, { return_timestamps: 'word' });\n * // {\n * //   \"text\": \" And so my fellow Americans ask not what your country can do for you ask what you can do for your country.\",\n * //   \"chunks\": [\n * //     { \"text\": \" And\", \"timestamp\": [0, 0.78] },\n * //     { \"text\": \" so\", \"timestamp\": [0.78, 1.06] },\n * //     { \"text\": \" my\", \"timestamp\": [1.06, 1.46] },\n * //     ...\n * //     { \"text\": \" for\", \"timestamp\": [9.72, 9.92] },\n * //     { \"text\": \" your\", \"timestamp\": [9.92, 10.22] },\n * //     { \"text\": \" country.\", \"timestamp\": [10.22, 13.5] }\n * //   ]\n * // }\n * ```\n * \n * **Example:** Transcribe French.\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/french-audio.mp3';\n * let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-small');\n * let output = await transcriber(url, { language: 'french', task: 'transcribe' });\n * // { text: \" J'adore, j'aime, je n'aime pas, je déteste.\" }\n * ```\n * \n * **Example:** Translate French to English.\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/french-audio.mp3';\n * let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-small');\n * let output = await transcriber(url, { language: 'french', task: 'translate' });\n * // { text: \" I love, I like, I don't like, I hate.\" }\n * ```\n * \n * **Example:** Transcribe/translate audio longer than 30 seconds.\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/ted_60.wav';\n * let transcriber = await pipeline('automatic-speech-recognition', 'Xenova/whisper-tiny.en');\n * let output = await transcriber(url, { chunk_length_s: 30, stride_length_s: 5 });\n * // { text: \" So in college, I was a government major, which means [...] So I'd start off light and I'd bump it up\" }\n * ```\n */\nexport class AutomaticSpeechRecognitionPipeline extends Pipeline {\n\n    /**\n     * Create a new AutomaticSpeechRecognitionPipeline.\n     * @param {Object} options An object containing the following properties:\n     * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.\n     * @param {PreTrainedModel} [options.model] The model to use.\n     * @param {PreTrainedTokenizer} [options.tokenizer] The tokenizer to use.\n     * @param {Processor} [options.processor] The processor to use.\n     */\n    constructor(options) {\n        super(options);\n    }\n\n    /**\n     * Preprocesses the input audio for the AutomaticSpeechRecognitionPipeline.\n     * @param {any} audio The audio to be preprocessed.\n     * @param {number} sampling_rate The sampling rate of the audio.\n     * @returns {Promise<Float32Array>} A promise that resolves to the preprocessed audio data.\n     * @private\n     */\n    async _preprocess(audio, sampling_rate) {\n        if (isString(audio)) {\n            audio = await read_audio(audio, sampling_rate);\n        }\n\n        return audio;\n    }\n\n    /**\n     * @typedef {{stride: number[], input_features: import('./utils/tensor.js').Tensor, is_last: boolean, tokens?: number[], token_timestamps?: number[]}} Chunk\n     * \n     * @callback ChunkCallback\n     * @param {Chunk} chunk The chunk to process.\n     */\n\n    /**\n     * Asynchronously processes audio and generates text transcription using the model.\n     * @param {Float32Array|Float32Array[]} audio The audio to be transcribed. Can be a single Float32Array or an array of Float32Arrays.\n     * @param {Object} [kwargs={}] Optional arguments.\n     * @param {boolean|'word'} [kwargs.return_timestamps] Whether to return timestamps or not. Default is `false`.\n     * @param {number} [kwargs.chunk_length_s] The length of audio chunks to process in seconds. Default is 0 (no chunking).\n     * @param {number} [kwargs.stride_length_s] The length of overlap between consecutive audio chunks in seconds. If not provided, defaults to `chunk_length_s / 6`.\n     * @param {ChunkCallback} [kwargs.chunk_callback] Callback function to be called with each chunk processed.\n     * @param {boolean} [kwargs.force_full_sequences] Whether to force outputting full sequences or not. Default is `false`.\n     * @param {string} [kwargs.language] The source language. Default is `null`, meaning it should be auto-detected. Use this to potentially improve performance if the source language is known.\n     * @param {string} [kwargs.task] The task to perform. Default is `null`, meaning it should be auto-detected.\n     * @param {number[][]} [kwargs.forced_decoder_ids] A list of pairs of integers which indicates a mapping from generation indices to token indices\n     * that will be forced before sampling. For example, [[1, 123]] means the second generated token will always be a token of index 123.\n     * @returns {Promise<Object>} A Promise that resolves to an object containing the transcription text and optionally timestamps if `return_timestamps` is `true`.\n     */\n    async _call(audio, kwargs = {}) {\n        switch (this.model.config.model_type) {\n            case 'whisper':\n                return this._call_whisper(audio, kwargs)\n            case 'wav2vec2':\n            case 'hubert':\n                return this._call_wav2vec2(audio, kwargs)\n            default:\n                throw new Error(`AutomaticSpeechRecognitionPipeline does not support model type '${this.model.config.model_type}'.`)\n        }\n    }\n\n    /** @private */\n    async _call_wav2vec2(audio, kwargs = {}) {\n        // TODO use kwargs\n\n        if (kwargs.language) {\n            console.warn('`language` parameter is not yet supported for `wav2vec2` models, defaulting to \"English\".');\n        }\n        if (kwargs.task) {\n            console.warn('`task` parameter is not yet supported for `wav2vec2` models, defaulting to \"transcribe\".');\n        }\n\n        let single = !Array.isArray(audio);\n        if (single) {\n            // @ts-ignore\n            audio = [audio];\n        }\n\n        const sampling_rate = this.processor.feature_extractor.config.sampling_rate;\n\n        let toReturn = [];\n        for (let aud of audio) {\n            aud = await this._preprocess(aud, sampling_rate)\n\n            const inputs = await this.processor(aud);\n            const output = await this.model(inputs);\n            const logits = output.logits[0];\n\n            const predicted_ids = [];\n            for (let item of logits) {\n                predicted_ids.push(max(item.data)[1])\n            }\n            const predicted_sentences = this.tokenizer.decode(predicted_ids)\n            toReturn.push({ text: predicted_sentences })\n        }\n        return single ? toReturn[0] : toReturn;\n    }\n\n    /** @private */\n    async _call_whisper(audio, kwargs = {}) {\n        let return_timestamps = kwargs.return_timestamps ?? false;\n        let chunk_length_s = kwargs.chunk_length_s ?? 0;\n        let stride_length_s = kwargs.stride_length_s ?? null;\n        let chunk_callback = kwargs.chunk_callback ?? null;\n        let force_full_sequences = kwargs.force_full_sequences ?? false;\n\n        if (return_timestamps === 'word') {\n            kwargs['return_token_timestamps'] = true;\n        }\n\n        let language = pop(kwargs, 'language', null);\n        let task = pop(kwargs, 'task', null);\n\n        if (language || task || return_timestamps) {\n            if (kwargs.forced_decoder_ids) {\n                throw new Error(\"Cannot specify `language`/`task`/`return_timestamps` and `forced_decoder_ids` at the same time.\")\n            }\n            // @ts-ignore\n            let decoder_prompt_ids = this.tokenizer.get_decoder_prompt_ids({ language, task, no_timestamps: !return_timestamps })\n            if (decoder_prompt_ids.length > 0) {\n                kwargs.forced_decoder_ids = decoder_prompt_ids;\n            }\n        }\n\n        let single = !Array.isArray(audio);\n        if (single) {\n            // @ts-ignore\n            audio = [audio];\n        }\n\n        const sampling_rate = this.processor.feature_extractor.config.sampling_rate;\n        const time_precision = this.processor.feature_extractor.config.chunk_length / this.model.config.max_source_positions;\n        const hop_length = this.processor.feature_extractor.config.hop_length;\n\n        let toReturn = [];\n        for (let aud of audio) {\n            aud = await this._preprocess(aud, sampling_rate)\n\n            /** @type {Chunk[]} */\n            let chunks = [];\n            if (chunk_length_s > 0) {\n                if (stride_length_s === null) {\n                    stride_length_s = chunk_length_s / 6;\n                } else if (chunk_length_s <= stride_length_s) {\n                    throw Error(\"`chunk_length_s` must be larger than `stride_length_s`.\")\n                }\n\n                // TODO support different stride_length_s (for left and right)\n\n                const window = sampling_rate * chunk_length_s;\n                const stride = sampling_rate * stride_length_s;\n                const jump = window - 2 * stride;\n                let offset = 0;\n\n                // Create subarrays of audio with overlaps\n\n                while (offset < aud.length) {\n                    let subarr = aud.subarray(offset, offset + window);\n                    let feature = await this.processor(subarr);\n\n                    let isFirst = offset === 0;\n                    let isLast = offset + jump >= aud.length;\n                    chunks.push({\n                        stride: [\n                            subarr.length,\n                            isFirst ? 0 : stride,\n                            isLast ? 0 : stride\n                        ],\n                        input_features: feature.input_features,\n                        is_last: isLast\n                    })\n                    offset += jump;\n                }\n\n            } else {\n                chunks = [{\n                    stride: [aud.length, 0, 0],\n                    input_features: (await this.processor(aud)).input_features,\n                    is_last: true\n                }]\n            }\n\n            // Generate for each set of input features\n            for (let chunk of chunks) {\n                kwargs.num_frames = Math.floor(chunk.stride[0] / hop_length);\n\n                // NOTE: doing sequentially for now\n                let data = await this.model.generate(chunk.input_features, kwargs);\n\n                // TODO: Right now we only get top beam\n                if (return_timestamps === 'word') {\n                    chunk.tokens = data.sequences[0];\n                    chunk.token_timestamps = data.token_timestamps.tolist()[0].map(\n                        x => round(x, 2)\n                    );\n\n                } else {\n                    chunk.tokens = data[0];\n                }\n\n                // convert stride to seconds\n                chunk.stride = chunk.stride.map(x => x / sampling_rate);\n\n                if (chunk_callback !== null) {\n                    chunk_callback(chunk)\n                }\n            }\n\n            // Merge text chunks\n            // @ts-ignore\n            let [full_text, optional] = this.tokenizer._decode_asr(chunks, {\n                time_precision, return_timestamps, force_full_sequences\n            });\n\n            toReturn.push({ text: full_text, ...optional })\n        }\n        return single ? toReturn[0] : toReturn;\n    }\n}\n\n/**\n * Image To Text pipeline using a `AutoModelForVision2Seq`. This pipeline predicts a caption for a given image.\n * \n * **Example:** Generate a caption for an image w/ `Xenova/vit-gpt2-image-captioning`.\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/cats.jpg';\n * let captioner = await pipeline('image-to-text', 'Xenova/vit-gpt2-image-captioning');\n * let output = await captioner(url);\n * // [{ generated_text: 'a cat laying on a couch with another cat' }]\n * ```\n * \n * **Example:** Optical Character Recognition (OCR) w/ `Xenova/trocr-small-handwritten`.\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/handwriting.jpg';\n * let captioner = await pipeline('image-to-text', 'Xenova/trocr-small-handwritten');\n * let output = await captioner(url);\n * // [{ generated_text: 'Mr. Brown commented icily.' }]\n * ```\n */\nexport class ImageToTextPipeline extends Pipeline {\n    /**\n     * Create a new ImageToTextPipeline.\n     * @param {Object} options An object containing the following properties:\n     * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.\n     * @param {PreTrainedModel} [options.model] The model to use.\n     * @param {PreTrainedTokenizer} [options.tokenizer] The tokenizer to use.\n     * @param {Processor} [options.processor] The processor to use.\n     */\n    constructor(options) {\n        super(options);\n    }\n\n    /**\n     * Assign labels to the image(s) passed as inputs.\n     * @param {any[]} images The images to be captioned.\n     * @param {Object} [generate_kwargs={}] Optional generation arguments.\n     * @returns {Promise<Object|Object[]>} A Promise that resolves to an object (or array of objects) containing the generated text(s).\n     */\n    async _call(images, generate_kwargs = {}) {\n        let isBatched = Array.isArray(images);\n\n        images = await prepareImages(images);\n\n        let { pixel_values } = await this.processor(images);\n\n        let toReturn = [];\n        for (let batch of pixel_values) {\n            batch.dims = [1, ...batch.dims]\n            let output = await this.model.generate(batch, generate_kwargs);\n            let decoded = this.tokenizer.batch_decode(output, {\n                skip_special_tokens: true,\n            }).map(x => {\n                return { generated_text: x.trim() }\n            })\n            toReturn.push(decoded);\n        }\n\n        return isBatched ? toReturn : toReturn[0];\n    }\n}\n\n/**\n * Image classification pipeline using any `AutoModelForImageClassification`.\n * This pipeline predicts the class of an image.\n * \n * **Example:** Classify an image.\n * ```javascript\n * let classifier = await pipeline('image-classification', 'Xenova/vit-base-patch16-224');\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/tiger.jpg';\n * let output = await classifier(url);\n * // [\n * //   {label: 'tiger, Panthera tigris', score: 0.632695734500885},\n * // ]\n * ```\n * \n * **Example:** Classify an image and return top `n` classes.\n * ```javascript\n * let classifier = await pipeline('image-classification', 'Xenova/vit-base-patch16-224');\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/tiger.jpg';\n * let output = await classifier(url, { topk: 3 });\n * // [\n * //   { label: 'tiger, Panthera tigris', score: 0.632695734500885 },\n * //   { label: 'tiger cat', score: 0.3634825646877289 },\n * //   { label: 'lion, king of beasts, Panthera leo', score: 0.00045060308184474707 },\n * // ]\n * ```\n * \n * **Example:** Classify an image and return all classes.\n * ```javascript\n * let classifier = await pipeline('image-classification', 'Xenova/vit-base-patch16-224');\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/tiger.jpg';\n * let output = await classifier(url, { topk: 0 });\n * // [\n * //   {label: 'tiger, Panthera tigris', score: 0.632695734500885},\n * //   {label: 'tiger cat', score: 0.3634825646877289},\n * //   {label: 'lion, king of beasts, Panthera leo', score: 0.00045060308184474707},\n * //   {label: 'jaguar, panther, Panthera onca, Felis onca', score: 0.00035465499968267977},\n * //   ...\n * // ]\n * ```\n */\nexport class ImageClassificationPipeline extends Pipeline {\n    /**\n     * Create a new ImageClassificationPipeline.\n     * @param {Object} options An object containing the following properties:\n     * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.\n     * @param {PreTrainedModel} [options.model] The model to use.\n     * @param {Processor} [options.processor] The processor to use.\n     */\n    constructor(options) {\n        super(options);\n    }\n\n    /**\n     * Classify the given images.\n     * @param {any} images The images to classify.\n     * @param {Object} options The options to use for classification.\n     * @param {number} [options.topk=1] The number of top results to return.\n     * @returns {Promise<any>} The top classification results for the images.\n     */\n    async _call(images, {\n        topk = 1\n    } = {}) {\n        let isBatched = Array.isArray(images);\n        images = await prepareImages(images);\n\n        let { pixel_values } = await this.processor(images);\n        let output = await this.model({ pixel_values });\n\n        let id2label = this.model.config.id2label;\n        let toReturn = [];\n        for (let batch of output.logits) {\n            let scores = getTopItems(softmax(batch.data), topk);\n\n            let vals = scores.map(function (x) {\n                return {\n                    label: id2label[x[0]],\n                    score: x[1],\n                }\n            });\n            if (topk === 1) {\n                toReturn.push(...vals);\n            } else {\n                toReturn.push(vals);\n            }\n        }\n\n        return isBatched || topk === 1 ? toReturn : toReturn[0];\n    }\n\n}\n\n/**\n * Image segmentation pipeline using any `AutoModelForXXXSegmentation`.\n * This pipeline predicts masks of objects and their classes.\n * \n * **Example:** Perform image segmentation with `Xenova/detr-resnet-50-panoptic`.\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/cats.jpg';\n * let segmenter = await pipeline('image-segmentation', 'Xenova/detr-resnet-50-panoptic');\n * let output = await segmenter(url);\n * // [\n * //   { label: 'remote', score: 0.9984649419784546, mask: RawImage { ... } },\n * //   { label: 'cat', score: 0.9994316101074219, mask: RawImage { ... } }\n * // ]\n * ```\n */\nexport class ImageSegmentationPipeline extends Pipeline {\n    /**\n     * Create a new ImageSegmentationPipeline.\n     * @param {Object} options An object containing the following properties:\n     * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.\n     * @param {PreTrainedModel} [options.model] The model to use.\n     * @param {Processor} [options.processor] The processor to use.\n     */\n    constructor(options) {\n        super(options);\n\n        this.subtasks_mapping = {\n            // Mapping of subtasks to their corresponding post-processing function names.\n            panoptic: 'post_process_panoptic_segmentation',\n            instance: 'post_process_instance_segmentation',\n            semantic: 'post_process_semantic_segmentation'\n        }\n    }\n\n    /**\n     * Segment the input images.\n     * @param {Array} images The input images.\n     * @param {Object} options The options to use for segmentation.\n     * @param {number} [options.threshold=0.5] Probability threshold to filter out predicted masks.\n     * @param {number} [options.mask_threshold=0.5] Threshold to use when turning the predicted masks into binary values.\n     * @param {number} [options.overlap_mask_area_threshold=0.8] Mask overlap threshold to eliminate small, disconnected segments.\n     * @param {null|string} [options.subtask=null] Segmentation task to be performed. One of [`panoptic`, `instance`, and `semantic`], depending on model capabilities. If not set, the pipeline will attempt to resolve (in that order).\n     * @param {Array} [options.label_ids_to_fuse=null] List of label ids to fuse. If not set, do not fuse any labels.\n     * @param {Array} [options.target_sizes=null] List of target sizes for the input images. If not set, use the original image sizes.\n     * @returns {Promise<Array>} The annotated segments.\n     */\n    async _call(images, {\n        threshold = 0.5,\n        mask_threshold = 0.5,\n        overlap_mask_area_threshold = 0.8,\n        label_ids_to_fuse = null,\n        target_sizes = null,\n        subtask = null, // TODO use\n    } = {}) {\n        let isBatched = Array.isArray(images);\n\n        if (isBatched && images.length !== 1) {\n            throw Error(\"Image segmentation pipeline currently only supports a batch size of 1.\");\n        }\n\n        images = await prepareImages(images);\n        let imageSizes = images.map(x => [x.height, x.width]);\n\n        let { pixel_values, pixel_mask } = await this.processor(images);\n        let output = await this.model({ pixel_values, pixel_mask });\n\n        let fn = null;\n        if (subtask !== null) {\n            fn = this.subtasks_mapping[subtask];\n        } else {\n            for (let [task, func] of Object.entries(this.subtasks_mapping)) {\n                if (func in this.processor.feature_extractor) {\n                    fn = this.processor.feature_extractor[func].bind(this.processor.feature_extractor);\n                    subtask = task;\n                    break;\n                }\n            }\n        }\n\n        // add annotations\n        let annotation = [];\n\n        if (subtask === 'panoptic' || subtask === 'instance') {\n\n            let processed = fn(\n                output,\n                threshold,\n                mask_threshold,\n                overlap_mask_area_threshold,\n                label_ids_to_fuse,\n                target_sizes ?? imageSizes, // TODO FIX?\n            )[0];\n\n            let segmentation = processed.segmentation;\n            let id2label = this.model.config.id2label;\n\n            for (let segment of processed.segments_info) {\n                let maskData = new Uint8ClampedArray(segmentation.data.length);\n                for (let i = 0; i < segmentation.data.length; ++i) {\n                    if (segmentation.data[i] === segment.id) {\n                        maskData[i] = 255;\n                    }\n                }\n\n                let mask = new RawImage(maskData, segmentation.dims[1], segmentation.dims[0], 1)\n\n                annotation.push({\n                    score: segment.score,\n                    label: id2label[segment.label_id],\n                    mask: mask\n                })\n            }\n\n        } else if (subtask === 'semantic') {\n            throw Error(`semantic segmentation not yet supported.`);\n\n        } else {\n            throw Error(`Subtask ${subtask} not supported.`);\n        }\n\n        return annotation;\n    }\n}\n\n\n/**\n * Zero shot image classification pipeline. This pipeline predicts the class of\n * an image when you provide an image and a set of `candidate_labels`.\n * \n * **Example:** Zero shot image classification w/ `Xenova/clip-vit-base-patch32`.\n * ```javascript\n * let classifier = await pipeline('zero-shot-image-classification', 'Xenova/clip-vit-base-patch32');\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/tiger.jpg';\n * let output = await classifier(url, ['tiger', 'horse', 'dog']);\n * // [\n * //   { score: 0.9993917942047119, label: 'tiger' },\n * //   { score: 0.0003519294841680676, label: 'horse' },\n * //   { score: 0.0002562698791734874, label: 'dog' }\n * // ]\n * ```\n */\nexport class ZeroShotImageClassificationPipeline extends Pipeline {\n\n    /**\n     * Create a new ZeroShotImageClassificationPipeline.\n     * @param {Object} options An object containing the following properties:\n     * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.\n     * @param {PreTrainedModel} [options.model] The model to use.\n     * @param {PreTrainedTokenizer} [options.tokenizer] The tokenizer to use.\n     * @param {Processor} [options.processor] The processor to use.\n     */\n    constructor(options) {\n        super(options);\n    }\n\n    /**\n     * Classify the input images with candidate labels using a zero-shot approach.\n     * @param {Array} images The input images.\n     * @param {string[]} candidate_labels The candidate labels.\n     * @param {Object} options The options for the classification.\n     * @param {string} [options.hypothesis_template] The hypothesis template to use for zero-shot classification. Default: \"This is a photo of {}\".\n     * @returns {Promise<any>} An array of classifications for each input image or a single classification object if only one input image is provided.\n     */\n    async _call(images, candidate_labels, {\n        hypothesis_template = \"This is a photo of {}\"\n    } = {}) {\n        const isBatched = Array.isArray(images);\n        images = await prepareImages(images);\n\n        // Insert label into hypothesis template \n        const texts = candidate_labels.map(\n            x => hypothesis_template.replace('{}', x)\n        );\n\n        // Run tokenization\n        const text_inputs = this.tokenizer(texts, {\n            padding: true,\n            truncation: true\n        });\n\n        // Run processor\n        const { pixel_values } = await this.processor(images);\n\n        // Run model with both text and pixel inputs\n        const output = await this.model({ ...text_inputs, pixel_values });\n\n        // Compare each image with each candidate label\n        const toReturn = [];\n        for (const batch of output.logits_per_image) {\n            // Compute softmax per image\n            const probs = softmax(batch.data);\n\n            const result = [...probs].map((x, i) => ({\n                score: x,\n                label: candidate_labels[i]\n            }));\n            result.sort((a, b) => b.score - a.score); // sort by score in descending order\n            toReturn.push(result);\n        }\n\n        return isBatched ? toReturn : toReturn[0];\n    }\n}\n\n/**\n * Object detection pipeline using any `AutoModelForObjectDetection`.\n * This pipeline predicts bounding boxes of objects and their classes.\n * \n * **Example:** Run object-detection with `facebook/detr-resnet-50`.\n * ```javascript\n * let img = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/cats.jpg';\n * \n * let detector = await pipeline('object-detection', 'Xenova/detr-resnet-50');\n * let output = await detector(img, { threshold: 0.9 });\n * // [{\n * //   \"score\": 0.9976370930671692,\n * //   \"label\": \"remote\",\n * //   \"box\": { \"xmin\": 31, \"ymin\": 68, \"xmax\": 190, \"ymax\": 118 }\n * // },\n * // ...\n * // {\n * //   \"score\": 0.9984092116355896,\n * //   \"label\": \"cat\",\n * //   \"box\": { \"xmin\": 331, \"ymin\": 19, \"xmax\": 649, \"ymax\": 371 }\n * // }]\n * ```\n */\nexport class ObjectDetectionPipeline extends Pipeline {\n    /**\n     * Create a new ObjectDetectionPipeline.\n     * @param {Object} options An object containing the following properties:\n     * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.\n     * @param {PreTrainedModel} [options.model] The model to use.\n     * @param {Processor} [options.processor] The processor to use.\n     */\n    constructor(options) {\n        super(options);\n    }\n\n    /**\n     * Detect objects (bounding boxes & classes) in the image(s) passed as inputs.\n     * @param {any[]} images The input images.\n     * @param {Object} options The options for the object detection.\n     * @param {number} [options.threshold=0.9] The threshold used to filter boxes by score.\n     * @param {boolean} [options.percentage=false] Whether to return the boxes coordinates in percentage (true) or in pixels (false).\n     */\n    async _call(images, {\n        threshold = 0.9,\n        percentage = false,\n    } = {}) {\n        let isBatched = Array.isArray(images);\n\n        if (isBatched && images.length !== 1) {\n            throw Error(\"Object detection pipeline currently only supports a batch size of 1.\");\n        }\n        images = await prepareImages(images);\n\n        let imageSizes = percentage ? null : images.map(x => [x.height, x.width]);\n\n        let { pixel_values, pixel_mask } = await this.processor(images);\n        let output = await this.model({ pixel_values, pixel_mask });\n\n        // @ts-ignore\n        let processed = this.processor.feature_extractor.post_process_object_detection(output, threshold, imageSizes);\n\n        // Add labels\n        let id2label = this.model.config.id2label;\n\n        // Format output\n        const result = processed.map(batch => {\n            return batch.boxes.map((box, i) => {\n                return {\n                    score: batch.scores[i],\n                    label: id2label[batch.classes[i]],\n                    box: get_bounding_box(box, !percentage),\n                }\n            })\n        })\n\n        return isBatched ? result : result[0];\n    }\n}\n\n/**\n * Zero-shot object detection pipeline. This pipeline predicts bounding boxes of\n * objects when you provide an image and a set of `candidate_labels`.\n * \n * **Example:** Zero-shot object detection w/ `Xenova/clip-vit-base-patch32`.\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/astronaut.png';\n * let candidate_labels = ['human face', 'rocket', 'helmet', 'american flag'];\n * let detector = await pipeline('zero-shot-object-detection', 'Xenova/owlvit-base-patch32');\n * let output = await detector(url, candidate_labels);\n * // [\n * //   {\n * //     score: 0.24392342567443848,\n * //     label: 'human face',\n * //     box: { xmin: 180, ymin: 67, xmax: 274, ymax: 175 }\n * //   },\n * //   {\n * //     score: 0.15129457414150238,\n * //     label: 'american flag',\n * //     box: { xmin: 0, ymin: 4, xmax: 106, ymax: 513 }\n * //   },\n * //   {\n * //     score: 0.13649864494800568,\n * //     label: 'helmet',\n * //     box: { xmin: 277, ymin: 337, xmax: 511, ymax: 511 }\n * //   },\n * //   {\n * //     score: 0.10262022167444229,\n * //     label: 'rocket',\n * //     box: { xmin: 352, ymin: -1, xmax: 463, ymax: 287 }\n * //   }\n * // ]\n * ```\n * \n * **Example:** Zero-shot object detection w/ `Xenova/clip-vit-base-patch32` (returning top 4 matches and setting a threshold).\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/beach.png';\n * let candidate_labels = ['hat', 'book', 'sunglasses', 'camera'];\n * let detector = await pipeline('zero-shot-object-detection', 'Xenova/owlvit-base-patch32');\n * let output = await detector(url, candidate_labels, { topk: 4, threshold: 0.05 });\n * // [\n * //   {\n * //     score: 0.1606510728597641,\n * //     label: 'sunglasses',\n * //     box: { xmin: 347, ymin: 229, xmax: 429, ymax: 264 }\n * //   },\n * //   {\n * //     score: 0.08935828506946564,\n * //     label: 'hat',\n * //     box: { xmin: 38, ymin: 174, xmax: 258, ymax: 364 }\n * //   },\n * //   {\n * //     score: 0.08530698716640472,\n * //     label: 'camera',\n * //     box: { xmin: 187, ymin: 350, xmax: 260, ymax: 411 }\n * //   },\n * //   {\n * //     score: 0.08349756896495819,\n * //     label: 'book',\n * //     box: { xmin: 261, ymin: 280, xmax: 494, ymax: 425 }\n * //   }\n * // ]\n * ```\n */\nexport class ZeroShotObjectDetectionPipeline extends Pipeline {\n\n    /**\n     * Create a new ZeroShotObjectDetectionPipeline.\n     * @param {Object} options An object containing the following properties:\n     * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.\n     * @param {PreTrainedModel} [options.model] The model to use.\n     * @param {PreTrainedTokenizer} [options.tokenizer] The tokenizer to use.\n     * @param {Processor} [options.processor] The processor to use.\n     */\n    constructor(options) {\n        super(options);\n    }\n\n    /**\n     * Detect objects (bounding boxes & classes) in the image(s) passed as inputs.\n     * @param {Array} images The input images.\n     * @param {string[]} candidate_labels What the model should recognize in the image.\n     * @param {Object} options The options for the classification.\n     * @param {number} [options.threshold] The probability necessary to make a prediction.\n     * @param {number} [options.topk] The number of top predictions that will be returned by the pipeline.\n     * If the provided number is `null` or higher than the number of predictions available, it will default\n     * to the number of predictions.\n     * @param {boolean} [options.percentage=false] Whether to return the boxes coordinates in percentage (true) or in pixels (false).\n     * @returns {Promise<any>} An array of classifications for each input image or a single classification object if only one input image is provided.\n     */\n    async _call(images, candidate_labels, {\n        threshold = 0.1,\n        topk = null,\n        percentage = false,\n    } = {}) {\n        const isBatched = Array.isArray(images);\n        images = await prepareImages(images);\n\n        // Run tokenization\n        const text_inputs = this.tokenizer(candidate_labels, {\n            padding: true,\n            truncation: true\n        });\n\n        // Run processor\n        const model_inputs = await this.processor(images);\n\n        // Since non-maximum suppression is performed for exporting, we need to\n        // process each image separately. For more information, see:\n        // https://github.com/huggingface/optimum/blob/e3b7efb1257c011db907ef40ab340e795cc5684c/optimum/exporters/onnx/model_configs.py#L1028-L1032\n        const toReturn = [];\n        for (let i = 0; i < images.length; ++i) {\n            const image = images[i];\n            const imageSize = percentage ? null : [[image.height, image.width]];\n            const pixel_values = model_inputs.pixel_values[i].unsqueeze_(0);\n\n            // Run model with both text and pixel inputs\n            const output = await this.model({ ...text_inputs, pixel_values });\n\n            // @ts-ignore\n            const processed = this.processor.feature_extractor.post_process_object_detection(output, threshold, imageSize, true)[0];\n            let result = processed.boxes.map((box, i) => ({\n                score: processed.scores[i],\n                label: candidate_labels[processed.classes[i]],\n                box: get_bounding_box(box, !percentage),\n            })).sort((a, b) => b.score - a.score);\n            if (topk !== null) {\n                result = result.slice(0, topk);\n            }\n            toReturn.push(result)\n        }\n\n        return isBatched ? toReturn : toReturn[0];\n    }\n}\n\n/**\n * Document Question Answering pipeline using any `AutoModelForDocumentQuestionAnswering`.\n * The inputs/outputs are similar to the (extractive) question answering pipeline; however,\n * the pipeline takes an image (and optional OCR'd words/boxes) as input instead of text context.\n * \n * **Example:** Answer questions about a document with `Xenova/donut-base-finetuned-docvqa`.\n * ```javascript\n * let image = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/invoice.png';\n * let question = 'What is the invoice number?';\n * \n * let qa_pipeline = await pipeline('document-question-answering', 'Xenova/donut-base-finetuned-docvqa');\n * let output = await qa_pipeline(image, question);\n * // [{ answer: 'us-001' }]\n * ```\n */\nexport class DocumentQuestionAnsweringPipeline extends Pipeline {\n    /**\n     * Create a new DocumentQuestionAnsweringPipeline.\n     * @param {Object} options An object containing the following properties:\n     * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.\n     * @param {PreTrainedModel} [options.model] The model to use.\n     * @param {PreTrainedTokenizer} [options.tokenizer] The tokenizer to use.\n     * @param {Processor} [options.processor] The processor to use.\n     */\n    constructor(options) {\n        super(options);\n    }\n\n    /**\n     * Answer the question given as input by using the document.\n     * @param {any} image The image of the document to use.\n     * @param {string} question A question to ask of the document.\n     * @param {Object} [generate_kwargs={}] Optional generation arguments.\n     * @returns {Promise<Object|Object[]>} A Promise that resolves to an object (or array of objects) containing the generated text(s).\n     */\n    async _call(image, question, generate_kwargs = {}) {\n        // NOTE: For now, we only support a batch size of 1\n\n        // Preprocess image\n        image = (await prepareImages(image))[0];\n        const { pixel_values } = await this.processor(image);\n\n        // Run tokenization\n        const task_prompt = `<s_docvqa><s_question>${question}</s_question><s_answer>`;\n        const decoder_input_ids = this.tokenizer(task_prompt, {\n            add_special_tokens: false,\n            padding: true,\n            truncation: true\n        }).input_ids;\n\n        // Run model\n        const output = await this.model.generate(\n            pixel_values,\n            {\n                ...generate_kwargs,\n                decoder_input_ids,\n                max_length: this.model.config.decoder.max_position_embeddings,\n            }\n        );\n\n        // Decode output\n        const decoded = this.tokenizer.batch_decode(output)[0];\n\n        // Parse answer\n        const match = decoded.match(/<s_answer>(.*?)<\\/s_answer>/);\n        let answer = null;\n        if (match && match.length >= 2) {\n            answer = match[1].trim();\n        }\n        return [{ answer }];\n    }\n}\n\n/**\n * Text-to-audio generation pipeline using any `AutoModelForTextToWaveform` or `AutoModelForTextToSpectrogram`.\n * This pipeline generates an audio file from an input text and optional other conditional inputs.\n * \n * **Example:** Generate audio from text with `Xenova/speecht5_tts`.\n * ```js\n * let speaker_embeddings = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/speaker_embeddings.bin';\n * let synthesizer = await pipeline('text-to-speech', 'Xenova/speecht5_tts', { quantized: false });\n * let out = await synthesizer('Hello, my dog is cute', { speaker_embeddings });\n * // {\n * //   audio: Float32Array(26112) [-0.00005657337896991521, 0.00020583874720614403, ...],\n * //   sampling_rate: 16000\n * // }\n * ```\n * \n * You can then save the audio to a .wav file with the `wavefile` package:\n * ```js\n * import wavefile from 'wavefile';\n * import fs from 'fs';\n * \n * let wav = new wavefile.WaveFile();\n * wav.fromScratch(1, out.sampling_rate, '32f', out.audio);\n * fs.writeFileSync('out.wav', wav.toBuffer());\n * ```\n */\nexport class TextToAudioPipeline extends Pipeline {\n    DEFAULT_VOCODER_ID = \"Xenova/speecht5_hifigan\"\n\n    /**\n     * Create a new TextToAudioPipeline.\n     * @param {Object} options An object containing the following properties:\n     * @param {string} [options.task] The task of the pipeline. Useful for specifying subtasks.\n     * @param {PreTrainedModel} [options.model] The model to use.\n     * @param {PreTrainedTokenizer} [options.tokenizer] The tokenizer to use.\n     * @param {Processor} [options.processor] The processor to use.\n     * @param {PreTrainedModel} [options.vocoder] The vocoder to use.\n     */\n    constructor(options) {\n        super(options);\n\n        // TODO: Find a better way for `pipeline` to set the default vocoder\n        this.vocoder = options.vocoder ?? null;\n    }\n\n    /**\n     * Generates speech/audio from the inputs.\n     * @param {string|string[]} text_inputs The text(s) to generate.\n     * @param {Object} options Parameters passed to the model generation/forward method.\n     * @param {PreTrainedModel} [options.vocoder=null] The vocoder to use (if the model uses one). If not provided, use the default HifiGan vocoder.\n     * @param {Tensor|Float32Array|string|URL} [options.speaker_embeddings=null]\n     * @returns {Promise<Object>} An object containing the generated audio and sampling rate.\n     */\n    async _call(text_inputs, {\n        speaker_embeddings = null,\n    } = {}) {\n        // Load vocoder, if not provided\n        if (!this.vocoder) {\n            console.log('No vocoder specified, using default HifiGan vocoder.');\n            this.vocoder = await AutoModel.from_pretrained(this.DEFAULT_VOCODER_ID, { quantized: false });\n        }\n\n        // Load speaker embeddings as Float32Array from path/URL\n        if (typeof speaker_embeddings === 'string' || speaker_embeddings instanceof URL) {\n            // Load from URL with fetch\n            speaker_embeddings = new Float32Array(\n                await (await fetch(speaker_embeddings)).arrayBuffer()\n            );\n        }\n\n        if (speaker_embeddings instanceof Float32Array) {\n            speaker_embeddings = new Tensor(\n                'float32',\n                speaker_embeddings,\n                [1, speaker_embeddings.length]\n            )\n        } else if (!(speaker_embeddings instanceof Tensor)) {\n            throw new Error(\"Speaker embeddings must be a `Tensor`, `Float32Array`, `string`, or `URL`.\")\n        }\n\n        // Run tokenization\n        const { input_ids } = this.tokenizer(text_inputs, {\n            padding: true,\n            truncation: true\n        });\n\n        // NOTE: At this point, we are guaranteed that `speaker_embeddings` is a `Tensor`\n        // @ts-ignore\n        const { waveform } = await this.model.generate_speech(input_ids, speaker_embeddings, { vocoder: this.vocoder });\n\n        const sampling_rate = this.processor.feature_extractor.config.sampling_rate;\n        return {\n            audio: waveform.data,\n            sampling_rate,\n        }\n    }\n}\n\n/**\n * Image to Image pipeline using any `AutoModelForImageToImage`. This pipeline generates an image based on a previous image input.\n * \n * **Example:** Super-resolution w/ `Xenova/swin2SR-classical-sr-x2-64`\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/butterfly.jpg';\n * let upscaler = await pipeline('image-to-image', 'Xenova/swin2SR-classical-sr-x2-64');\n * let output = await upscaler(url);\n * // RawImage {\n * //   data: Uint8Array(786432) [ 41, 31, 24,  43, ... ],\n * //   width: 512,\n * //   height: 512,\n * //   channels: 3\n * // }\n * ```\n */\nexport class ImageToImagePipeline extends Pipeline {\n    /**\n     * Transform the image(s) passed as inputs.\n     * @param {any} images The images to transform.\n     * @returns {Promise<any>} An image or a list of images containing result(s).\n     */\n    async _call(images) {\n        images = await prepareImages(images);\n\n        let inputs = await this.processor(images);\n        let outputs = await this.model(inputs);\n\n        let toReturn = [];\n        for (let batch of outputs.reconstruction) {\n            const output = batch.squeeze().clamp_(0, 1).mul_(255).round_().to('uint8');\n            toReturn.push(RawImage.fromTensor(output));\n        }\n\n        return toReturn.length > 1 ? toReturn : toReturn[0];\n    }\n}\n\n/**\n * Depth estimation pipeline using any `AutoModelForDepthEstimation`. This pipeline predicts the depth of an image.\n * \n * **Example:** Depth estimation w/ `Xenova/dpt-hybrid-midas`\n * ```javascript\n * let url = 'https://huggingface.co/datasets/Xenova/transformers.js-docs/resolve/main/cats.jpg';\n * let depth_estimator = await pipeline('depth-estimation', 'Xenova/dpt-hybrid-midas');\n * let out = await depth_estimator(url);\n * // {\n * //   predicted_depth: Tensor {\n * //     dims: [ 384, 384 ],\n * //     type: 'float32',\n * //     data: Float32Array(147456) [ 542.859130859375, 545.2833862304688, 546.1649169921875, ... ],\n * //     size: 147456\n * //   },\n * //   depth: RawImage {\n * //     data: Uint8Array(307200) [ 86, 86, 86, ... ],\n * //     width: 640,\n * //     height: 480,\n * //     channels: 1\n * //   }\n * // }\n * ```\n */\nexport class DepthEstimationPipeline extends Pipeline {\n    /**\n     * Predicts the depth for the image(s) passed as inputs.\n     * @param {any} images The images to compute depth for.\n     * @returns {Promise<any>} An image or a list of images containing result(s).\n     */\n    async _call(images) {\n        images = await prepareImages(images);\n\n        const inputs = await this.processor(images);\n        const { predicted_depth } = await this.model(inputs);\n\n        const toReturn = [];\n        for (let i = 0; i < images.length; ++i) {\n            const prediction = interpolate(predicted_depth[i], images[i].size.reverse(), 'bilinear', false);\n            const formatted = prediction.mul_(255 / max(prediction.data)[0]).to('uint8');\n            toReturn.push({\n                predicted_depth: predicted_depth[i],\n                depth: RawImage.fromTensor(formatted),\n            });\n        }\n\n        return toReturn.length > 1 ? toReturn : toReturn[0];\n    }\n}\n\nconst SUPPORTED_TASKS = {\n    \"text-classification\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": TextClassificationPipeline,\n        \"model\": AutoModelForSequenceClassification,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"distilbert-base-uncased-finetuned-sst-2-english\",\n            \"model\": \"Xenova/distilbert-base-uncased-finetuned-sst-2-english\",\n        },\n        \"type\": \"text\",\n    },\n    \"token-classification\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": TokenClassificationPipeline,\n        \"model\": AutoModelForTokenClassification,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"Davlan/bert-base-multilingual-cased-ner-hrl\",\n            \"model\": \"Xenova/bert-base-multilingual-cased-ner-hrl\",\n        },\n        \"type\": \"text\",\n    },\n    \"question-answering\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": QuestionAnsweringPipeline,\n        \"model\": AutoModelForQuestionAnswering,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"distilbert-base-cased-distilled-squad\",\n            \"model\": \"Xenova/distilbert-base-cased-distilled-squad\",\n        },\n        \"type\": \"text\",\n    },\n\n    \"fill-mask\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": FillMaskPipeline,\n        \"model\": AutoModelForMaskedLM,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"bert-base-uncased\",\n            \"model\": \"Xenova/bert-base-uncased\",\n        },\n        \"type\": \"text\",\n    },\n    \"summarization\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": SummarizationPipeline,\n        \"model\": AutoModelForSeq2SeqLM,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"sshleifer/distilbart-cnn-6-6\",\n            \"model\": \"Xenova/distilbart-cnn-6-6\",\n        },\n        \"type\": \"text\",\n    },\n    \"translation\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": TranslationPipeline,\n        \"model\": AutoModelForSeq2SeqLM,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"t5-small\",\n            \"model\": \"Xenova/t5-small\",\n        },\n        \"type\": \"text\",\n    },\n    \"text2text-generation\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": Text2TextGenerationPipeline,\n        \"model\": AutoModelForSeq2SeqLM,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"google/flan-t5-small\",\n            \"model\": \"Xenova/flan-t5-small\",\n        },\n        \"type\": \"text\",\n    },\n    \"text-generation\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": TextGenerationPipeline,\n        \"model\": AutoModelForCausalLM,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"gpt2\",\n            \"model\": \"Xenova/gpt2\",\n        },\n        \"type\": \"text\",\n    },\n    \"zero-shot-classification\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": ZeroShotClassificationPipeline,\n        \"model\": AutoModelForSequenceClassification,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"typeform/distilbert-base-uncased-mnli\",\n            \"model\": \"Xenova/distilbert-base-uncased-mnli\",\n        },\n        \"type\": \"text\",\n    },\n    \"audio-classification\": {\n        \"pipeline\": AudioClassificationPipeline,\n        \"model\": AutoModelForAudioClassification,\n        \"processor\": AutoProcessor,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"superb/wav2vec2-base-superb-ks\",\n            \"model\": \"Xenova/wav2vec2-base-superb-ks\",\n        },\n        \"type\": \"audio\",\n    },\n    \"zero-shot-audio-classification\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": ZeroShotAudioClassificationPipeline,\n        \"model\": AutoModel,\n        \"processor\": AutoProcessor,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"laion/clap-htsat-fused\",\n            \"model\": \"Xenova/clap-htsat-unfused\",\n        },\n        \"type\": \"multimodal\",\n    },\n    \"automatic-speech-recognition\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": AutomaticSpeechRecognitionPipeline,\n        \"model\": [AutoModelForSpeechSeq2Seq, AutoModelForCTC],\n        \"processor\": AutoProcessor,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"openai/whisper-tiny.en\",\n            \"model\": \"Xenova/whisper-tiny.en\",\n        },\n        \"type\": \"multimodal\",\n    },\n    \"text-to-audio\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": TextToAudioPipeline,\n        \"model\": [ /* TODO: AutoModelForTextToWaveform, */ AutoModelForTextToSpectrogram],\n        \"processor\": AutoProcessor,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"microsoft/speecht5_tts\",\n            \"model\": \"Xenova/speecht5_tts\",\n        },\n        \"type\": \"text\",\n    },\n    \"image-to-text\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": ImageToTextPipeline,\n        \"model\": AutoModelForVision2Seq,\n        \"processor\": AutoProcessor,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"nlpconnect/vit-gpt2-image-captioning\",\n            \"model\": \"Xenova/vit-gpt2-image-captioning\",\n        },\n        \"type\": \"multimodal\",\n    },\n\n    \"image-classification\": {\n        // no tokenizer\n        \"pipeline\": ImageClassificationPipeline,\n        \"model\": AutoModelForImageClassification,\n        \"processor\": AutoProcessor,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"google/vit-base-patch16-224\",\n            \"model\": \"Xenova/vit-base-patch16-224\",\n        },\n        \"type\": \"multimodal\",\n    },\n\n    \"image-segmentation\": {\n        // no tokenizer\n        \"pipeline\": ImageSegmentationPipeline,\n        \"model\": AutoModelForImageSegmentation,\n        \"processor\": AutoProcessor,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"facebook/detr-resnet-50-panoptic\",\n            \"model\": \"Xenova/detr-resnet-50-panoptic\",\n        },\n        \"type\": \"multimodal\",\n    },\n\n    \"zero-shot-image-classification\": {\n        // no tokenizer\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": ZeroShotImageClassificationPipeline,\n        \"model\": AutoModel,\n        \"processor\": AutoProcessor,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"openai/clip-vit-base-patch32\",\n            \"model\": \"Xenova/clip-vit-base-patch32\",\n        },\n        \"type\": \"multimodal\",\n    },\n\n    \"object-detection\": {\n        // no tokenizer\n        \"pipeline\": ObjectDetectionPipeline,\n        \"model\": AutoModelForObjectDetection,\n        \"processor\": AutoProcessor,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"facebook/detr-resnet-50\",\n            \"model\": \"Xenova/detr-resnet-50\",\n        },\n        \"type\": \"multimodal\",\n    },\n    \"zero-shot-object-detection\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": ZeroShotObjectDetectionPipeline,\n        \"model\": AutoModelForZeroShotObjectDetection,\n        \"processor\": AutoProcessor,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"google/owlvit-base-patch32\",\n            \"model\": \"Xenova/owlvit-base-patch32\",\n        },\n        \"type\": \"multimodal\",\n    },\n    \"document-question-answering\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": DocumentQuestionAnsweringPipeline,\n        \"model\": AutoModelForDocumentQuestionAnswering,\n        \"processor\": AutoProcessor,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"naver-clova-ix/donut-base-finetuned-docvqa\",\n            \"model\": \"Xenova/donut-base-finetuned-docvqa\",\n        },\n        \"type\": \"multimodal\",\n    },\n    \"image-to-image\": {\n        // no tokenizer\n        \"pipeline\": ImageToImagePipeline,\n        \"model\": AutoModelForImageToImage,\n        \"processor\": AutoProcessor,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"caidas/swin2SR-classical-sr-x2-64\",\n            \"model\": \"Xenova/swin2SR-classical-sr-x2-64\",\n        },\n        \"type\": \"image\",\n    },\n    \"depth-estimation\": {\n        // no tokenizer\n        \"pipeline\": DepthEstimationPipeline,\n        \"model\": AutoModelForDepthEstimation,\n        \"processor\": AutoProcessor,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"Intel/dpt-large\",\n            \"model\": \"Xenova/dpt-large\",\n        },\n        \"type\": \"image\",\n    },\n\n    // This task serves as a useful interface for dealing with sentence-transformers (https://huggingface.co/sentence-transformers).\n    \"feature-extraction\": {\n        \"tokenizer\": AutoTokenizer,\n        \"pipeline\": FeatureExtractionPipeline,\n        \"model\": AutoModel,\n        \"default\": {\n            // TODO: replace with original\n            // \"model\": \"sentence-transformers/all-MiniLM-L6-v2\",\n            \"model\": \"Xenova/all-MiniLM-L6-v2\",\n        },\n        \"type\": \"text\",\n    },\n}\n\n\nconst TASK_ALIASES = {\n    \"sentiment-analysis\": \"text-classification\",\n    \"ner\": \"token-classification\",\n    \"vqa\": \"visual-question-answering\",\n    \"asr\": \"automatic-speech-recognition\",\n    \"text-to-speech\": \"text-to-audio\",\n\n    // Add for backwards compatibility\n    \"embeddings\": \"feature-extraction\",\n}\n\n/**\n * Utility factory method to build a [`Pipeline`] object.\n *\n * @param {string} task The task defining which pipeline will be returned. Currently accepted tasks are:\n *  - `\"audio-classification\"`: will return a `AudioClassificationPipeline`.\n *  - `\"automatic-speech-recognition\"`: will return a `AutomaticSpeechRecognitionPipeline`.\n *  - `\"depth-estimation\"`: will return a `DepthEstimationPipeline`.\n *  - `\"document-question-answering\"`: will return a `DocumentQuestionAnsweringPipeline`.\n *  - `\"feature-extraction\"`: will return a `FeatureExtractionPipeline`.\n *  - `\"fill-mask\"`: will return a `FillMaskPipeline`.\n *  - `\"image-classification\"`: will return a `ImageClassificationPipeline`.\n *  - `\"image-segmentation\"`: will return a `ImageSegmentationPipeline`.\n *  - `\"image-to-text\"`: will return a `ImageToTextPipeline`.\n *  - `\"object-detection\"`: will return a `ObjectDetectionPipeline`.\n *  - `\"question-answering\"`: will return a `QuestionAnsweringPipeline`.\n *  - `\"summarization\"`: will return a `SummarizationPipeline`.\n *  - `\"text2text-generation\"`: will return a `Text2TextGenerationPipeline`.\n *  - `\"text-classification\"` (alias \"sentiment-analysis\" available): will return a `TextClassificationPipeline`.\n *  - `\"text-generation\"`: will return a `TextGenerationPipeline`.\n *  - `\"token-classification\"` (alias \"ner\" available): will return a `TokenClassificationPipeline`.\n *  - `\"translation\"`: will return a `TranslationPipeline`.\n *  - `\"translation_xx_to_yy\"`: will return a `TranslationPipeline`.\n *  - `\"zero-shot-classification\"`: will return a `ZeroShotClassificationPipeline`.\n *  - `\"zero-shot-audio-classification\"`: will return a `ZeroShotAudioClassificationPipeline`.\n *  - `\"zero-shot-image-classification\"`: will return a `ZeroShotImageClassificationPipeline`.\n *  - `\"zero-shot-object-detection\"`: will return a `ZeroShotObjectDetectionPipeline`.\n * @param {string} [model=null] The name of the pre-trained model to use. If not specified, the default model for the task will be used.\n * @param {import('./utils/hub.js').PretrainedOptions} [options] Optional parameters for the pipeline.\n * @returns {Promise<Pipeline>} A Pipeline object for the specified task.\n * @throws {Error} If an unsupported pipeline is requested.\n */\nexport async function pipeline(\n    task,\n    model = null,\n    {\n        quantized = true,\n        progress_callback = null,\n        config = null,\n        cache_dir = null,\n        local_files_only = false,\n        revision = 'main',\n    } = {}\n) {\n    // Helper method to construct pipeline\n\n    // Apply aliases\n    task = TASK_ALIASES[task] ?? task;\n\n    // Get pipeline info\n    let pipelineInfo = SUPPORTED_TASKS[task.split('_', 1)[0]];\n    if (!pipelineInfo) {\n        throw Error(`Unsupported pipeline: ${task}. Must be one of [${Object.keys(SUPPORTED_TASKS)}]`)\n    }\n\n    // Use model if specified, otherwise, use default\n    if (!model) {\n        model = pipelineInfo.default.model\n        console.log(`No model specified. Using default model: \"${model}\".`);\n    }\n\n    let pretrainedOptions = {\n        quantized,\n        progress_callback,\n        config,\n        cache_dir,\n        local_files_only,\n        revision,\n    }\n\n    const classes = new Map([\n        ['tokenizer', pipelineInfo.tokenizer],\n        ['model', pipelineInfo.model],\n        ['processor', pipelineInfo.processor],\n    ]);\n\n    // Load model, tokenizer, and processor (if they exist)\n    let results = await loadItems(classes, model, pretrainedOptions);\n    results.task = task;\n\n    dispatchCallback(progress_callback, {\n        'status': 'ready',\n        'task': task,\n        'model': model,\n    });\n\n    let pipelineClass = pipelineInfo.pipeline;\n    return new pipelineClass(results);\n}\n\n\n/**\n * Helper function to get applicable model, tokenizer, or processor classes for a given model.\n * @param {Map<string, any>} mapping The mapping of names to classes, arrays of classes, or null.\n * @param {string} model The name of the model to load.\n * @param {import('./utils/hub.js').PretrainedOptions} pretrainedOptions The options to pass to the `from_pretrained` method.\n * @private\n */\nasync function loadItems(mapping, model, pretrainedOptions) {\n\n    const result = Object.create(null);\n\n    /**@type {Promise[]} */\n    const promises = [];\n    for (let [name, cls] of mapping.entries()) {\n        if (!cls) continue;\n\n        /**@type {Promise} */\n        let promise;\n        if (Array.isArray(cls)) {\n            promise = new Promise(async (resolve, reject) => {\n                let e;\n                for (let c of cls) {\n                    try {\n                        resolve(await c.from_pretrained(model, pretrainedOptions));\n                        return;\n                    } catch (err) {\n                        e = err;\n                    }\n                }\n                reject(e);\n            })\n        } else {\n            promise = cls.from_pretrained(model, pretrainedOptions);\n        }\n\n        result[name] = promise;\n        promises.push(promise);\n    }\n\n    // Wait for all promises to resolve (in parallel)\n    await Promise.all(promises);\n\n    // Then assign to result\n    for (let [name, promise] of Object.entries(result)) {\n        result[name] = await promise;\n    }\n\n    return result;\n}"],"names":["l","Symbol","for","n","p","q","r","t","u","v","w","x","y","z","iterator","B","isMounted","enqueueForceUpdate","enqueueReplaceState","enqueueSetState","C","Object","assign","D","E","a","b","e","this","props","context","refs","updater","F","G","prototype","isReactComponent","setState","Error","forceUpdate","H","constructor","isPureReactComponent","I","Array","isArray","J","hasOwnProperty","K","current","L","key","ref","__self","__source","M","d","c","k","h","call","g","arguments","length","children","f","m","defaultProps","$$typeof","type","_owner","O","P","Q","replace","escape","toString","R","N","push","A","next","done","value","String","keys","join","S","T","_status","_result","then","default","U","V","transition","W","ReactCurrentDispatcher","ReactCurrentBatchConfig","ReactCurrentOwner","exports","Component","module","async","loadTokenizer","pretrained_model_name_or_path","options","info","Promise","all","getModelJSON","legacy","createPattern","pattern","invert","undefined","Regex","regex","RegExp","escaped","escapeRegExp","concat","console","warn","objectToMap","obj","Map","entries","prepareTensorForDecode","tensor","dims","tolist","clean_up_tokenization","text","remove_accents","PUNCTUATION_REGEX","AddedToken","config","_config$single_word","_config$lstrip","_config$rstrip","_config$special","_config$normalized","content","id","single_word","lstrip","rstrip","special","normalized","TokenizerModel","Callable","_this$config$fuse_unk","super","vocab","tokens_to_ids","unk_token_id","unk_token","end_of_word_suffix","fuse_unk","fromConfig","_len","args","_key","WordPieceTokenizer","Unigram","BPE","LegacyTokenizerModel","_call","tokens","encode","convert_tokens_to_ids","ids","map","_this$tokens_to_ids$g","get","arr","fused","i","fuse","convert_ids_to_tokens","_this$vocab$i","_config$max_input_cha","max_input_chars_per_word","size","outputTokens","token","chars","isUnknown","start","subTokens","end","currentSubstring","substr","slice","continuing_subword_prefix","has","moreConfig","vocabSize","scores","piece","unk_id","bosToken","bosTokenId","eosToken","eos_token","eosTokenId","unkToken","minScore","min","unkScore","trie","CharTrie","extend","populateNodes","lattice","sentence","len","beginPos","mblen","hasSingleNode","commonPrefixSearch","tokenId","tokenScore","insert","tokenize","TokenLattice","toReturn","tokenized","BYTES_TO_UNICODE","bs","from","charCodeAt","_","cs","includes","ccs","fromCharCode","fromEntries","UNICODE_TO_BYTES","reverseDictionary","_config$continuing_su","_this$config$byte_fal","BPE_SPLIT_TOKEN","bpe_ranks","merges","split","continuing_subword_suffix","byte_fallback","text_encoder","TextEncoder","cache","bpe","cached","word","result","queue","PriorityQueue","score","startingNode","bias","prev","previousNode","currentNode","_add_node","isEmpty","node","pop","deleted","newPreviousNode","merged","set","rank","bpe_token_list","toUpperCase","padStart","target_lang","bos_token","bos_token_id","eos_token_id","pad_token","pad_token_id","Normalizer","BertNormalizer","Precompiled","NormalizerSequence","Replace","NFC","NFKC","NFKD","StripNormalizer","StripAccents","Lowercase","Prepend","normalize","replaceAll","strip_left","strip_right","trim","trimStart","trimEnd","toLowerCase","prepend","normalizers","reduce","normalizer","_tokenize_chinese_chars","output","char","cp","_is_chinese_char","stripAccents","handle_chinese_chars","lowercase","strip_accents","PreTokenizer","BertPreTokenizer","PreTokenizerSequence","WhitespaceSplit","MetaspacePreTokenizer","ByteLevelPreTokenizer","SplitPreTokenizer","PunctuationPreTokenizer","DigitsPreTokenizer","ReplacePreTokenizer","pre_tokenize_text","pre_tokenize","flat","match","_this$config$use_rege","add_prefix_space","trim_offsets","use_regex","byte_encoder","startsWith","byte","matchAll","fullMatch","index","regexSplit","digit_pattern","individual_digits","PostProcessor","TemplateProcessing","ByteLevelPostProcessor","RobertaProcessing","BertProcessing","post_process","_len2","_key2","cls","sep","tokens_pair","mergeArrays","single","pair","item","SpecialToken","Sequence","Decoder","added_tokens","WordPieceDecoder","MetaspaceDecoder","ByteLevelDecoder","ReplaceDecoder","ByteFallback","FuseDecoder","StripDecoder","DecoderSequence","CTCDecoder","BPEDecoder","decode","decode_chain","text_decoder","TextDecoder","new_tokens","previous_byte_tokens","bytes","endsWith","parseInt","isNaN","string","Uint8Array","stop","start_cut","stop_cut","cleanup","prefix","byte_decoder","fatal","ignoreBOM","convert_tokens_to_string","byteArray","sub_texts","current_sub_text","find","word_delimiter_token","grouped_tokens","at","filter","decoders","toks","decoder","suffix","_config$prepend_schem","addPrefixSpace","replacement","strRep","str_rep","prepend_scheme","section_index","substring","charsmap","precompiled_charsmap","parts","part","tokenizers","pretokenizers","preTokenizedText","tokenizer","whitespace_split","SPECIAL_TOKEN_ATTRIBUTES","PreTrainedTokenizer","tokenizerJSON","tokenizerConfig","_tokenizerConfig$addi","_tokenizerConfig$clea","_tokenizerConfig$do_l","_tokenizerConfig$chat","_defineProperty","_tokenizer_config","pre_tokenizer","model","post_processor","special_tokens","all_special_ids","addedToken","additional_special_tokens","Set","added_tokens_regex","mask_token","getToken","mask_token_id","sep_token","sep_token_id","model_max_length","remove_space","clean_up_tokenization_spaces","do_lowercase_and_remove_accent","padding_side","chat_template","_compiled_template_cache","_len3","_key3","__type","from_pretrained","progress_callback","cache_dir","local_files_only","revision","prepare_model_inputs","inputs","text_pair","add_special_tokens","padding","truncation","max_length","return_tensor","maxLengthOfBatch","max","Math","attention_mask","fill","diff","unshift","some","Tensor","BigInt64Array","BigInt","modelInputs","input_ids","_encode_text","lowercase_and_remove_accent","sectionTokens","tokens2","combinedTokens","batch_decode","batch","decode_args","token_ids","isIntegralNumber","decode_single","_ref","skip_special_tokens","decoded","default_chat_template","_warned_about_chat_template","_default_chat_template","apply_chat_template","conversation","_chat_template","_this$chat_template","add_generation_prompt","compiledTemplate","Template","special_tokens_map","create","rendered","render","messages","add_token_types","token_type_ids","data","GPT2Tokenizer","MBartTokenizer","languageRegex","language_codes","test","lang_to_token","_build_translation_inputs","raw_inputs","tokenizer_options","generate_kwargs","SPIECE_UNDERLINE","LlamaTokenizer","_tokenizerConfig$use_","_tokenizerConfig$lega","use_default_system_prompt","DEFAULT_SYSTEM_PROMPT","self","src_lang_token","src_lang","tgt_lang_token","tgt_lang","forced_bos_token_id","WHISPER_LANGUAGES","WHISPER_LANGUAGE_MAPPING","WHISPER_TO_LANGUAGE_CODE_MAPPING","_ref2","BlenderbotTokenizer","AutoTokenizer","_tokenizerConfig$toke","_tokenizerConfig$toke2","quantized","tokenizerName","tokenizer_class","TOKENIZER_CLASS_MAPPING","T5Tokenizer","DistilBertTokenizer","CamembertTokenizer","DebertaTokenizer","DebertaV2Tokenizer","BertTokenizer","HerbertTokenizer","ConvBertTokenizer","XLMTokenizer","ElectraTokenizer","MobileBertTokenizer","SqueezeBertTokenizer","AlbertTokenizer","BartTokenizer","MBart50Tokenizer","RobertaTokenizer","WhisperTokenizer","_decode_asr","sequences","return_timestamps","return_language","time_precision","force_full_sequences","last_language","returnWordTimestamps","new_chunk","chunks","chunk","time_offset","timestamp_begin","previous_tokens","previous_token_timestamps","skip","right_stride_start","token_timestamps","last_timestamp","first_timestamp","chunk_len","stride_left","stride_right","stride","current_tokens","current_token_timestamps","language","resolved_tokens","findLongestCommonSequence","resolved_text","time","rounded_time","round","timestamp","resolved_token_timestamps","words","collateWordTimestamps","end_time","start_time","every","optional","full_text","new_chunks","token_timestamp_sequences","leftSequence","leftLength","totalSequence","use_token_timestamp_sequences","total_token_timestamp_sequence","left_token_timestamp_sequence","rightSequence","maxIndices","rightLength","j","eps","leftStart","leftStop","left","rightStart","rightStop","right","matches","elem","idx","matching","leftMid","floor","rightMid","token_indices","combineTokensIntoWords","timings","indices","_language","word_tokens","prepend_punctionations","append_punctuations","splitTokensOnUnicode","splitTokensOnSpaces","mergePunctuations","decode_with_timestamps","decodeWithTimestamps","_decode_args$time_pre","outputs","s","decoded_full","current_indices","unicode_offset","token_idx","indexOf","subwords","subword_tokens_list","subword_indices_list","punctuationRegex","subword","subword_tokens","subword_indices","with_space","trimmed","punctuation","ix","prepended","appended","newWords","structuredClone","newTokens","newIndices","get_decoder_prompt_ids","task","no_timestamps","forced_decoder_ids","language_code","langs","values","JSON","stringify","language_token_id","task_token_id","no_timestamps_id","CodeGenTokenizer","CLIPTokenizer","MarianTokenizer","supported_language_codes","matchInfo","remainder","BloomTokenizer","_tokenizerJSON$pre_to","splitChars","patternObject","NllbTokenizer","M2M100Tokenizer","CodeLlamaTokenizer","XLMRobertaTokenizer","MPNetTokenizer","FalconTokenizer","GPTNeoXTokenizer","EsmTokenizer","Wav2Vec2CTCTokenizer","BlenderbotSmallTokenizer","SpeechT5Tokenizer","NougatTokenizer","InferenceSession","ONNXTensor","env","ONNX","MODEL_TYPES","MODEL_TYPE_MAPPING","MODEL_NAME_TO_CLASS_MAPPING","MODEL_CLASS_TO_NAME_MAPPING","constructSession","fileName","modelFileName","buffer","getModelFile","executionProviders","err","sessionRun","session","checkedInputs","missingInputs","inputName","inputNames","wasm","proxy","clone","numInputsProvided","numInputsNeeded","ignored","validateInputs","run","replaceTensors","error","prop","toI64Tensor","items","prepareAttentionMask","_self$config$pad_toke","_self$config$eos_toke","is_pad_token_in_inputs","is_pad_token_not_equal_to_eos_token_id","ones_like","preparePositionIds","feeds","use_cache_branch","sum","position_ids","unsqueeze_","boolTensor","seq2seqForward","model_inputs","encoder_outputs","past_key_values","encoderForward","last_hidden_state","decoderFeeds","decoder_input_ids","encoder_hidden_states","decoder_merged_session","encoder_attention_mask","addPastKeyValues","decoderResults","logits","getPastKeyValues","attns","getAttentions","Seq2SeqLMOutput","seq2seqStartBeams","inputTokenIds","generation_config","numOutputTokens","_self$requires_attent","_generation_config$de","beams","beamId","requires_attention_mask","decoder_start_token_id","prev_model_outputs","output_token_ids","seq2seqRunBeam","beam","_beam$prev_model_outp","input_name","main_input_name","forward","seq2seqUpdatebeam","newTokenId","encoderFeeds","decoderForward","decoderStartBeams","inputs_attention_mask","attn_mask","Number","input","model_input_ids","num_output_tokens","decoderRunBeam","_beam$prev_model_outp2","attnMaskData","decoderUpdatebeam","PreTrainedModel","modelName","modelType","can_generate","_runBeam","_getStartBeams","_updateBeam","_forward","dispose","promises","handler","model_file_name","_options$model_file_n","AutoConfig","_options$model_file_n2","_get_logits_processor","input_ids_seq_length","logits_processor","processors","LogitsProcessorList","repetition_penalty","RepetitionPenaltyLogitsProcessor","no_repeat_ngram_size","NoRepeatNGramLogitsProcessor","bad_words_ids","NoBadWordsLogitsProcessor","min_length","MinLengthLogitsProcessor","min_new_tokens","MinNewTokensLengthLogitsProcessor","ForcedBOSTokenLogitsProcessor","forced_eos_token_id","ForcedEOSTokenLogitsProcessor","begin_suppress_tokens","begin_index","SuppressTokensAtBeginLogitsProcessor","ForceTokensLogitsProcessor","_get_generation_config","gen_config","GenerationConfig","generate","_logits_processor","_generation_config$ma","_generation_config$ma2","_ref3","_ref4","_MODEL_WITH_LM_HEAD_M","errorMessage","model_type","possibleInfo","MODEL_WITH_LM_HEAD_MAPPING_NAMES","MODEL_FOR_SEQ_TO_SEQ_CAUSAL_LM_MAPPING_NAMES","MODEL_FOR_SPEECH_SEQ_2_SEQ_MAPPING_NAMES","MODEL_FOR_VISION_2_SEQ_MAPPING_NAMES","isTypedArray","name","is_encoder_decoder","eos_token_ids","maxOutputTokens","max_new_tokens","Infinity","useMaxLength","isInteger","sampler","Sampler","getSampler","getStartBeams","newest_beams","runBeam","output_attentions","addAttentionsToBeam","output_scores","sampledTokens","logProb","newBeam","updateBeam","groupBeams","group","sort","num_beams","callback_function","groupedBeams","getFlattened","num_return_sequences","return_dict_in_generate","decoder_attentions","cross_attentions","groups","pastKeyValues","pkvs","newName","attnName","_this$add_encoder_pkv","batch_size","add_encoder_pkv","encoder_dims","num_encoder_heads","encoder_dim_kv","decoder_dims","num_decoder_heads","decoder_dim_kv","num_decoder_layers","num_heads","dim_kv","num_layers","multi_query","keyDims","valueDims","ModelOutput","BertPreTrainedModel","ConvBertPreTrainedModel","ElectraPreTrainedModel","CamembertPreTrainedModel","DebertaPreTrainedModel","DebertaV2PreTrainedModel","DistilBertPreTrainedModel","EsmPreTrainedModel","MobileBertPreTrainedModel","MPNetPreTrainedModel","SqueezeBertPreTrainedModel","AlbertPreTrainedModel","T5PreTrainedModel","LongT5PreTrainedModel","MT5PreTrainedModel","BartPretrainedModel","MBartPreTrainedModel","BlenderbotPreTrainedModel","BlenderbotSmallPreTrainedModel","RobertaPreTrainedModel","XLMPreTrainedModel","XLMRobertaPreTrainedModel","ASTPreTrainedModel","WhisperPreTrainedModel","VisionEncoderDecoderModel","_MODEL_MAPPING_NAMES_","encoderConfig","encoder","decoderConfig","encoderModelType","MODEL_MAPPING_NAMES_ENCODER_ONLY","MODEL_MAPPING_NAMES_ENCODER_DECODER","decoderModel","decoderModelClass","num_encoder_layers","CLIPPreTrainedModel","ChineseCLIPPreTrainedModel","GPT2PreTrainedModel","n_head","n_layer","n_embd","GPTNeoPreTrainedModel","hidden_size","GPTNeoXPreTrainedModel","num_attention_heads","num_hidden_layers","GPTJPreTrainedModel","GPTBigCodePreTrainedModel","CodeGenPreTrainedModel","LlamaPreTrainedModel","_this$config$num_key_","num_key_value_heads","PhiPreTrainedModel","BloomPreTrainedModel","MptPreTrainedModel","n_heads","n_layers","d_model","OPTPreTrainedModel","ViTPreTrainedModel","VitMattePreTrainedModel","MobileViTPreTrainedModel","OwlViTPreTrainedModel","BeitPreTrainedModel","DetrPreTrainedModel","DetrObjectDetectionOutput","_ref7","pred_boxes","DetrSegmentationOutput","_ref8","pred_masks","DeiTPreTrainedModel","ResNetPreTrainedModel","SwinPreTrainedModel","Swin2SRPreTrainedModel","DPTPreTrainedModel","GLPNPreTrainedModel","DonutSwinPreTrainedModel","ConvNextPreTrainedModel","ConvNextV2PreTrainedModel","Dinov2PreTrainedModel","YolosPreTrainedModel","YolosObjectDetectionOutput","_ref9","SamPreTrainedModel","SamModel","SamImageSegmentationOutput","_ref10","iou_scores","MarianPreTrainedModel","M2M100PreTrainedModel","Wav2Vec2PreTrainedModel","WavLMPreTrainedModel","SpeechT5PreTrainedModel","SpeechT5ForTextToSpeech","decoder_layers","decoder_attention_heads","encoder_layers","encoder_attention_heads","generate_speech","input_values","speaker_embeddings","threshold","minlenratio","maxlenratio","vocoder","reduction_factor","maxlen","minlen","num_mel_bins","spectrogramParts","decoder_outputs","output_sequence","output_sequence_out","Float32Array","prob","spectrum","spectrogram","cat","waveform","SpeechT5HifiGan","TrOCRPreTrainedModel","MistralPreTrainedModel","FalconPreTrainedModel","ClapPreTrainedModel","PretrainedMixin","MODEL_CLASS_MAPPINGS","MODEL_CLASS_MAPPING","modelInfo","BASE_IF_FAIL","MODEL_MAPPING_NAMES_DECODER_ONLY","_generation_config","_generation_config$re","WhisperTimeStampLogitsProcessor","return_token_timestamps","alignment_heads","_extract_token_timestamps","num_frames","generate_outputs","median_filter_width","batchedMatrices","weights","stack","_ref6","transpose","std","calculatedMean","std_mean","smoothedWeights","aTensor","bTensor","stdTensor","meanTensor","cTensor","medianFilter","mean","timestampsShape","timestamps","batch_idx","matrix","neg","squeeze_","text_indices","time_indices","dynamicTimeWarping","diffs","jumps","jump_times","MODEL_FOR_TEXT_TO_SPECTROGRAM_MAPPING_NAMES","MODEL_FOR_SEQUENCE_CLASSIFICATION_MAPPING_NAMES","SequenceClassifierOutput","MODEL_FOR_TOKEN_CLASSIFICATION_MAPPING_NAMES","TokenClassifierOutput","d_kv","MODEL_FOR_MASKED_LM_MAPPING_NAMES","MaskedLMOutput","MODEL_FOR_QUESTION_ANSWERING_MAPPING_NAMES","QuestionAnsweringModelOutput","MODEL_FOR_DOCUMENT_QUESTION_ANSWERING_MAPPING_NAMES","MODEL_FOR_IMAGE_CLASSIFICATION_MAPPING_NAMES","MODEL_FOR_OBJECT_DETECTION_MAPPING_NAMES","MODEL_FOR_ZERO_SHOT_OBJECT_DETECTION_MAPPING_NAMES","MODEL_FOR_IMAGE_SEGMENTATION_MAPPING_NAMES","MODEL_FOR_MASK_GENERATION_MAPPING_NAMES","MODEL_FOR_CTC_MAPPING_NAMES","CausalLMOutput","MODEL_FOR_AUDIO_CLASSIFICATION_MAPPING_NAMES","MODEL_FOR_IMAGE_MATTING_MAPPING_NAMES","ImageMattingOutput","MODEL_FOR_IMAGE_TO_IMAGE_MAPPING_NAMES","MODEL_FOR_DEPTH_ESTIMATION_MAPPING_NAMES","MODEL_CLASS_TYPE_MAPPING","mappings","CUSTOM_MAPPING","_options$model_file_n3","_options$model_file_n4","_options$model_file_n5","_options$model_file_n6","AutoModel","AutoModelForSequenceClassification","AutoModelForTokenClassification","AutoModelForSeq2SeqLM","AutoModelForSpeechSeq2Seq","AutoModelForTextToSpectrogram","AutoModelForCausalLM","AutoModelForMaskedLM","AutoModelForQuestionAnswering","AutoModelForVision2Seq","AutoModelForImageClassification","AutoModelForImageSegmentation","AutoModelForObjectDetection","AutoModelForZeroShotObjectDetection","AutoModelForCTC","AutoModelForAudioClassification","AutoModelForDocumentQuestionAnswering","AutoModelForImageToImage","AutoModelForDepthEstimation","_ref11","_ref12","_ref13","_ref14","_ref15","start_logits","end_logits","_ref16","_ref18","alphas","prepareImages","images","RawImage","read","Pipeline","processor","texts","Text2TextGenerationPipeline","task_specific_params","TranslationPipeline","outputTokenIds","AutoProcessor"],"sourceRoot":""}
"use strict";(self.webpackChunkweb_portfolio=self.webpackChunkweb_portfolio||[]).push([[326],{4326:(e,t,s)=>{s.d(t,{OBj:()=>An.O,qCb:()=>a.qC,EUT:()=>Mn});var n=s(4942),o=s(2172),i=s(1873),a=s(4323),r=s(8814),l=s(7944),c=s(5357);async function d(e,t){let s=await Promise.all([(0,i.yM)(e,"tokenizer.json",!0,t),(0,i.yM)(e,"tokenizer_config.json",!0,t)]);return null!==t.legacy&&(s[1].legacy=t.legacy),s}function _(e){let t=!(arguments.length>1&&void 0!==arguments[1])||arguments[1];if(void 0!==e.Regex){const t=e.Regex.replace(/\\([#&~])/g,"$1");return new RegExp(t,"gu")}if(void 0!==e.String){const s=(0,o.hr)(e.String);return new RegExp(t?s:"(".concat(s,")"),"gu")}return console.warn("Unknown pattern type:",e),null}function u(e){return new Map(Object.entries(e))}function h(e){const t=e.dims;switch(t.length){case 1:return e.tolist();case 2:if(1!==t[0])throw new Error("Unable to decode tensor with `batch size !== 1`. Use `tokenizer.batch_decode(...)` for batched inputs.");return e.tolist()[0];default:throw new Error("Expected tensor to have 1-2 dimensions, got ".concat(t.length,"."))}}function p(e){return e.replace(/ \./g,".").replace(/ \?/g,"?").replace(/ \!/g,"!").replace(/ ,/g,",").replace(/ \' /g,"'").replace(/ n\'t/g,"n't").replace(/ \'m/g,"'m").replace(/ \'s/g,"'s").replace(/ \'ve/g,"'ve").replace(/ \'re/g,"'re")}function m(e){return e.replace(/[\u0300-\u036f]/g,"")}const g="\\p{P}\\u0021-\\u002F\\u003A-\\u0040\\u005B-\\u0060\\u007B-\\u007E";class f{constructor(e){var t,s,n,o,i;this.content=e.content,this.id=e.id,this.single_word=null!==(t=e.single_word)&&void 0!==t&&t,this.lstrip=null!==(s=e.lstrip)&&void 0!==s&&s,this.rstrip=null!==(n=e.rstrip)&&void 0!==n&&n,this.special=null!==(o=e.special)&&void 0!==o&&o,this.normalized=null!==(i=e.normalized)&&void 0!==i?i:null}}class k extends o.Ag{constructor(e){var t;super(),this.config=e,this.vocab=[],this.tokens_to_ids=new Map,this.unk_token_id=void 0,this.unk_token=void 0,this.end_of_word_suffix=void 0,this.fuse_unk=null!==(t=this.config.fuse_unk)&&void 0!==t&&t}static fromConfig(e){for(var t=arguments.length,s=new Array(t>1?t-1:0),n=1;n<t;n++)s[n-1]=arguments[n];switch(e.type){case"WordPiece":return new w(e);case"Unigram":return new x(e,...s);case"BPE":return new b(e);default:if(e.vocab)return new M(e,...s);throw new Error("Unknown TokenizerModel type: ".concat(e.type))}}_call(e){return this.encode(e)}encode(e){throw Error("encode should be implemented in subclass.")}convert_tokens_to_ids(e){let t=e.map((e=>{var t;return null!==(t=this.tokens_to_ids.get(e))&&void 0!==t?t:this.unk_token_id}));return this.fuse_unk&&(t=function(e,t){let s=[],n=0;for(;n<e.length;)if(s.push(e[n]),e[n]===t)for(;n<e.length&&e[n]===t;)++n;else++n;return s}(t,this.unk_token_id)),t}convert_ids_to_tokens(e){return e.map((e=>{var t;return null!==(t=this.vocab[e])&&void 0!==t?t:this.unk_token}))}}class w extends k{constructor(e){var t;super(e),this.tokens_to_ids=u(e.vocab),this.unk_token_id=this.tokens_to_ids.get(e.unk_token),this.unk_token=e.unk_token,this.max_input_chars_per_word=null!==(t=e.max_input_chars_per_word)&&void 0!==t?t:100,this.vocab=new Array(this.tokens_to_ids.size);for(const[s,n]of this.tokens_to_ids)this.vocab[n]=s}encode(e){let t=[];for(let s of e){let e=[...s];if(e.length>this.max_input_chars_per_word){t.push(this.unk_token);continue}let n=!1,o=0,i=[];for(;o<e.length;){let t=e.length,s=null;for(;o<t;){let n=e.slice(o,t).join("");if(o>0&&(n=this.config.continuing_subword_prefix+n),this.tokens_to_ids.has(n)){s=n;break}--t}if(null===s){n=!0;break}i.push(s),o=t}n?t.push(this.unk_token):t.push(...i)}return t}}class x extends k{constructor(e,t){super(e);const s=e.vocab.length;this.vocab=new Array(s),this.scores=new Array(s);for(let n=0;n<s;++n){const t=e.vocab[n];this.vocab[n]=t[0],this.scores[n]=t[1]}this.unk_token_id=e.unk_id,this.unk_token=this.vocab[e.unk_id],this.tokens_to_ids=new Map(this.vocab.map(((e,t)=>[e,t]))),this.bosToken=" ",this.bosTokenId=this.tokens_to_ids.get(this.bosToken),this.eosToken=t.eos_token,this.eosTokenId=this.tokens_to_ids.get(this.eosToken),this.unkToken=this.vocab[this.unk_token_id],this.minScore=(0,a.VV)(this.scores)[0],this.unkScore=this.minScore-10,this.scores[this.unk_token_id]=this.unkScore,this.trie=new l.GA,this.trie.extend(this.vocab),this.fuse_unk=!0}populateNodes(e){const t=e.sentence,s=t.length;let n=0;for(;n<s;){const s=1;let o=!1;const i=[];for(let a of this.trie.commonPrefixSearch(t.slice(n))){i.push(a);const t=this.tokens_to_ids.get(a),r=this.scores[t],l=a.length;e.insert(n,l,r,t),o||l!==s||(o=!0)}o||e.insert(n,s,this.unkScore,this.unk_token_id),n+=s}}tokenize(e){const t=new l.pQ(e,this.bosTokenId,this.eosTokenId);return this.populateNodes(t),t.tokens()}encode(e){let t=[];for(let s of e){const e=this.tokenize(s);t.push(...e)}return t}}const y=(()=>{const e=[...Array.from({length:"~".charCodeAt(0)-"!".charCodeAt(0)+1},((e,t)=>t+"!".charCodeAt(0))),...Array.from({length:"\xac".charCodeAt(0)-"\xa1".charCodeAt(0)+1},((e,t)=>t+"\xa1".charCodeAt(0))),...Array.from({length:"\xff".charCodeAt(0)-"\xae".charCodeAt(0)+1},((e,t)=>t+"\xae".charCodeAt(0)))];let t=e.slice(),s=0;for(let o=0;o<256;++o)e.includes(o)||(e.push(o),t.push(256+s),s+=1);let n=t.map((e=>String.fromCharCode(e)));return Object.fromEntries(e.map(((e,t)=>[e,n[t]])))})(),v=(0,o.$2)(y);class b extends k{constructor(e){var t,s;super(e),this.BPE_SPLIT_TOKEN=" ",this.tokens_to_ids=u(e.vocab),this.unk_token_id=this.tokens_to_ids.get(e.unk_token),this.unk_token=e.unk_token,this.vocab=new Array(this.tokens_to_ids.size);for(const[n,o]of this.tokens_to_ids)this.vocab[o]=n;this.bpe_ranks=new Map(e.merges.map(((e,t)=>[e,t]))),this.merges=e.merges.map((e=>e.split(this.BPE_SPLIT_TOKEN))),this.end_of_word_suffix=e.end_of_word_suffix,this.continuing_subword_suffix=null!==(t=e.continuing_subword_suffix)&&void 0!==t?t:null,this.byte_fallback=null!==(s=this.config.byte_fallback)&&void 0!==s&&s,this.byte_fallback&&(this.text_encoder=new TextEncoder),this.cache=new Map}bpe(e){if(0===e.length)return[];const t=this.cache.get(e);if(void 0!==t)return t;const s=Array.from(e);this.end_of_word_suffix&&(s[s.length-1]+=this.end_of_word_suffix);let n=[];if(s.length>1){const e=new l.Z3(((e,t)=>e.score<t.score));let t={token:s[0],bias:0,prev:null,next:null},o=t;for(let n=1;n<s.length;++n){const t={bias:n/s.length,token:s[n],prev:o,next:null};o.next=t,this._add_node(e,o),o=t}for(;!e.isEmpty();){const s=e.pop();if(s.deleted||!s.next||s.next.deleted)continue;if(s.deleted=!0,s.next.deleted=!0,s.prev){const e={...s.prev};s.prev.deleted=!0,s.prev=e,e.prev?e.prev.next=e:t=e}const n={token:s.token+s.next.token,bias:s.bias,prev:s.prev,next:s.next.next};n.prev?(n.prev.next=n,this._add_node(e,n.prev)):t=n,n.next&&(n.next.prev=n,this._add_node(e,n))}for(let s=t;null!==s;s=s.next)n.push(s.token)}else n=s;if(this.continuing_subword_suffix)for(let o=0;o<n.length-1;++o)n[o]+=this.continuing_subword_suffix;return this.cache.set(e,n),n}_add_node(e,t){const s=this.bpe_ranks.get(t.token+this.BPE_SPLIT_TOKEN+t.next.token);void 0!==s&&(t.score=s+t.bias,e.push(t))}encode(e){let t=[];for(let s of e){let e=this.bpe(s);for(let s of e)this.tokens_to_ids.has(s)?t.push(s):this.byte_fallback?t.push(...Array.from(this.text_encoder.encode(s)).map((e=>"<0x".concat(e.toString(16).toUpperCase().padStart(2,"0"),">")))):t.push(this.unk_token)}return t}}class M extends k{constructor(e,t){super(e),this.tokens_to_ids=u(t.target_lang?e.vocab[t.target_lang]:e.vocab),this.bos_token=t.bos_token,this.bos_token_id=this.tokens_to_ids.get(this.bos_token),this.eos_token=t.eos_token,this.eos_token_id=this.tokens_to_ids.get(this.eos_token),this.pad_token=t.pad_token,this.pad_token_id=this.tokens_to_ids.get(this.pad_token),this.unk_token=t.unk_token,this.unk_token_id=this.tokens_to_ids.get(this.unk_token),this.vocab=new Array(this.tokens_to_ids.size);for(const[s,n]of this.tokens_to_ids)this.vocab[n]=s}encode(e){return e}}class A extends o.Ag{constructor(e){super(),this.config=e}static fromConfig(e){if(null===e)return null;switch(e.type){case"BertNormalizer":return new N(e);case"Precompiled":return new ie(e);case"Sequence":return new B(e);case"Replace":return new z(e);case"NFC":return new S(e);case"NFKC":return new T(e);case"NFKD":return new C(e);case"Strip":return new F(e);case"StripAccents":return new P(e);case"Lowercase":return new E(e);case"Prepend":return new L(e);default:throw new Error("Unknown Normalizer type: ".concat(e.type))}}normalize(e){throw Error("normalize should be implemented in subclass.")}_call(e){return this.normalize(e)}}class z extends A{normalize(e){let t=_(this.config.pattern);return null===t?e:e=e.replaceAll(t,this.config.content)}}class S extends A{normalize(e){return e=e.normalize("NFC")}}class T extends A{normalize(e){return e=e.normalize("NFKC")}}class C extends A{normalize(e){return e=e.normalize("NFKD")}}class F extends A{normalize(e){return this.config.strip_left&&this.config.strip_right?e=e.trim():(this.config.strip_left&&(e=e.trimStart()),this.config.strip_right&&(e=e.trimEnd())),e}}class P extends A{normalize(e){return e=m(e)}}class E extends A{normalize(e){return e=e.toLowerCase()}}class L extends A{normalize(e){return e=this.config.prepend+e}}class B extends A{constructor(e){super(e),this.normalizers=e.normalizers.map((e=>A.fromConfig(e)))}normalize(e){return this.normalizers.reduce(((e,t)=>t.normalize(e)),e)}}class N extends A{_tokenize_chinese_chars(e){let t=[];for(let s=0;s<e.length;++s){let n=e[s],o=n.charCodeAt(0);this._is_chinese_char(o)?(t.push(" "),t.push(n),t.push(" ")):t.push(n)}return t.join("")}_is_chinese_char(e){return e>=19968&&e<=40959||e>=13312&&e<=19903||e>=131072&&e<=173791||e>=173824&&e<=177983||e>=177984&&e<=178207||e>=178208&&e<=183983||e>=63744&&e<=64255||e>=194560&&e<=195103}stripAccents(e){return e.normalize("NFD").replace(/[\u0300-\u036f]/g,"")}normalize(e){return this.config.handle_chinese_chars&&(e=this._tokenize_chinese_chars(e)),this.config.lowercase?(e=e.toLowerCase(),!1!==this.config.strip_accents&&(e=this.stripAccents(e))):this.config.strip_accents&&(e=this.stripAccents(e)),e}}class I extends o.Ag{static fromConfig(e){if(null===e)return null;switch(e.type){case"BertPreTokenizer":return new O(e);case"Sequence":return new ae(e);case"WhitespaceSplit":return new re(e);case"Metaspace":return new ne(e);case"ByteLevel":return new D(e);case"Split":return new q(e);case"Punctuation":return new j(e);case"Digits":return new G(e);case"Replace":return new le(e);default:throw new Error("Unknown PreTokenizer type: ".concat(e.type))}}pre_tokenize_text(e,t){throw Error("pre_tokenize_text should be implemented in subclass.")}pre_tokenize(e,t){let s=[];return s=Array.isArray(e)?e.map((e=>this.pre_tokenize_text(e,t))):this.pre_tokenize_text(e,t),s.flat()}_call(e,t){return this.pre_tokenize(e,t)}}class O extends I{constructor(e){super(),this.pattern=new RegExp("[^\\s".concat(g,"]+|[").concat(g,"]"),"gu")}pre_tokenize_text(e,t){return e.trim().match(this.pattern)||[]}}class D extends I{constructor(e){var t;super(),this.config=e,this.add_prefix_space=this.config.add_prefix_space,this.trim_offsets=this.config.trim_offsets,this.use_regex=null===(t=this.config.use_regex)||void 0===t||t,this.pattern=/'s|'t|'re|'ve|'m|'ll|'d| ?\p{L}+| ?\p{N}+| ?[^\s\p{L}\p{N}]+|\s+(?!\S)|\s+/gu,this.byte_encoder=y,this.text_encoder=new TextEncoder}pre_tokenize_text(e,t){return this.add_prefix_space&&!e.startsWith(" ")&&(e=" "+e),(this.use_regex?e.match(this.pattern)||[]:[e]).map((e=>Array.from(this.text_encoder.encode(e),(e=>this.byte_encoder[e])).join("")))}}class q extends I{constructor(e){super(),this.config=e,this.pattern=_(this.config.pattern,this.config.invert)}pre_tokenize_text(e,t){return null===this.pattern?[]:this.config.invert?e.match(this.pattern)||[]:function(e,t){const s=[];let n=0;for(const o of e.matchAll(t)){const t=o[0];n<o.index&&s.push(e.slice(n,o.index)),t.length>0&&s.push(t),n=o.index+t.length}return n<e.length&&s.push(e.slice(n)),s}(e,this.pattern)}}class j extends I{constructor(e){super(),this.config=e,this.pattern=new RegExp("[^".concat(g,"]+|[").concat(g,"]+"),"gu")}pre_tokenize_text(e,t){return e.match(this.pattern)||[]}}class G extends I{constructor(e){super(),this.config=e;const t="[^\\d]+|\\d".concat(this.config.individual_digits?"":"+");this.pattern=new RegExp(t,"gu")}pre_tokenize_text(e,t){return e.match(this.pattern)||[]}}class R extends o.Ag{constructor(e){super(),this.config=e}static fromConfig(e){if(null===e)return null;switch(e.type){case"TemplateProcessing":return new U(e);case"ByteLevel":return new W(e);case"RobertaProcessing":return new X(e);case"BertProcessing":return new Z(e);default:throw new Error("Unknown PostProcessor type: ".concat(e.type))}}post_process(e){throw Error("post_process should be implemented in subclass.")}_call(e){for(var t=arguments.length,s=new Array(t>1?t-1:0),n=1;n<t;n++)s[n-1]=arguments[n];return this.post_process(e,...s)}}class Z extends R{constructor(e){super(e),this.cls=e.cls[0],this.sep=e.sep[0]}post_process(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null;return e=(0,o.eG)([this.cls],e,[this.sep]),null!==t&&(e=(0,o.eG)(e,[this.sep],t,[this.sep])),e}}class X extends Z{}class U extends R{constructor(e){super(e),this.single=e.single,this.pair=e.pair}post_process(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null,s=null===t?this.single:this.pair,n=[];for(let i of s)"SpecialToken"in i?n.push(i.SpecialToken.id):"Sequence"in i&&("A"===i.Sequence.id?n=(0,o.eG)(n,e):"B"===i.Sequence.id&&(n=(0,o.eG)(n,t)));return n}}class W extends R{post_process(e){return e}}class V extends o.Ag{constructor(e){super(),this.config=e,this.added_tokens=[],this.end_of_word_suffix=null,this.trim_offsets=e.trim_offsets}static fromConfig(e){if(null===e)return null;switch(e.type){case"WordPiece":return new J(e);case"Metaspace":return new oe(e);case"ByteLevel":return new $(e);case"Replace":return new K(e);case"ByteFallback":return new Q(e);case"Fuse":return new Y(e);case"Strip":return new H(e);case"Sequence":return new te(e);case"CTC":return new ee(e);case"BPEDecoder":return new se(e);default:throw new Error("Unknown Decoder type: ".concat(e.type))}}_call(e){return this.decode(e)}decode(e){return this.decode_chain(e).join("")}decode_chain(e){throw Error("`decode_chain` should be implemented in subclass.")}}class K extends V{decode_chain(e){let t=_(this.config.pattern);return null===t?e:e.map((e=>e.replaceAll(t,this.config.content)))}}class Q extends V{constructor(e){super(e),this.text_decoder=new TextDecoder}decode_chain(e){let t=[],s=[];for(let n of e){let e=null;if(6===n.length&&n.startsWith("<0x")&&n.endsWith(">")){let t=parseInt(n.slice(3,5),16);isNaN(t)||(e=t)}if(null!==e)s.push(e);else{if(s.length>0){let e=this.text_decoder.decode(Uint8Array.from(s));t.push(e),s=[]}t.push(n)}}if(s.length>0){let e=this.text_decoder.decode(Uint8Array.from(s));t.push(e),s=[]}return t}}class Y extends V{decode_chain(e){return[e.join("")]}}class H extends V{constructor(e){super(e),this.content=this.config.content,this.start=this.config.start,this.stop=this.config.stop}decode_chain(e){return e.map((e=>{let t=0;for(let n=0;n<this.start&&e[n]===this.content;++n)t=n+1;let s=e.length;for(let n=0;n<this.stop;++n){const t=e.length-n-1;if(e[t]!==this.content)break;s=t}return e.slice(t,s)}))}}class J extends V{constructor(e){super(e),this.cleanup=e.cleanup}decode_chain(e){return e.map(((e,t)=>(0!==t&&(e=e.startsWith(this.config.prefix)?e.replace(this.config.prefix,""):" "+e),this.cleanup&&(e=p(e)),e)))}}class $ extends V{constructor(e){super(e),this.byte_decoder=v,this.text_decoder=new TextDecoder("utf-8",{fatal:!1,ignoreBOM:!0}),this.end_of_word_suffix=null}convert_tokens_to_string(e){let t=e.join(""),s=new Uint8Array([...t].map((e=>this.byte_decoder[e])));return this.text_decoder.decode(s)}decode_chain(e){let t=[],s=[];for(let n of e)void 0!==this.added_tokens.find((e=>e.content===n))?(s.length>0&&(t.push(this.convert_tokens_to_string(s)),s=[]),t.push(n)):s.push(n);return s.length>0&&t.push(this.convert_tokens_to_string(s)),t}}class ee extends V{constructor(e){super(e),this.pad_token=this.config.pad_token,this.word_delimiter_token=this.config.word_delimiter_token,this.cleanup=this.config.cleanup}convert_tokens_to_string(e){if(0===e.length)return"";let t=[e[0]];for(let n=1;n<e.length;++n)e[n]!==t.at(-1)&&t.push(e[n]);let s=t.filter((e=>e!==this.pad_token)).join("");return this.cleanup&&(s=p(s).replaceAll(this.word_delimiter_token," ").trim()),s}decode_chain(e){return[this.convert_tokens_to_string(e)]}}class te extends V{constructor(e){super(e),this.decoders=e.decoders.map((e=>V.fromConfig(e)))}decode_chain(e){return this.decoders.reduce(((e,t)=>t.decode_chain(e)),e)}}class se extends V{constructor(e){super(e),this.suffix=this.config.suffix}decode_chain(e){return e.map(((t,s)=>t.replaceAll(this.suffix,s===e.length-1?"":" ")))}}class ne extends I{constructor(e){var t;super(),this.addPrefixSpace=e.add_prefix_space,this.replacement=e.replacement,this.strRep=e.str_rep||this.replacement,this.prepend_scheme=null!==(t=e.prepend_scheme)&&void 0!==t?t:"always"}pre_tokenize_text(e){let{section_index:t}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},s=e.replaceAll(" ",this.strRep);return this.addPrefixSpace&&!s.startsWith(this.replacement)&&("always"===this.prepend_scheme||"first"===this.prepend_scheme&&0===t)&&(s=this.strRep+s),[s]}}class oe extends V{constructor(e){super(e),this.addPrefixSpace=e.add_prefix_space,this.replacement=e.replacement}decode_chain(e){let t=[];for(let s=0;s<e.length;++s){let n=e[s].replaceAll(this.replacement," ");this.addPrefixSpace&&0==s&&n.startsWith(" ")&&(n=n.substring(1)),t.push(n)}return t}}class ie extends A{constructor(e){super(e),this.charsmap=e.precompiled_charsmap}normalize(e){if((e=(e=e.replace(/[\u0001-\u0008\u000B\u000E-\u001F\u007F\u008F\u009F]/gm,"")).replace(/[\u0009\u000A\u000C\u000D\u1680\u200B\u200C\u200E\u200F\u2028\u2029\u2581\uFEFF\uFFFD]/gm," ")).includes("\uff5e")){const t=e.split("\uff5e");e=t.map((e=>e.normalize("NFKC"))).join("\uff5e")}else e=e.normalize("NFKC");return e}}class ae extends I{constructor(e){super(),this.tokenizers=e.pretokenizers.map((e=>I.fromConfig(e)))}pre_tokenize_text(e,t){return this.tokenizers.reduce(((e,s)=>s.pre_tokenize(e,t)),[e])}}class re extends I{constructor(e){super()}pre_tokenize_text(e,t){return function(e){return e.match(/\S+/g)||[]}(e)}}class le extends I{constructor(e){super(),this.config=e,this.pattern=_(this.config.pattern),this.content=this.config.content}pre_tokenize_text(e,t){return null===this.pattern?[e]:[e.replaceAll(this.pattern,this.config.content)]}}const ce=["bos_token","eos_token","unk_token","sep_token","pad_token","cls_token","mask_token"];class de extends o.Ag{constructor(e,t){var s,i,a,r;super(),(0,n.Z)(this,"_default_chat_template","{% for message in messages %}{{'<|im_start|>' + message['role'] + '\n' + message['content'] + '<|im_end|>' + '\n'}}{% endfor %}{% if add_generation_prompt %}{{ '<|im_start|>assistant\n' }}{% endif %}"),this._tokenizer_config=t,this.normalizer=A.fromConfig(e.normalizer),this.pre_tokenizer=I.fromConfig(e.pre_tokenizer),this.model=k.fromConfig(e.model,t),this.post_processor=R.fromConfig(e.post_processor),this.decoder=V.fromConfig(e.decoder),this.special_tokens=[],this.all_special_ids=[],this.added_tokens=[];for(let n of e.added_tokens){const e=new f(n);this.added_tokens.push(e),this.model.tokens_to_ids.set(e.content,e.id),this.model.vocab[e.id]=e.content,e.special&&(this.special_tokens.push(e.content),this.all_special_ids.push(e.id))}this.additional_special_tokens=null!==(s=t.additional_special_tokens)&&void 0!==s?s:[],this.special_tokens.push(...this.additional_special_tokens),this.special_tokens=[...new Set(this.special_tokens)],this.decoder&&(this.decoder.added_tokens=this.added_tokens,this.decoder.end_of_word_suffix=this.model.end_of_word_suffix),this.added_tokens_regex=this.added_tokens.length>0?new RegExp(this.added_tokens.map((e=>"".concat(e.lstrip?"\\s*":"","(").concat((0,o.hr)(e.content),")").concat(e.rstrip?"\\s*":""))).join("|")):null,this.mask_token=this.getToken("mask_token"),this.mask_token_id=this.model.tokens_to_ids.get(this.mask_token),this.pad_token=this.getToken("pad_token","eos_token"),this.pad_token_id=this.model.tokens_to_ids.get(this.pad_token),this.sep_token=this.getToken("sep_token"),this.sep_token_id=this.model.tokens_to_ids.get(this.sep_token),this.unk_token=this.getToken(t,"unk_token"),this.unk_token_id=this.model.tokens_to_ids.get(this.unk_token),this.model_max_length=t.model_max_length,this.remove_space=t.remove_space,this.clean_up_tokenization_spaces=null===(i=t.clean_up_tokenization_spaces)||void 0===i||i,this.do_lowercase_and_remove_accent=null!==(a=t.do_lowercase_and_remove_accent)&&void 0!==a&&a,this.padding_side="right",this.legacy=!1,this.chat_template=null!==(r=t.chat_template)&&void 0!==r?r:null,this._compiled_template_cache=new Map}getToken(){for(var e=arguments.length,t=new Array(e),s=0;s<e;s++)t[s]=arguments[s];for(let n of t){let e=this._tokenizer_config[n];if(e){if("object"===typeof e){if("AddedToken"===e.__type)return e.content;throw Error("Unknown token: ".concat(e))}return e}}return null}static async from_pretrained(e){let{progress_callback:t=null,config:s=null,cache_dir:n=null,local_files_only:o=!1,revision:i="main",legacy:a=null}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return new this(...await d(e,{progress_callback:t,config:s,cache_dir:n,local_files_only:o,revision:i,legacy:a}))}prepare_model_inputs(e){return e}_call(e){let t,{text_pair:s=null,add_special_tokens:n=!0,padding:o=!1,truncation:i=null,max_length:l=null,return_tensor:c=!0}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if(Array.isArray(e)){if(0===e.length)throw Error("text array must be non-empty");if(null!==s){if(!Array.isArray(s))throw Error("text_pair must also be an array");if(e.length!==s.length)throw Error("text and text_pair must have the same length");t=e.map(((e,t)=>this.encode(e,s[t],{add_special_tokens:n})))}else t=e.map((e=>this.encode(e,null,{add_special_tokens:n})))}else{if(null===e)throw Error("text may not be null");if(Array.isArray(s))throw Error("When specifying `text_pair`, since `text` is a string, `text_pair` must also be a string (i.e., not an array).");t=[this.encode(e,s,{add_special_tokens:n})]}let d=(0,a.Fp)(t.map((e=>e.length)))[0];null===l&&(l=d),l=Math.min(l,this.model_max_length);let _=[];if(o||i)for(let a=0;a<t.length;++a)if(t[a].length!==l)if(t[a].length>l)i&&(t[a]=t[a].slice(0,l)),_.push(new Array(t[a].length).fill(1));else if(o){let e=l-t[a].length;"right"===this.padding_side?(_.push(new Array(t[a].length).fill(1).concat(new Array(e).fill(0))),t[a].push(...new Array(e).fill(this.pad_token_id))):(_.push(new Array(e).fill(0).concat(new Array(t[a].length).fill(1))),t[a].unshift(...new Array(e).fill(this.pad_token_id)))}else _.push(new Array(t[a].length).fill(1));else _.push(new Array(t[a].length).fill(1));else _=t.map((e=>new Array(e.length).fill(1)));if(c){if((!o||!i)&&t.some((e=>e.length!==t[0].length)))throw Error("Unable to create tensor, you should probably activate truncation and/or padding with 'padding=true' and 'truncation=true' to have batched tensors with the same length.");let e=[t.length,t[0].length];t=new r.es("int64",BigInt64Array.from(t.flat().map(BigInt)),e),_=new r.es("int64",BigInt64Array.from(_.flat().map(BigInt)),e)}else Array.isArray(e)||(t=t[0],_=_[0]);let u={input_ids:t,attention_mask:_};return u=this.prepare_model_inputs(u),u}_encode_text(e){if(null===e)return null;const t=(this.added_tokens_regex?e.split(this.added_tokens_regex).filter((e=>e)):[e]).map(((e,t)=>{if(void 0!==this.added_tokens.find((t=>t.content===e)))return e;{!0===this.remove_space&&(e=e.trim().split(/\s+/).join(" ")),this.do_lowercase_and_remove_accent&&(e=function(e){return m(e.toLowerCase())}(e)),null!==this.normalizer&&(e=this.normalizer(e));const s=null!==this.pre_tokenizer?this.pre_tokenizer(e,{section_index:t}):[e];return this.model(s)}})).flat();return t}encode(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null,{add_special_tokens:s=!0}=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{},n=this._encode_text(e),i=this._encode_text(t),a=null!==this.post_processor&&s?this.post_processor(n,i):(0,o.eG)(null!==n&&void 0!==n?n:[],null!==i&&void 0!==i?i:[]);return this.model.convert_tokens_to_ids(a)}batch_decode(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return e instanceof r.es&&(e=e.tolist()),e.map((e=>this.decode(e,t)))}decode(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if(e instanceof r.es&&(e=h(e)),!Array.isArray(e)||0===e.length||!(0,o.Wy)(e[0]))throw Error("token_ids must be a non-empty array of integers.");return this.decode_single(e,t)}decode_single(e,t){let{skip_special_tokens:s=!1,clean_up_tokenization_spaces:n=null}=t,o=this.model.convert_ids_to_tokens(e);s&&(o=o.filter((e=>!this.special_tokens.includes(e))));let i=this.decoder?this.decoder(o):o.join(" ");return this.decoder&&this.decoder.end_of_word_suffix&&(i=i.replaceAll(this.decoder.end_of_word_suffix," "),s&&(i=i.trim())),(null!==n&&void 0!==n?n:this.clean_up_tokenization_spaces)&&(i=p(i)),i}get default_chat_template(){return this._warned_about_chat_template||(console.warn("No chat template is defined for this tokenizer - using a default chat template that implements the ChatML format. If the default is not appropriate for your model, please set `tokenizer.chat_template` to an appropriate template. See https://huggingface.co/docs/transformers/main/chat_templating for more information."),this._warned_about_chat_template=!0),this._default_chat_template}apply_chat_template(e){var t,s;let{chat_template:n=null,add_generation_prompt:o=!1,tokenize:i=!0,padding:a=!1,truncation:r=!1,max_length:l=null,return_tensor:d=!0}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};null!==(t=n)&&void 0!==t||(n=null!==(s=this.chat_template)&&void 0!==s?s:this.default_chat_template);let _=this._compiled_template_cache.get(n);void 0===_&&(_=new c.YS(n),this._compiled_template_cache.set(n,_));const u=Object.create(null);for(const c of ce){const e=this.getToken(c);e&&(u[c]=e)}const h=_.render({messages:e,add_generation_prompt:o,...u});return i?this._call(h,{add_special_tokens:!1,padding:a,truncation:r,max_length:l,return_tensor:d}).input_ids:h}}function _e(e){if(e.input_ids instanceof r.es)e.token_type_ids=new r.es("int64",new BigInt64Array(e.input_ids.data.length),e.input_ids.dims);else{if(!Array.isArray(e.input_ids))throw new Error("Input ids must be a Tensor or an Array");Array.isArray(e.input_ids[0])?e.token_type_ids=e.input_ids.map((e=>new Array(e.length).fill(0))):e.token_type_ids=new Array(e.input_ids.length).fill(0)}return e}class ue extends de{constructor(){super(...arguments),(0,n.Z)(this,"_default_chat_template",'{% for message in messages %}" "{{ message.content }}{{ eos_token }}" "{% endfor %}')}}class he extends de{constructor(e,t){super(e,t),this.languageRegex=/^[a-z]{2}_[A-Z]{2}$/,this.language_codes=this.special_tokens.filter((e=>this.languageRegex.test(e))),this.lang_to_token=e=>e}_build_translation_inputs(e,t,s){return ge(this,e,t,s)}}const pe="\u2581";class me extends de{constructor(e,t){var s,o;super(e,t),(0,n.Z)(this,"_default_chat_template","{% if messages[0]['role'] == 'system' %}{% set loop_messages = messages[1:] %}{% set system_message = messages[0]['content'] %}{% elif USE_DEFAULT_PROMPT == true and not '<<SYS>>' in messages[0]['content'] %}{% set loop_messages = messages %}{% set system_message = 'DEFAULT_SYSTEM_MESSAGE' %}{% else %}{% set loop_messages = messages %}{% set system_message = false %}{% endif %}{% for message in loop_messages %}{% if (message['role'] == 'user') != (loop.index0 % 2 == 0) %}{{ raise_exception('Conversation roles must alternate user/assistant/user/assistant/...') }}{% endif %}{% if loop.index0 == 0 and system_message != false %}{% set content = '<<SYS>>\n' + system_message + '\n<</SYS>>\n\n' + message['content'] %}{% else %}{% set content = message['content'] %}{% endif %}{% if message['role'] == 'user' %}{{ bos_token + '[INST] ' + content.strip() + ' [/INST]' }}{% elif message['role'] == 'system' %}{{ '<<SYS>>\n' + content.strip() + '\n<</SYS>>\n\n' }}{% elif message['role'] == 'assistant' %}{{ ' '  + content.strip() + ' ' + eos_token }}{% endif %}{% endfor %}"),(0,n.Z)(this,"DEFAULT_SYSTEM_PROMPT","You are a helpful, respectful and honest assistant. Always answer as helpfully as possible, while being safe. Your answers should not include any harmful, unethical, racist, sexist, toxic, dangerous, or illegal content. Please ensure that your responses are socially unbiased and positive in nature.\n\nIf a question does not make any sense, or is not factually coherent, explain why instead of answering something not correct. If you don't know the answer to a question, please don't share false information."),this.use_default_system_prompt=null!==(s=t.use_default_system_prompt)&&void 0!==s&&s,this.legacy=null===(o=t.legacy)||void 0===o||o,this.legacy||(this.normalizer=null,this.pre_tokenizer=new ne({replacement:pe,add_prefix_space:!0,prepend_scheme:"first"}))}_encode_text(e){if(null===e)return null;if(this.legacy||0===e.length)return super._encode_text(e);let t=super._encode_text(pe+e.replaceAll(pe," "));return t.length>1&&t[0]===pe&&this.special_tokens.includes(t[1])&&(t=t.slice(1)),t}get default_chat_template(){return super.default_chat_template.replaceAll("USE_DEFAULT_PROMPT",this.use_default_system_prompt?"true":"false").replaceAll("DEFAULT_SYSTEM_MESSAGE",this.DEFAULT_SYSTEM_PROMPT.replaceAll("\n","\\n").replaceAll("'","\\'"))}}function ge(e,t,s,n){if(!("language_codes"in e)||!Array.isArray(e.language_codes))throw new Error("Tokenizer must have `language_codes` attribute set and it should be an array of language ids.");if(!("languageRegex"in e)||!(e.languageRegex instanceof RegExp))throw new Error("Tokenizer must have `languageRegex` attribute set and it should be a regular expression.");if(!("lang_to_token"in e)||"function"!==typeof e.lang_to_token)throw new Error("Tokenizer must have `lang_to_token` attribute set and it should be a function.");const o=n.src_lang,i=n.tgt_lang;if(!e.language_codes.includes(i))throw new Error('Target language code "'.concat(i,'" is not valid. Must be one of: {').concat(e.language_codes.join(", "),"}"));if(void 0!==o){if(!e.language_codes.includes(o))throw new Error('Source language code "'.concat(o,'" is not valid. Must be one of: {').concat(e.language_codes.join(", "),"}"));for(let t of e.post_processor.config.single)if("SpecialToken"in t&&e.languageRegex.test(t.SpecialToken.id)){t.SpecialToken.id=e.lang_to_token(o);break}}return n.forced_bos_token_id=e.model.convert_tokens_to_ids([e.lang_to_token(i)])[0],e._call(t,s)}const fe=[["en","english"],["zh","chinese"],["de","german"],["es","spanish"],["ru","russian"],["ko","korean"],["fr","french"],["ja","japanese"],["pt","portuguese"],["tr","turkish"],["pl","polish"],["ca","catalan"],["nl","dutch"],["ar","arabic"],["sv","swedish"],["it","italian"],["id","indonesian"],["hi","hindi"],["fi","finnish"],["vi","vietnamese"],["he","hebrew"],["uk","ukrainian"],["el","greek"],["ms","malay"],["cs","czech"],["ro","romanian"],["da","danish"],["hu","hungarian"],["ta","tamil"],["no","norwegian"],["th","thai"],["ur","urdu"],["hr","croatian"],["bg","bulgarian"],["lt","lithuanian"],["la","latin"],["mi","maori"],["ml","malayalam"],["cy","welsh"],["sk","slovak"],["te","telugu"],["fa","persian"],["lv","latvian"],["bn","bengali"],["sr","serbian"],["az","azerbaijani"],["sl","slovenian"],["kn","kannada"],["et","estonian"],["mk","macedonian"],["br","breton"],["eu","basque"],["is","icelandic"],["hy","armenian"],["ne","nepali"],["mn","mongolian"],["bs","bosnian"],["kk","kazakh"],["sq","albanian"],["sw","swahili"],["gl","galician"],["mr","marathi"],["pa","punjabi"],["si","sinhala"],["km","khmer"],["sn","shona"],["yo","yoruba"],["so","somali"],["af","afrikaans"],["oc","occitan"],["ka","georgian"],["be","belarusian"],["tg","tajik"],["sd","sindhi"],["gu","gujarati"],["am","amharic"],["yi","yiddish"],["lo","lao"],["uz","uzbek"],["fo","faroese"],["ht","haitian creole"],["ps","pashto"],["tk","turkmen"],["nn","nynorsk"],["mt","maltese"],["sa","sanskrit"],["lb","luxembourgish"],["my","myanmar"],["bo","tibetan"],["tl","tagalog"],["mg","malagasy"],["as","assamese"],["tt","tatar"],["haw","hawaiian"],["ln","lingala"],["ha","hausa"],["ba","bashkir"],["jw","javanese"],["su","sundanese"]],ke=new Map(fe),we=new Map([...fe.map((e=>{let[t,s]=e;return[s,t]})),["burmese","my"],["valencian","ca"],["flemish","nl"],["haitian","ht"],["letzeburgesch","lb"],["pushto","ps"],["panjabi","pa"],["moldavian","ro"],["moldovan","ro"],["sinhalese","si"],["castilian","es"]]);class xe extends de{constructor(){super(...arguments),(0,n.Z)(this,"_default_chat_template","{% for message in messages %}{% if message['role'] == 'user' %}{{ ' ' }}{% endif %}{{ message['content'] }}{% if not loop.last %}{{ '  ' }}{% endif %}{% endfor %}{{ eos_token }}")}}class ye{static async from_pretrained(e){var t,s;let{quantized:n=!0,progress_callback:o=null,config:i=null,cache_dir:a=null,local_files_only:r=!1,revision:l="main",legacy:c=null}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},[_,u]=await d(e,{quantized:n,progress_callback:o,config:i,cache_dir:a,local_files_only:r,revision:l,legacy:c}),h=null!==(t=null===(s=u.tokenizer_class)||void 0===s?void 0:s.replace(/Fast$/,""))&&void 0!==t?t:"PreTrainedTokenizer",p=this.TOKENIZER_CLASS_MAPPING[h];return p||(console.warn('Unknown tokenizer class "'.concat(h,'", attempting to construct from base class.')),p=de),new p(_,u)}}(0,n.Z)(ye,"TOKENIZER_CLASS_MAPPING",{T5Tokenizer:class extends de{},DistilBertTokenizer:class extends de{},CamembertTokenizer:class extends de{},DebertaTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},DebertaV2Tokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},BertTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},HerbertTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},ConvBertTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},XLMTokenizer:class extends de{constructor(e,t){super(e,t),console.warn('WARNING: `XLMTokenizer` is not yet supported by Hugging Face\'s "fast" tokenizers library. Therefore, you may experience slightly inaccurate results.')}prepare_model_inputs(e){return _e(e)}},ElectraTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},MobileBertTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},SqueezeBertTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},AlbertTokenizer:class extends de{prepare_model_inputs(e){return _e(e)}},GPT2Tokenizer:ue,BartTokenizer:class extends de{},MBartTokenizer:he,MBart50Tokenizer:class extends he{},RobertaTokenizer:class extends de{},WhisperTokenizer:class extends de{constructor(){super(...arguments),(0,n.Z)(this,"_default_chat_template",'{% for message in messages %}" "{{ message.content }}{{ eos_token }}" "{% endfor %}')}_decode_asr(e){let{return_timestamps:t=!1,return_language:s=!1,time_precision:n=null,force_full_sequences:o=!0}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if(null===n)throw Error("Must specify time_precision");let i=null;const r="word"===t;function l(){return{language:i,timestamp:[null,null],text:""}}const c=[];let d=l(),_=0;const u=this.model.convert_tokens_to_ids(["<|notimestamps|>"])[0]+1;let h=[],p=[],m=!1,g=null;const f=new Set(this.all_special_ids);for(let x of e){const e=x.tokens,s=r?x.token_timestamps:null;let o=null,k=u;if("stride"in x){const[t,s,i]=x.stride;if(_-=s,g=t-i,s&&(k=s/n+u),i)for(let a=e.length-1;a>=0;--a){const t=e[a];if(t>=u){if(null!==o&&(t-u)*n<g)break;o=t}}}let w=[],y=[];for(let g=0;g<e.length;++g){const x=e[g];if(f.has(x)){const e=this.decode([x]),s=ke.get(e.slice(2,-2));if(void 0!==s){if(null!==i&&s!==i&&!t){h.push(w);const e=this.findLongestCommonSequence(h)[0],t=this.decode(e);d.text=t,c.push(d),h=[],w=[],d=l()}i=d.language=s}}else if(x>=u){const e=(x-u)*n+_,t=(0,a.NM)(e,2);if(null!==o&&x>=o)m=!0;else if(m||h.length>0&&x<k)m=!1;else if(null===d.timestamp[0])d.timestamp[0]=t;else if(t===d.timestamp[0]);else{d.timestamp[1]=t,h.push(w),r&&p.push(y);const[e,s]=this.findLongestCommonSequence(h,p),n=this.decode(e);d.text=n,r&&(d.words=this.collateWordTimestamps(e,s,i)),c.push(d),h=[],w=[],p=[],y=[],d=l()}}else if(w.push(x),r){let e,t=(0,a.NM)(s[g]+_,2);e=g+1<s.length?(0,a.NM)(s[g+1]+_,2):null,y.push([t,e])}}if("stride"in x){const[e,t,s]=x.stride;_+=e-s}w.length>0?(h.push(w),r&&p.push(y)):h.every((e=>0===e.length))&&(d=l(),h=[],w=[],p=[],y=[])}if(h.length>0){if(o&&t)throw new Error("Whisper did not predict an ending timestamp, which can happen if audio is cut off in the middle of a word. Also make sure WhisperTimeStampLogitsProcessor was used during generation.");const[e,s]=this.findLongestCommonSequence(h,p),n=this.decode(e);d.text=n,r&&(d.words=this.collateWordTimestamps(e,s,i)),c.push(d)}let k=Object.create(null);const w=c.map((e=>e.text)).join("");if(t||s){for(let e=0;e<c.length;++e){const n=c[e];t||delete n.timestamp,s||delete n.language}if(r){let e=[];for(let t of c)for(let s of t.words)e.push(s);k={chunks:e}}else k={chunks:c}}return[w,k]}findLongestCommonSequence(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null,s=e[0],n=s.length,o=[];const i=Array.isArray(t)&&t.length>0;let a=i?[]:null,r=i?t[0]:null;for(let l=1;l<e.length;++l){const c=e[l];let d=0,_=[n,n,0,0];const u=c.length;for(let e=1;e<n+u;++e){const t=e/1e4,o=Math.max(0,n-e),i=Math.min(n,n+u-e),a=s.slice(o,i),r=Math.max(0,e-n),l=Math.min(u,e),h=c.slice(r,l);if(a.length!==h.length)throw new Error("There is a bug within whisper `decode_asr` function, please report it. Dropping to prevent bad inference.");const p=a.filter(((e,t)=>e===h[t])).length,m=p/e+t;p>1&&m>d&&(d=m,_=[o,i,r,l])}const[h,p,m,g]=_,f=Math.floor((p+h)/2),k=Math.floor((g+m)/2);o.push(...s.slice(0,f)),s=c.slice(k),n=s.length,i&&(a.push(...r.slice(0,f)),r=t[l].slice(k))}return o.push(...s),i?(a.push(...r),[o,a]):[o,[]]}collateWordTimestamps(e,t,s){let[n,o,i]=this.combineTokensIntoWords(e,s),a=[];for(let r=0;r<n.length;++r){const e=i[r];a.push({text:n[r],timestamp:[t[e.at(0)][0],t[e.at(-1)][1]]})}return a}combineTokensIntoWords(e,t){var s;let n,o,i,a=arguments.length>2&&void 0!==arguments[2]?arguments[2]:"\"'\u201c\xa1\xbf([{-",r=arguments.length>3&&void 0!==arguments[3]?arguments[3]:"\"'.\u3002,\uff0c!\uff01?\uff1f:\uff1a\u201d)]}\u3001";return t=null!==(s=t)&&void 0!==s?s:"english",["chinese","japanese","thai","lao","myanmar"].includes(t)?[n,o,i]=this.splitTokensOnUnicode(e):[n,o,i]=this.splitTokensOnSpaces(e),this.mergePunctuations(n,o,i,a,r)}decode(e,t){let s;return t&&t.decode_with_timestamps?(e instanceof r.es&&(e=h(e)),s=this.decodeWithTimestamps(e,t)):s=super.decode(e,t),s}decodeWithTimestamps(e,t){var s;const n=null!==(s=null===t||void 0===t?void 0:t.time_precision)&&void 0!==s?s:.02,o=Array.from(this.all_special_ids).at(-1)+1;let i=[[]];for(let r of e)if(r>=o){let e=(r-o)*n;e=(0,a.NM)(e,2),i.push("<|".concat(e,"|>")),i.push([])}else i[i.length-1].push(r);return i=i.map((e=>"string"===typeof e?e:super.decode(e,t))),i.join("")}splitTokensOnUnicode(e){const t=this.decode(e,{decode_with_timestamps:!0});let s=[],n=[],o=[],i=[],a=[],r=0;for(let l=0;l<e.length;++l){const c=e[l];i.push(c),a.push(l);const d=this.decode(i,{decode_with_timestamps:!0});d.includes("\ufffd")&&"\ufffd"!==t[r+d.indexOf("\ufffd")]||(s.push(d),n.push(i),o.push(a),i=[],a=[],r+=d.length)}return[s,n,o]}splitTokensOnSpaces(e){let[t,s,n]=this.splitTokensOnUnicode(e),o=[],i=[],a=[];const r=new RegExp("^[".concat(g,"]$"),"gu");for(let l=0;l<t.length;++l){const e=t[l],c=s[l],d=n[l],_=c[0]>=this.model.tokens_to_ids.get("<|endoftext|>"),u=e.startsWith(" "),h=e.trim(),p=r.test(h);if(_||u||p||0===o.length)o.push(e),i.push(c),a.push(d);else{const t=o.length-1;o[t]+=e,i[t].push(...c),a[t].push(...d)}}return[o,i,a]}mergePunctuations(e,t,s,n,i){let a=structuredClone(e),r=structuredClone(t),l=structuredClone(s),c=a.length-2,d=a.length-1;for(;c>=0;)a[c].startsWith(" ")&&n.includes(a[c].trim())?(a[d]=a[c]+a[d],r[d]=(0,o.eG)(r[c],r[d]),l[d]=(0,o.eG)(l[c],l[d]),a[c]="",r[c]=[],l[c]=[]):d=c,--c;for(c=0,d=1;d<a.length;)!a[c].endsWith(" ")&&i.includes(a[d])?(a[c]+=a[d],r[c]=(0,o.eG)(r[c],r[d]),l[c]=(0,o.eG)(l[c],l[d]),a[d]="",r[d]=[],l[d]=[]):c=d,++d;return[a.filter((e=>e)),r.filter((e=>e.length>0)),l.filter((e=>e.length>0))]}get_decoder_prompt_ids(){let{language:e=null,task:t=null,no_timestamps:s=!0}=arguments.length>0&&void 0!==arguments[0]?arguments[0]:{},n=[];if(e){e=e.toLowerCase();let t=we.get(e);if(void 0===t){if(!ke.has(e)){const t=2===e.length?ke.keys():ke.values();throw new Error('Language "'.concat(e,'" is not supported. Must be one of: ').concat(JSON.stringify(t)))}t=e}let s=this.model.tokens_to_ids.get("<|".concat(t,"|>"));if(void 0===s)throw new Error('Unable to find language "'.concat(t,'" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.'));n.push(s)}else n.push(null);if(t){if(t=t.toLowerCase(),"transcribe"!==t&&"translate"!==t)throw new Error('Task "'.concat(t,'" is not supported. Must be one of: ["transcribe", "translate"]'));let e=this.model.tokens_to_ids.get("<|".concat(t,"|>"));if(void 0===e)throw new Error('Unable to find task "'.concat(t,'" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.'));n.push(e)}else n.push(null);if(s){let e=this.model.tokens_to_ids.get("<|notimestamps|>");if(void 0===e)throw new Error('Unable to find "<|notimestamps|>" in model vocabulary. Please report this issue at https://github.com/xenova/transformers.js/issues/new/choose.');n.push(e)}return n.map(((e,t)=>[t+1,e])).filter((e=>null!==e[1]))}},CodeGenTokenizer:class extends de{},CLIPTokenizer:class extends de{},MarianTokenizer:class extends de{constructor(e,t){super(e,t),this.languageRegex=/^(>>\w+<<)\s*/g,this.supported_language_codes=this.model.vocab.filter((e=>this.languageRegex.test(e))),console.warn('WARNING: `MarianTokenizer` is not yet supported by Hugging Face\'s "fast" tokenizers library. Therefore, you may experience slightly inaccurate results.')}_encode_text(e){if(null===e)return null;let[t,...s]=e.trim().split(this.languageRegex);if(0===s.length)return super._encode_text(t);if(2===s.length){let[e,t]=s;return this.supported_language_codes.includes(e)||console.warn('Unsupported language code "'.concat(e,'" detected, which may lead to unexpected behavior. Should be one of: ').concat(JSON.stringify(this.supported_language_codes))),(0,o.eG)([e],super._encode_text(t))}}},BloomTokenizer:class extends ue{constructor(e,t){var s;const n=".,!?\u2026\u3002\uff0c\u3001\u0964\u06d4\u060c",o=null===(s=e.pre_tokenizer)||void 0===s||null===(s=s.pretokenizers[0])||void 0===s?void 0:s.pattern;o&&o.Regex===" ?[^(\\s|[".concat(n,"])]+")&&(o.Regex=" ?[^\\s".concat(n,"]+")),super(e,t)}},NllbTokenizer:class extends de{constructor(e,t){super(e,t),this.languageRegex=/^[a-z]{3}_[A-Z][a-z]{3}$/,this.language_codes=this.special_tokens.filter((e=>this.languageRegex.test(e))),this.lang_to_token=e=>e}_build_translation_inputs(e,t,s){return ge(this,e,t,s)}},M2M100Tokenizer:class extends de{constructor(e,t){super(e,t),this.languageRegex=/^__[a-z]{2,3}__$/,this.language_codes=this.special_tokens.filter((e=>this.languageRegex.test(e))).map((e=>e.slice(2,-2))),this.lang_to_token=e=>"__".concat(e,"__")}_build_translation_inputs(e,t,s){return ge(this,e,t,s)}},LlamaTokenizer:me,CodeLlamaTokenizer:class extends me{},XLMRobertaTokenizer:class extends de{},MPNetTokenizer:class extends de{},FalconTokenizer:class extends de{},GPTNeoXTokenizer:class extends de{},EsmTokenizer:class extends de{},Wav2Vec2CTCTokenizer:class extends de{},BlenderbotTokenizer:xe,BlenderbotSmallTokenizer:class extends xe{},SpeechT5Tokenizer:class extends de{},NougatTokenizer:class extends de{},PreTrainedTokenizer:de});var ve=s(2146),be=s(3472),Me=s(5664);const{InferenceSession:Ae,Tensor:ze,env:Se}=Me.ONNX,Te=0,Ce=1,Fe=2,Pe=3,Ee=4,Le=new Map,Be=new Map,Ne=new Map;async function Ie(e,t,s){let n="onnx/".concat(t).concat(s.quantized?"_quantized":"",".onnx"),o=await(0,i.st)(e,n,!0,s);try{return await Ae.create(o,{executionProviders:Me.p})}catch(a){if(1===Me.p.length&&"wasm"===Me.p[0])throw a;return console.warn(a),console.warn("Something went wrong during model construction (most likely a missing operation). Using `wasm` as a fallback. "),await Ae.create(o,{executionProviders:["wasm"]})}}async function Oe(e,t){const s=function(e,t){const s=Object.create(null),n=[];for(const a of e.inputNames){const e=t[a];e instanceof r.es?s[a]=Se.wasm.proxy?e.clone():e:n.push(a)}if(n.length>0)throw new Error('An error occurred during model execution: "Missing the following inputs: '.concat(n.join(", "),"."));const o=Object.keys(t).length,i=e.inputNames.length;if(o>i){let s=Object.keys(t).filter((t=>!e.inputNames.includes(t)));console.warn("WARNING: Too many inputs were provided (".concat(o," > ").concat(i,'). The following inputs will be ignored: "').concat(s.join(", "),'".'))}return s}(e,t);try{let t=await e.run(s);return t=De(t),t}catch(n){throw console.error('An error occurred during model execution: "'.concat(n,'".')),console.error("Inputs given to model:",s),n}}function De(e){for(let t in e)e[t]instanceof ze?e[t]=new r.es(e[t]):"object"===typeof e[t]&&De(e[t]);return e}function qe(e){if(e instanceof r.es)return e;if(0===e.length)throw Error("items must be non-empty");if(Array.isArray(e[0])){if(e.some((t=>t.length!==e[0].length)))throw Error("Unable to create tensor, you should probably activate truncation and/or padding with 'padding=True' and/or 'truncation=True' to have batched tensors with the same length.");return new r.es("int64",BigInt64Array.from(e.flat().map((e=>BigInt(e)))),[e.length,e[0].length])}return new r.es("int64",BigInt64Array.from(e.map((e=>BigInt(e)))),[1,e.length])}function je(e,t){var s,n;let i=null!==(s=e.config.pad_token_id)&&void 0!==s?s:null,a=null!==(n=e.config.eos_token_id)&&void 0!==n?n:null;(0,o.Wy)(a)&&(a=[a]);let l=-1!==t.indexOf(i),c=null===a||!a.includes(i);if(l&&c){let e=BigInt64Array.from(t.data.map((e=>e!=i)));return new r.es("int64",e,t.dims)}return(0,r.r6)(t)}function Ge(e,t,s){if(!e.inputNames.includes("position_ids"))return;const n=new BigInt64Array(t.attention_mask.data.length);for(let o=0;o<t.attention_mask.dims[0];++o){let e=o*t.attention_mask.dims[1],s=BigInt(0);for(let o=0;o<t.attention_mask.dims[1];++o){const i=e+o;0n===t.attention_mask.data[i]?n[i]=BigInt(1):(n[i]=s,s+=t.attention_mask.data[i])}}t.position_ids=new r.es("int64",n,t.attention_mask.dims),s&&(t.position_ids=t.position_ids.slice(null,-1).unsqueeze_(-1))}function Re(e){return new r.es("bool",[e],[1])}async function Ze(e,t){let{encoder_outputs:s,past_key_values:n}=t;s||(s=(await Ve(e,t)).last_hidden_state);let o={input_ids:t.decoder_input_ids,encoder_hidden_states:s};const i=!!n;e.decoder_merged_session.inputNames.includes("use_cache_branch")&&(o.use_cache_branch=Re(i)),e.decoder_merged_session.inputNames.includes("encoder_attention_mask")&&(o.encoder_attention_mask=t.attention_mask),Ge(e.decoder_merged_session,o,i),e.addPastKeyValues(o,n);const a=await Oe(e.decoder_merged_session,o);let r=a.logits;n=e.getPastKeyValues(a,n);const l=e.getAttentions(a);return new ln({logits:r,past_key_values:n,encoder_outputs:s,...l})}function Xe(e,t,s,n){var o,i,a,l;let c=[],d=0;const _=null===(o=e.requires_attention_mask)||void 0===o||o;let u=null!==(i=null!==(a=null!==(l=s.decoder_input_ids)&&void 0!==l?l:s.decoder_start_token_id)&&void 0!==a?a:s.bos_token_id)&&void 0!==i?i:s.eos_token_id;u instanceof r.es?u=u.tolist().flat():Array.isArray(u)||(u=[u]);for(let r of t){r.dims=[1,...r.dims];let t={inputs:r,encoder_outputs:null,prev_model_outputs:null,output_token_ids:u,done:!1,score:0,id:d++};_&&(t.attention_mask=je(e,r)),c.push(t)}return c}async function Ue(e,t){var s;const n=e.main_input_name;let o=t.output_token_ids;t.prev_model_outputs&&(o=o.slice(-1));let i={[n]:t.inputs,decoder_input_ids:qe(o),encoder_outputs:t.encoder_outputs,past_key_values:null===(s=t.prev_model_outputs)||void 0===s?void 0:s.past_key_values};t.attention_mask&&(i.attention_mask=t.attention_mask);let a=await e.forward(i);return t.prev_model_outputs=a,t.encoder_outputs=a.encoder_outputs,a}function We(e,t){e.output_token_ids=[...e.output_token_ids,t]}async function Ve(e,t){const s=Object.create(null);for(const n of e.session.inputNames)s[n]=t[n];return e.session.inputNames.includes("token_type_ids")&&!s.token_type_ids&&_e(s),await Oe(e.session,s)}async function Ke(e,t){let{input_ids:s,past_key_values:n,attention_mask:o}=t,i={input_ids:s,attention_mask:null!==o&&void 0!==o?o:je(e,s)};const a=!!n;e.session.inputNames.includes("use_cache_branch")&&(i.use_cache_branch=Re(a)),Ge(e.session,i,a),e.addPastKeyValues(i,n);let r=await Oe(e.session,i),l=r.logits;return n=e.getPastKeyValues(r,n),{logits:l,past_key_values:n}}function Qe(e,t,s,n,o){let i=[],a=0;for(let r of t){let t,s=r.tolist().map(Number);r.dims=[1,...r.dims],o?(t=o[a],t.dims=[1,...t.dims]):t=je(e,r);let l={input:r,model_input_ids:r,attention_mask:t,prev_model_outputs:null,output_token_ids:s,num_output_tokens:n,done:!1,score:0,id:a++};i.push(l)}return i}async function Ye(e,t){var s;let n=new BigInt64Array(t.output_token_ids.length).fill(1n),o={input_ids:t.model_input_ids,attention_mask:new r.es("int64",n,[1,n.length]),past_key_values:null===(s=t.prev_model_outputs)||void 0===s?void 0:s.past_key_values},i=await e.forward(o);return t.prev_model_outputs=i,i}function He(e,t){e.output_token_ids=[...e.output_token_ids,t],e.model_input_ids=new r.es("int64",[BigInt(t)],[1,1])}class Je extends o.Ag{constructor(e,t){super(),(0,n.Z)(this,"main_input_name","input_ids"),this.config=e,this.session=t;const s=Ne.get(this.constructor),o=Le.get(s);this.can_generate=!1,this._runBeam=null,this._getStartBeams=null,this._updateBeam=null,this._forward=null,o===Ee?(this.can_generate=!0,this._runBeam=Ye,this._getStartBeams=Qe,this._updateBeam=He,this._forward=Ke):o===Fe||o===Pe?(this.can_generate=!0,this._runBeam=Ue,this._getStartBeams=Xe,this._updateBeam=We,this._forward=Ze):this._forward=Ve}async dispose(){const e=[];for(let t of Object.keys(this)){const s=this[t];s instanceof Ae&&e.push(s.handler.dispose())}return await Promise.all(e)}static async from_pretrained(e){let{quantized:t=!0,progress_callback:s=null,config:n=null,cache_dir:o=null,local_files_only:a=!1,revision:r="main",model_file_name:l=null}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},c={quantized:t,progress_callback:s,config:n,cache_dir:o,local_files_only:a,revision:r,model_file_name:l};const d=Ne.get(this),_=Le.get(d);let u;var h;if(_===Ee)u=await Promise.all([ve.z.from_pretrained(e,c),Ie(e,null!==(h=c.model_file_name)&&void 0!==h?h:"decoder_model_merged",c),(0,i.yM)(e,"generation_config.json",!1,c)]);else if(_===Fe||_===Pe)u=await Promise.all([ve.z.from_pretrained(e,c),Ie(e,"encoder_model",c),Ie(e,"decoder_model_merged",c),(0,i.yM)(e,"generation_config.json",!1,c)]);else if(_===Ce)u=await Promise.all([ve.z.from_pretrained(e,c),Ie(e,"encoder_model",c),Ie(e,"decoder_model_merged",c)]);else{var p;_!==Te&&console.warn("Model type for '".concat(d,"' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose.")),u=await Promise.all([ve.z.from_pretrained(e,c),Ie(e,null!==(p=c.model_file_name)&&void 0!==p?p:"model",c)])}return new this(...u)}async _call(e){return await this.forward(e)}async forward(e){return await this._forward(this,e)}_get_logits_processor(e,t){let s=arguments.length>2&&void 0!==arguments[2]?arguments[2]:null;const n=new be.Jm;if(null!==e.repetition_penalty&&1!==e.repetition_penalty&&n.push(new be.Jj(e.repetition_penalty)),null!==e.no_repeat_ngram_size&&e.no_repeat_ngram_size>0&&n.push(new be.jF(e.no_repeat_ngram_size)),null!==e.bad_words_ids&&n.push(new be.AE(e.bad_words_ids,e.eos_token_id)),null!==e.min_length&&null!==e.eos_token_id&&e.min_length>0&&n.push(new be.ez(e.min_length,e.eos_token_id)),null!==e.min_new_tokens&&null!==e.eos_token_id&&e.min_new_tokens>0&&n.push(new be.CJ(t,e.min_new_tokens,e.eos_token_id)),null!==e.forced_bos_token_id&&n.push(new be.C9(e.forced_bos_token_id)),null!==e.forced_eos_token_id&&n.push(new be.dZ(e.max_length,e.forced_eos_token_id)),null!==e.begin_suppress_tokens){let s=t>1||null===e.forced_bos_token_id?t:t+1;null!==e.forced_decoder_ids&&(s+=e.forced_decoder_ids[e.forced_decoder_ids.length-1][0]),n.push(new be.GU(e.begin_suppress_tokens,s))}return null!==e.forced_decoder_ids&&n.push(new be.E(e.forced_decoder_ids)),null!==s&&n.extend(s),n}_get_generation_config(e){let t=new be.aP(this.config);return"generation_config"in this&&Object.assign(t,this.generation_config),null!==e&&Object.assign(t,e),t}async generate(e){var t,s,n;let i,a=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null,l=arguments.length>2&&void 0!==arguments[2]?arguments[2]:null,{inputs_attention_mask:c=null}=arguments.length>3&&void 0!==arguments[3]?arguments[3]:{};if(!this.can_generate){var d,_,u;const e=Ne.get(this.constructor);let t="The current model class (".concat(e,") is not compatible with `.generate()`, as it doesn't have a language model head.");const s=this.config.model_type,n=null!==(d=null!==(_=null!==(u=As.get(s))&&void 0!==u?u:Ms.get(s))&&void 0!==_?_:xs.get(s))&&void 0!==d?d:Ts.get(s);throw n&&(t+=" Please use the following class instead: '".concat(n[0],"'")),Error(t)}if(!(e instanceof r.es)&&!(0,o.fU)(e)&&!Array.isArray(e))throw Error('`inputs` must be a Tensor, TypedArray, or Array, but is "'.concat(e.constructor.name,'".'));if(this.config.is_encoder_decoder)i=0;else if(i=e instanceof r.es?e.dims.at(-1):e.length,0===i)throw Error("Must supply a non-empty array of input token ids.");a=this._get_generation_config(a),l=null!==(t=l)&&void 0!==t?t:new be.Jm,l=this._get_logits_processor(a,i,l);let h=a.eos_token_id;null===h||Array.isArray(h)||(h=[h]);let p=1;const m=p+(null!==(s=a.max_new_tokens)&&void 0!==s?s:1/0),g=Number.isInteger(a.max_length)&&null===(null!==(n=a.max_new_tokens)&&void 0!==n?n:null);let f=be.Z4.getSampler(a),k=this.getStartBeams(e,a,p,c);for(;k.some((e=>!e.done))&&p<m;){let e=[];for(let t of k){if(t.done){e.push(t);continue}if(g&&t.output_token_ids.length>=a.max_length){t.done=!0,e.push(t);continue}let s=await this.runBeam(t);a.output_attentions&&this.addAttentionsToBeam(t,s),a.output_scores;let n=s.logits.slice(null,-1,null);l(t.output_token_ids,n);let o=f(n);for(let[i,a]of o){let s={...t};this.updateBeam(s,i),s.score+=a,h&&h.includes(i)&&(s.done=!0),e.push(s)}}++p,e=this.groupBeams(e).map((e=>e.sort(((e,t)=>t.score-e.score)).slice(0,a.num_beams))),k=e.flat(),a.callback_function&&a.callback_function(k)}const w=this.groupBeams(k),x=e=>w.map((t=>a.num_return_sequences>1?t.slice(0,a.num_return_sequences).map((t=>t[e])):[t[0][e]])).flat(),y=x("output_token_ids");if(a.return_dict_in_generate){return{sequences:y,decoder_attentions:x("decoder_attentions"),cross_attentions:x("cross_attentions")}}return y}addAttentionsToBeam(e,t){if(this.config.is_encoder_decoder){if(!t.cross_attentions||0===t.cross_attentions.length)throw Error("`output_attentions` is true, but the model did not produce cross-attentions. This is most likely because the model was not exported with `output_attentions=True`.");e.cross_attentions||(e.cross_attentions=[]),e.cross_attentions.push(t.cross_attentions)}if(!t.decoder_attentions||0===t.decoder_attentions.length)throw Error("`output_attentions` is true, but the model did not produce decoder-attentions. This is most likely because the model was not exported with `output_attentions=True`.");e.decoder_attentions||(e.decoder_attentions=[]),e.decoder_attentions.push(t.decoder_attentions)}groupBeams(e){const t=Object.create(null);for(const s of e)void 0===t[s.id]?t[s.id]=[s]:t[s.id].push(s);return Object.values(t)}getPastKeyValues(e,t){const s=Object.create(null);for(const n in e)if(n.startsWith("present")){let o=n.replace("present","past_key_values");t&&n.includes("encoder")?s[o]=t[o]:s[o]=e[n]}return s}getAttentions(e){const t=Object.create(null);for(const s of["cross_attentions","decoder_attentions"]){const n=[];for(const t in e)if(t.startsWith(s)){n[t.split(".").pop()]=e[t]}t[s]=n}return t}addPastKeyValues(e,t){if(t)Object.assign(e,t);else{var s;const t=1;if(this.config.is_encoder_decoder&&(null===(s=this.add_encoder_pkv)||void 0===s||s)){let s=[t,this.num_encoder_heads,0,this.encoder_dim_kv],n=[t,this.num_decoder_heads,0,this.decoder_dim_kv];for(let t=0;t<this.num_decoder_layers;++t)e["past_key_values.".concat(t,".encoder.key")]=new r.es("float32",[],s),e["past_key_values.".concat(t,".encoder.value")]=new r.es("float32",[],s),e["past_key_values.".concat(t,".decoder.key")]=new r.es("float32",[],n),e["past_key_values.".concat(t,".decoder.value")]=new r.es("float32",[],n)}else if("falcon"===this.config.model_type){let s=[t*this.num_heads,0,this.dim_kv];for(let t=0;t<this.num_layers;++t)e["past_key_values.".concat(t,".key")]=new r.es("float32",[],s),e["past_key_values.".concat(t,".value")]=new r.es("float32",[],s)}else if(this.config.multi_query){let s=[t*this.num_heads,0,2*this.dim_kv];for(let t=0;t<this.num_layers;++t)e["past_key_values.".concat(t,".key_value")]=new r.es("float32",[],s)}else if("bloom"===this.config.model_type){let s=[t*this.num_heads,this.dim_kv,0],n=[t*this.num_heads,0,this.dim_kv];for(let t=0;t<this.num_layers;++t)e["past_key_values.".concat(t,".key")]=new r.es("float32",[],s),e["past_key_values.".concat(t,".value")]=new r.es("float32",[],n)}else{let s=[t,this.num_heads,0,this.dim_kv];for(let t=0;t<this.num_layers;++t)e["past_key_values.".concat(t,".key")]=new r.es("float32",[],s),e["past_key_values.".concat(t,".value")]=new r.es("float32",[],s)}}}getStartBeams(e,t,s,n){return this._getStartBeams(this,e,t,s,n)}async runBeam(e){return await this._runBeam(this,e)}updateBeam(e,t){return this._updateBeam(e,t)}}class $e{}class et extends Je{}class tt extends Je{}class st extends Je{}class nt extends Je{}class ot extends Je{}class it extends Je{}class at extends Je{}class rt extends Je{}class lt extends Je{}class ct extends Je{}class dt extends Je{}class _t extends Je{}class ut extends Je{}class ht extends Je{}class pt extends Je{}class mt extends Je{}class gt extends Je{}class ft extends Je{}class kt extends Je{}class wt extends Je{}class xt extends Je{}class yt extends Je{}class vt extends Je{}class bt extends Je{}class Mt extends Je{constructor(e,t,s,o){var i;super(e,t),(0,n.Z)(this,"main_input_name","pixel_values"),this.decoder_merged_session=s,this.generation_config=o;const a=this.config.encoder,r=this.config.decoder,l=a.model_type;(null!==(i=fs.get(l))&&void 0!==i?i:ks.get(l))||console.warn("Model type for encoder '".concat(l,"' not found, assuming encoder-only architecture. Please report this at https://github.com/xenova/transformers.js/issues/new/choose."));const c=As.get(r.model_type);if(!c)throw new Error('Unable to construct `VisionEncoderDecoder` due to unsupported decoder: "'.concat(this.config.decoder.model_type,'"'));const d=new(0,c[1])(r,s,o);this.add_encoder_pkv="num_decoder_layers"in d,this.add_encoder_pkv?(this.num_decoder_layers=d.num_decoder_layers,this.num_decoder_heads=d.num_decoder_heads,this.decoder_dim_kv=d.decoder_dim_kv,this.num_encoder_layers=d.num_encoder_layers,this.num_encoder_heads=d.num_encoder_heads,this.encoder_dim_kv=d.encoder_dim_kv):(this.num_layers=d.num_layers,this.num_heads=d.num_heads,this.dim_kv=d.dim_kv)}}class At extends Je{}class zt extends Je{}class St extends Je{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.n_embd/this.num_heads}}class Tt extends Je{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_heads,this.num_layers=this.config.num_layers,this.dim_kv=this.config.hidden_size/this.num_heads}}class Ct extends Je{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_attention_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.num_heads}}class Ft extends Je{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.n_embd/this.num_heads}}class Pt extends Je{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.n_embd/this.num_heads}}class Et extends Je{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.n_embd/this.num_heads}}class Lt extends Je{constructor(e,t,s){var n;super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=null!==(n=this.config.num_key_value_heads)&&void 0!==n?n:this.config.num_attention_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.config.num_attention_heads}}class Bt extends Je{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_attention_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.num_heads}}class Nt extends Je{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_head,this.num_layers=this.config.n_layer,this.dim_kv=this.config.hidden_size/this.num_heads}}class It extends Je{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.n_heads,this.num_layers=this.config.n_layers,this.dim_kv=this.config.d_model/this.num_heads}}class Ot extends Je{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_attention_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.num_heads}}class Dt extends Je{}class qt extends Je{}class jt extends Je{}class Gt extends Je{}class Rt extends Je{}class Zt extends Je{}class Xt extends $e{constructor(e){let{logits:t,pred_boxes:s}=e;super(),this.logits=t,this.pred_boxes=s}}class Ut extends $e{constructor(e){let{logits:t,pred_boxes:s,pred_masks:n}=e;super(),this.logits=t,this.pred_boxes=s,this.pred_masks=n}}class Wt extends Je{}class Vt extends Je{}class Kt extends Je{}class Qt extends Je{}class Yt extends Je{}class Ht extends Je{}class Jt extends Je{}class $t extends Je{}class es extends Je{}class ts extends Je{}class ss extends Je{}class ns extends $e{constructor(e){let{logits:t,pred_boxes:s}=e;super(),this.logits=t,this.pred_boxes=s}}class os extends Je{}class is extends os{async _call(e){return new as(await super._call(e))}}class as extends $e{constructor(e){let{iou_scores:t,pred_masks:s}=e;super(),this.iou_scores=t,this.pred_masks=s}}class rs extends Je{}class ls extends Je{}class cs extends Je{}class ds extends Je{}class _s extends Je{}class us extends Je{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_encoder_layers=this.num_decoder_layers=this.config.decoder_layers,this.num_encoder_heads=this.num_decoder_heads=this.config.decoder_attention_heads,this.encoder_dim_kv=this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads}}class hs extends Je{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_key_value_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.config.num_attention_heads}}class ps extends Je{constructor(e,t,s){super(e,t),this.generation_config=s,this.config.pad_token_id=this.config.eos_token_id,this.num_heads=this.config.num_attention_heads,this.num_layers=this.config.num_hidden_layers,this.dim_kv=this.config.hidden_size/this.config.num_attention_heads}}class ms extends Je{}class gs{static async from_pretrained(e){let{quantized:t=!0,progress_callback:s=null,config:n=null,cache_dir:o=null,local_files_only:i=!1,revision:a="main",model_file_name:r=null}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},l={quantized:t,progress_callback:s,config:n,cache_dir:o,local_files_only:i,revision:a,model_file_name:r};if(n=await ve.z.from_pretrained(e,l),l.config||(l.config=n),!this.MODEL_CLASS_MAPPINGS)throw new Error("`MODEL_CLASS_MAPPINGS` not implemented for this type of `AutoClass`: "+this.name);for(let c of this.MODEL_CLASS_MAPPINGS){const t=c.get(n.model_type);if(t)return await t[1].from_pretrained(e,l)}if(this.BASE_IF_FAIL)return console.warn('Unknown model class "'.concat(n.model_type,'", attempting to construct from base class.')),await Je.from_pretrained(e,l);throw Error("Unsupported model type: ".concat(n.model_type))}}(0,n.Z)(gs,"MODEL_CLASS_MAPPINGS",null),(0,n.Z)(gs,"BASE_IF_FAIL",!1);const fs=new Map([["bert",["BertModel",class extends et{}]],["electra",["ElectraModel",class extends st{}]],["esm",["EsmModel",class extends rt{}]],["convbert",["ConvBertModel",class extends tt{}]],["camembert",["CamembertModel",class extends nt{}]],["deberta",["DebertaModel",class extends ot{}]],["deberta-v2",["DebertaV2Model",class extends it{}]],["mpnet",["MPNetModel",class extends ct{}]],["albert",["AlbertModel",class extends _t{}]],["distilbert",["DistilBertModel",class extends at{}]],["roberta",["RobertaModel",class extends wt{}]],["xlm",["XLMModel",class extends xt{}]],["xlm-roberta",["XLMRobertaModel",class extends yt{}]],["clap",["ClapModel",class extends ms{}]],["clip",["CLIPModel",class extends At{}]],["chinese_clip",["ChineseCLIPModel",class extends zt{}]],["mobilebert",["MobileBertModel",class extends lt{}]],["squeezebert",["SqueezeBertModel",class extends dt{}]],["wav2vec2",["Wav2Vec2Model",class extends cs{}]],["hubert",["HubertModel",class extends cs{}]],["wavlm",["WavLMModel",class extends ds{}]],["audio-spectrogram-transformer",["ASTModel",class extends vt{}]],["detr",["DetrModel",class extends Zt{}]],["vit",["ViTModel",class extends Dt{}]],["mobilevit",["MobileViTModel",class extends jt{}]],["owlvit",["OwlViTModel",class extends Gt{}]],["beit",["BeitModel",class extends Rt{}]],["deit",["DeiTModel",class extends Wt{}]],["convnext",["ConvNextModel",class extends $t{}]],["convnextv2",["ConvNextV2Model",class extends es{}]],["dinov2",["Dinov2Model",class extends ts{}]],["resnet",["ResNetModel",class extends Vt{}]],["swin",["SwinModel",class extends Kt{}]],["swin2sr",["Swin2SRModel",class extends Qt{}]],["donut-swin",["DonutSwinModel",class extends Jt{}]],["yolos",["YolosModel",class extends ss{}]],["dpt",["DPTModel",class extends Yt{}]],["glpn",["GLPNModel",class extends Ht{}]],["hifigan",["SpeechT5HifiGan",class extends Je{constructor(){super(...arguments),(0,n.Z)(this,"main_input_name","spectrogram")}}]],["sam",["SamModel",is]]]),ks=new Map([["t5",["T5Model",class extends ut{}]],["longt5",["LongT5Model",class extends ht{}]],["mt5",["MT5Model",class extends pt{}]],["bart",["BartModel",class extends mt{}]],["mbart",["MBartModel",class extends gt{}]],["marian",["MarianModel",class extends rs{}]],["whisper",["WhisperModel",class extends bt{}]],["m2m_100",["M2M100Model",class extends ls{}]],["blenderbot",["BlenderbotModel",class extends ft{}]],["blenderbot-small",["BlenderbotSmallModel",class extends kt{}]]]),ws=new Map([["bloom",["BloomModel",class extends Nt{}]],["gpt2",["GPT2Model",class extends St{}]],["gptj",["GPTJModel",class extends Ft{}]],["gpt_bigcode",["GPTBigCodeModel",class extends Pt{}]],["gpt_neo",["GPTNeoModel",class extends Tt{}]],["gpt_neox",["GPTNeoXModel",class extends Ct{}]],["codegen",["CodeGenModel",class extends Et{}]],["llama",["LlamaModel",class extends Lt{}]],["phi",["PhiModel",class extends Bt{}]],["mpt",["MptModel",class extends It{}]],["opt",["OPTModel",class extends Ot{}]],["mistral",["MistralModel",class extends hs{}]],["falcon",["FalconModel",class extends ps{}]]]),xs=new Map([["speecht5",["SpeechT5ForSpeechToText",class extends _s{}]],["whisper",["WhisperForConditionalGeneration",class extends bt{constructor(e,t,s,o){super(e,t),(0,n.Z)(this,"requires_attention_mask",!1),(0,n.Z)(this,"main_input_name","input_features"),this.decoder_merged_session=s,this.generation_config=o,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}async generate(e){var t,s;let n=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null,o=arguments.length>2&&void 0!==arguments[2]?arguments[2]:null;if(n=this._get_generation_config(n),null!==(s=(t=n).return_timestamps)&&void 0!==s||(t.return_timestamps=!1),n.return_timestamps&&(o=[new be.Pg(n)]),n.return_token_timestamps&&(n.output_attentions=!0,n.return_dict_in_generate=!0,"translate"===n.task&&console.warn("Token-level timestamps may not be reliable for task 'translate'."),!n.alignment_heads))throw new Error("Model generation config has no `alignment_heads`, token-level timestamps not available. See https://gist.github.com/hollance/42e32852f24243b748ae6bc1f985b13a on how to add this property to the generation config.");const i=await super.generate(e,n,o);return n.return_token_timestamps&&n.alignment_heads&&(i.token_timestamps=this._extract_token_timestamps(i,n.alignment_heads,n.num_frames)),i}_extract_token_timestamps(e,t){let s=arguments.length>2&&void 0!==arguments[2]?arguments[2]:null,n=arguments.length>3&&void 0!==arguments[3]?arguments[3]:.02;if(!e.cross_attentions)throw new Error("Model outputs must contain cross attentions to extract timestamps. This is most likely because the model was not exported with `output_attentions=True`.");let i=this.config.median_filter_width;void 0===i&&(console.warn("Model config has no `median_filter_width`, using default value of 7."),i=7);const l=e.cross_attentions.map((e=>{let n=Array.from({length:this.config.decoder_layers},((t,s)=>(0,r.d3)(e.map((e=>e[s])),2))),o=(0,r.kn)(t.map((e=>{let[t,o]=e;return s?n[t].slice(null,o,null,[0,s]):n[t].slice(null,o)})));o=o.transpose(1,0,2,3);let[l,c]=(0,r.f3)(o,-2,0,!0),d=o.clone();for(let t=0;t<d.dims[0];++t){let e=d[t];for(let s=0;s<e.dims[0];++s){let n=e[s];const o=l[t][s][0],r=c[t][s][0];for(let e=0;e<n.dims[0];++e){let t=n[e];for(let e=0;e<t.data.length;++e)t.data[e]=(t.data[e]-r.data[e])/o.data[e];t.data.set((0,a.qC)(t.data,i))}}}return(0,r.J6)(d,1)})),c=[e.sequences.length,e.sequences[0].length],d=new r.es("float32",new Float32Array(c[0]*c[1]),c);for(let a=0;a<c[0];++a){const e=l[a].neg().squeeze_(0);let[t,s]=(0,r.Ks)(e),i=Array.from({length:t.length-1},((e,s)=>t[s+1]-t[s])),c=(0,o.eG)([1],i).map((e=>!!e)),_=[];for(let o=0;o<c.length;++o)c[o]&&_.push(s[o]*n);d[a].data.set(_,1)}return d}}]]]),ys=new Map([["speecht5",["SpeechT5ForTextToSpeech",class extends _s{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.hidden_size/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.hidden_size/this.num_encoder_heads}async generate_speech(e,t){let{threshold:s=.5,minlenratio:n=0,maxlenratio:o=20,vocoder:i=null}=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};const a={input_ids:e},{encoder_outputs:l,encoder_attention_mask:c}=await Ve(this,a),d=l.dims[1]/this.config.reduction_factor,_=Math.floor(d*o),u=Math.floor(d*n),h=this.config.num_mel_bins;let p=[],m=null,g=null,f=0;for(;;){++f;const e=Re(!!g);let n;n=g?g.output_sequence_out:new r.es("float32",new Float32Array(h),[1,1,h]);let o={use_cache_branch:e,output_sequence:n,encoder_attention_mask:c,speaker_embeddings:t,encoder_hidden_states:l};this.addPastKeyValues(o,m),g=await Oe(this.decoder_merged_session,o),m=this.getPastKeyValues(g,m);const{prob:i,spectrum:a}=g;if(p.push(a),f>=u&&(Array.from(i.data).filter((e=>e>=s)).length>0||f>=_))break}const k=(0,r.d3)(p),{waveform:w}=await Oe(i.session,{spectrogram:k});return{spectrogram:k,waveform:w}}}]]]),vs=new Map([["bert",["BertForSequenceClassification",class extends et{async _call(e){return new cn(await super._call(e))}}]],["electra",["ElectraForSequenceClassification",class extends st{async _call(e){return new cn(await super._call(e))}}]],["esm",["EsmForSequenceClassification",class extends rt{async _call(e){return new cn(await super._call(e))}}]],["convbert",["ConvBertForSequenceClassification",class extends tt{async _call(e){return new cn(await super._call(e))}}]],["camembert",["CamembertForSequenceClassification",class extends nt{async _call(e){return new cn(await super._call(e))}}]],["deberta",["DebertaForSequenceClassification",class extends ot{async _call(e){return new cn(await super._call(e))}}]],["deberta-v2",["DebertaV2ForSequenceClassification",class extends it{async _call(e){return new cn(await super._call(e))}}]],["mpnet",["MPNetForSequenceClassification",class extends ct{async _call(e){return new cn(await super._call(e))}}]],["albert",["AlbertForSequenceClassification",class extends _t{async _call(e){return new cn(await super._call(e))}}]],["distilbert",["DistilBertForSequenceClassification",class extends at{async _call(e){return new cn(await super._call(e))}}]],["roberta",["RobertaForSequenceClassification",class extends wt{async _call(e){return new cn(await super._call(e))}}]],["xlm",["XLMForSequenceClassification",class extends xt{async _call(e){return new cn(await super._call(e))}}]],["xlm-roberta",["XLMRobertaForSequenceClassification",class extends yt{async _call(e){return new cn(await super._call(e))}}]],["bart",["BartForSequenceClassification",class extends mt{async _call(e){return new cn(await super._call(e))}}]],["mbart",["MBartForSequenceClassification",class extends gt{async _call(e){return new cn(await super._call(e))}}]],["mobilebert",["MobileBertForSequenceClassification",class extends lt{async _call(e){return new cn(await super._call(e))}}]],["squeezebert",["SqueezeBertForSequenceClassification",class extends dt{async _call(e){return new cn(await super._call(e))}}]]]),bs=new Map([["bert",["BertForTokenClassification",class extends et{async _call(e){return new dn(await super._call(e))}}]],["electra",["ElectraForTokenClassification",class extends st{async _call(e){return new dn(await super._call(e))}}]],["esm",["EsmForTokenClassification",class extends rt{async _call(e){return new dn(await super._call(e))}}]],["convbert",["ConvBertForTokenClassification",class extends tt{async _call(e){return new dn(await super._call(e))}}]],["camembert",["CamembertForTokenClassification",class extends nt{async _call(e){return new dn(await super._call(e))}}]],["deberta",["DebertaForTokenClassification",class extends ot{async _call(e){return new dn(await super._call(e))}}]],["deberta-v2",["DebertaV2ForTokenClassification",class extends it{async _call(e){return new dn(await super._call(e))}}]],["mpnet",["MPNetForTokenClassification",class extends ct{async _call(e){return new dn(await super._call(e))}}]],["distilbert",["DistilBertForTokenClassification",class extends at{async _call(e){return new dn(await super._call(e))}}]],["roberta",["RobertaForTokenClassification",class extends wt{async _call(e){return new dn(await super._call(e))}}]],["xlm",["XLMForTokenClassification",class extends xt{async _call(e){return new dn(await super._call(e))}}]],["xlm-roberta",["XLMRobertaForTokenClassification",class extends yt{async _call(e){return new dn(await super._call(e))}}]]]),Ms=new Map([["t5",["T5ForConditionalGeneration",class extends ut{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.num_decoder_layers,this.num_decoder_heads=this.config.num_heads,this.decoder_dim_kv=this.config.d_kv,this.num_encoder_layers=this.config.num_layers,this.num_encoder_heads=this.config.num_heads,this.encoder_dim_kv=this.config.d_kv}}]],["longt5",["LongT5ForConditionalGeneration",class extends ht{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.num_decoder_layers,this.num_decoder_heads=this.config.num_heads,this.decoder_dim_kv=this.config.d_kv,this.num_encoder_layers=this.config.num_layers,this.num_encoder_heads=this.config.num_heads,this.encoder_dim_kv=this.config.d_kv}}]],["mt5",["MT5ForConditionalGeneration",class extends pt{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.num_decoder_layers,this.num_decoder_heads=this.config.num_heads,this.decoder_dim_kv=this.config.d_kv,this.num_encoder_layers=this.config.num_layers,this.num_encoder_heads=this.config.num_heads,this.encoder_dim_kv=this.config.d_kv}}]],["bart",["BartForConditionalGeneration",class extends mt{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["mbart",["MBartForConditionalGeneration",class extends gt{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["marian",["MarianMTModel",class extends rs{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["m2m_100",["M2M100ForConditionalGeneration",class extends ls{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["blenderbot",["BlenderbotForConditionalGeneration",class extends ft{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["blenderbot-small",["BlenderbotSmallForConditionalGeneration",class extends kt{constructor(e,t,s,n){super(e,t),this.decoder_merged_session=s,this.generation_config=n,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]]]),As=new Map([["bloom",["BloomForCausalLM",class extends Nt{}]],["gpt2",["GPT2LMHeadModel",class extends St{}]],["gptj",["GPTJForCausalLM",class extends Ft{}]],["gpt_bigcode",["GPTBigCodeForCausalLM",class extends Pt{}]],["gpt_neo",["GPTNeoForCausalLM",class extends Tt{}]],["gpt_neox",["GPTNeoXForCausalLM",class extends Ct{}]],["codegen",["CodeGenForCausalLM",class extends Et{}]],["llama",["LlamaForCausalLM",class extends Lt{}]],["phi",["PhiForCausalLM",class extends Bt{}]],["mpt",["MptForCausalLM",class extends It{}]],["opt",["OPTForCausalLM",class extends Ot{}]],["mbart",["MBartForCausalLM",class extends gt{constructor(e,t,s){super(e,t),this.generation_config=s,this.num_decoder_layers=this.config.decoder_layers,this.num_decoder_heads=this.config.decoder_attention_heads,this.decoder_dim_kv=this.config.d_model/this.num_decoder_heads,this.num_encoder_layers=this.config.encoder_layers,this.num_encoder_heads=this.config.encoder_attention_heads,this.encoder_dim_kv=this.config.d_model/this.num_encoder_heads}}]],["mistral",["MistralForCausalLM",class extends hs{}]],["falcon",["FalconForCausalLM",class extends ps{}]],["trocr",["TrOCRForCausalLM",class extends us{}]]]),zs=new Map([["bert",["BertForMaskedLM",class extends et{async _call(e){return new _n(await super._call(e))}}]],["electra",["ElectraForMaskedLM",class extends st{async _call(e){return new _n(await super._call(e))}}]],["esm",["EsmForMaskedLM",class extends rt{async _call(e){return new _n(await super._call(e))}}]],["convbert",["ConvBertForMaskedLM",class extends tt{async _call(e){return new _n(await super._call(e))}}]],["camembert",["CamembertForMaskedLM",class extends nt{async _call(e){return new _n(await super._call(e))}}]],["deberta",["DebertaForMaskedLM",class extends ot{async _call(e){return new _n(await super._call(e))}}]],["deberta-v2",["DebertaV2ForMaskedLM",class extends it{async _call(e){return new _n(await super._call(e))}}]],["mpnet",["MPNetForMaskedLM",class extends ct{async _call(e){return new _n(await super._call(e))}}]],["albert",["AlbertForMaskedLM",class extends _t{async _call(e){return new _n(await super._call(e))}}]],["distilbert",["DistilBertForMaskedLM",class extends at{async _call(e){return new _n(await super._call(e))}}]],["roberta",["RobertaForMaskedLM",class extends wt{async _call(e){return new _n(await super._call(e))}}]],["xlm",["XLMWithLMHeadModel",class extends xt{async _call(e){return new _n(await super._call(e))}}]],["xlm-roberta",["XLMRobertaForMaskedLM",class extends yt{async _call(e){return new _n(await super._call(e))}}]],["mobilebert",["MobileBertForMaskedLM",class extends lt{async _call(e){return new _n(await super._call(e))}}]],["squeezebert",["SqueezeBertForMaskedLM",class extends dt{async _call(e){return new _n(await super._call(e))}}]]]),Ss=new Map([["bert",["BertForQuestionAnswering",class extends et{async _call(e){return new un(await super._call(e))}}]],["electra",["ElectraForQuestionAnswering",class extends st{async _call(e){return new un(await super._call(e))}}]],["convbert",["ConvBertForQuestionAnswering",class extends tt{async _call(e){return new un(await super._call(e))}}]],["camembert",["CamembertForQuestionAnswering",class extends nt{async _call(e){return new un(await super._call(e))}}]],["deberta",["DebertaForQuestionAnswering",class extends ot{async _call(e){return new un(await super._call(e))}}]],["deberta-v2",["DebertaV2ForQuestionAnswering",class extends it{async _call(e){return new un(await super._call(e))}}]],["mpnet",["MPNetForQuestionAnswering",class extends ct{async _call(e){return new un(await super._call(e))}}]],["albert",["AlbertForQuestionAnswering",class extends _t{async _call(e){return new un(await super._call(e))}}]],["distilbert",["DistilBertForQuestionAnswering",class extends at{async _call(e){return new un(await super._call(e))}}]],["roberta",["RobertaForQuestionAnswering",class extends wt{async _call(e){return new un(await super._call(e))}}]],["xlm",["XLMForQuestionAnswering",class extends xt{async _call(e){return new un(await super._call(e))}}]],["xlm-roberta",["XLMRobertaForQuestionAnswering",class extends yt{async _call(e){return new un(await super._call(e))}}]],["mobilebert",["MobileBertForQuestionAnswering",class extends lt{async _call(e){return new un(await super._call(e))}}]],["squeezebert",["SqueezeBertForQuestionAnswering",class extends dt{async _call(e){return new un(await super._call(e))}}]]]),Ts=new Map([["vision-encoder-decoder",["VisionEncoderDecoderModel",Mt]]]),Cs=new Map([["vision-encoder-decoder",["VisionEncoderDecoderModel",Mt]]]),Fs=new Map([["vit",["ViTForImageClassification",class extends Dt{async _call(e){return new cn(await super._call(e))}}]],["mobilevit",["MobileViTForImageClassification",class extends jt{async _call(e){return new cn(await super._call(e))}}]],["beit",["BeitForImageClassification",class extends Rt{async _call(e){return new cn(await super._call(e))}}]],["deit",["DeiTForImageClassification",class extends Wt{async _call(e){return new cn(await super._call(e))}}]],["convnext",["ConvNextForImageClassification",class extends $t{async _call(e){return new cn(await super._call(e))}}]],["convnextv2",["ConvNextV2ForImageClassification",class extends es{async _call(e){return new cn(await super._call(e))}}]],["dinov2",["Dinov2ForImageClassification",class extends ts{async _call(e){return new cn(await super._call(e))}}]],["resnet",["ResNetForImageClassification",class extends Vt{async _call(e){return new cn(await super._call(e))}}]],["swin",["SwinForImageClassification",class extends Kt{async _call(e){return new cn(await super._call(e))}}]]]),Ps=new Map([["detr",["DetrForObjectDetection",class extends Zt{async _call(e){return new Xt(await super._call(e))}}]],["yolos",["YolosForObjectDetection",class extends ss{async _call(e){return new ns(await super._call(e))}}]]]),Es=new Map([["owlvit",["OwlViTForObjectDetection",class extends Gt{}]]]),Ls=new Map([["detr",["DetrForSegmentation",class extends Zt{async _call(e){return new Ut(await super._call(e))}}]]]),Bs=new Map([["sam",["SamModel",is]]]),Ns=new Map([["wav2vec2",["Wav2Vec2ForCTC",class extends cs{async _call(e){return new hn(await super._call(e))}}]],["wavlm",["WavLMForCTC",class extends ds{async _call(e){return new hn(await super._call(e))}}]],["hubert",["HubertForCTC",class extends cs{async _call(e){return new hn(await super._call(e))}}]]]),Is=new Map([["wav2vec2",["Wav2Vec2ForSequenceClassification",class extends cs{async _call(e){return new cn(await super._call(e))}}]],["wavlm",["WavLMForSequenceClassification",class extends ds{async _call(e){return new cn(await super._call(e))}}]],["hubert",["HubertForSequenceClassification",class extends cs{async _call(e){return new cn(await super._call(e))}}]],["audio-spectrogram-transformer",["ASTForAudioClassification",class extends vt{}]]]),Os=new Map([["vitmatte",["VitMatteForImageMatting",class extends qt{async _call(e){return new pn(await super._call(e))}}]]]),Ds=new Map([["swin2sr",["Swin2SRForImageSuperResolution",class extends Qt{}]]]),qs=new Map([["dpt",["DPTForDepthEstimation",class extends Yt{}]],["glpn",["GLPNForDepthEstimation",class extends Ht{}]]]),js=[[fs,Te],[ks,Ce],[ws,Ee],[vs,Te],[bs,Te],[Ms,Fe],[xs,Fe],[As,Ee],[zs,Te],[Ss,Te],[Ts,Pe],[Fs,Te],[Ls,Te],[Os,Te],[Ds,Te],[qs,Te],[Ps,Te],[Es,Te],[Bs,Te],[Ns,Te],[Is,Te],[ys,Fe]];for(const[zn,Sn]of js)for(const[e,t]of zn.values())Le.set(e,Sn),Ne.set(t,e),Be.set(e,t);const Gs=[["CLIPTextModelWithProjection",class extends At{static async from_pretrained(e){var t;let s=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return null!==(t=s.model_file_name)&&void 0!==t||(s.model_file_name="text_model"),super.from_pretrained(e,s)}},Te],["CLIPVisionModelWithProjection",class extends At{static async from_pretrained(e){var t;let s=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return null!==(t=s.model_file_name)&&void 0!==t||(s.model_file_name="vision_model"),super.from_pretrained(e,s)}},Te],["ClapTextModelWithProjection",class extends ms{static async from_pretrained(e){var t;let s=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return null!==(t=s.model_file_name)&&void 0!==t||(s.model_file_name="text_model"),super.from_pretrained(e,s)}},Te],["ClapAudioModelWithProjection",class extends ms{static async from_pretrained(e){var t;let s=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};return null!==(t=s.model_file_name)&&void 0!==t||(s.model_file_name="audio_model"),super.from_pretrained(e,s)}},Te]];for(const[zn,Sn,Tn]of Gs)Le.set(zn,Tn),Ne.set(Sn,zn),Be.set(zn,Sn);class Rs extends gs{}(0,n.Z)(Rs,"MODEL_CLASS_MAPPINGS",[fs,ks,ws]),(0,n.Z)(Rs,"BASE_IF_FAIL",!0);class Zs extends gs{}(0,n.Z)(Zs,"MODEL_CLASS_MAPPINGS",[vs]);class Xs extends gs{}(0,n.Z)(Xs,"MODEL_CLASS_MAPPINGS",[bs]);class Us extends gs{}(0,n.Z)(Us,"MODEL_CLASS_MAPPINGS",[Ms]);class Ws extends gs{}(0,n.Z)(Ws,"MODEL_CLASS_MAPPINGS",[xs]);class Vs extends gs{}(0,n.Z)(Vs,"MODEL_CLASS_MAPPINGS",[ys]);class Ks extends gs{}(0,n.Z)(Ks,"MODEL_CLASS_MAPPINGS",[As]);class Qs extends gs{}(0,n.Z)(Qs,"MODEL_CLASS_MAPPINGS",[zs]);class Ys extends gs{}(0,n.Z)(Ys,"MODEL_CLASS_MAPPINGS",[Ss]);class Hs extends gs{}(0,n.Z)(Hs,"MODEL_CLASS_MAPPINGS",[Ts]);class Js extends gs{}(0,n.Z)(Js,"MODEL_CLASS_MAPPINGS",[Fs]);class $s extends gs{}(0,n.Z)($s,"MODEL_CLASS_MAPPINGS",[Ls]);class en extends gs{}(0,n.Z)(en,"MODEL_CLASS_MAPPINGS",[Ps]);class tn extends gs{}(0,n.Z)(tn,"MODEL_CLASS_MAPPINGS",[Es]);(0,n.Z)(class extends gs{},"MODEL_CLASS_MAPPINGS",[Bs]);class sn extends gs{}(0,n.Z)(sn,"MODEL_CLASS_MAPPINGS",[Ns]);class nn extends gs{}(0,n.Z)(nn,"MODEL_CLASS_MAPPINGS",[Is]);class on extends gs{}(0,n.Z)(on,"MODEL_CLASS_MAPPINGS",[Cs]);(0,n.Z)(class extends gs{},"MODEL_CLASS_MAPPINGS",[Os]);class an extends gs{}(0,n.Z)(an,"MODEL_CLASS_MAPPINGS",[Ds]);class rn extends gs{}(0,n.Z)(rn,"MODEL_CLASS_MAPPINGS",[qs]);class ln extends $e{constructor(e){let{logits:t,past_key_values:s,encoder_outputs:n,decoder_attentions:o=null,cross_attentions:i=null}=e;super(),this.logits=t,this.past_key_values=s,this.encoder_outputs=n,this.decoder_attentions=o,this.cross_attentions=i}}class cn extends $e{constructor(e){let{logits:t}=e;super(),this.logits=t}}class dn extends $e{constructor(e){let{logits:t}=e;super(),this.logits=t}}class _n extends $e{constructor(e){let{logits:t}=e;super(),this.logits=t}}class un extends $e{constructor(e){let{start_logits:t,end_logits:s}=e;super(),this.start_logits=t,this.end_logits=s}}class hn extends $e{constructor(e){let{logits:t}=e;super(),this.logits=t}}class pn extends $e{constructor(e){let{alphas:t}=e;super(),this.alphas=t}}var mn=s(3620),gn=s(6264),fn=s(5456);async function kn(e){return Array.isArray(e)||(e=[e]),e=await Promise.all(e.map((e=>fn.O.read(e))))}class wn extends o.Ag{constructor(e){let{task:t,model:s,tokenizer:n=null,processor:o=null}=e;super(),this.task=t,this.model=s,this.tokenizer=n,this.processor=o}async dispose(){await this.model.dispose()}async _call(e){let t=this.tokenizer(e,{padding:!0,truncation:!0});return[t,await this.model(t)]}}class xn extends wn{constructor(){super(...arguments),(0,n.Z)(this,"_key","generated_text")}async _call(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};Array.isArray(e)||(e=[e]),this.model.config.prefix&&(e=e.map((e=>this.model.config.prefix+e)));let s=this.model.config.task_specific_params;s&&s[this.task]&&s[this.task].prefix&&(e=e.map((e=>s[this.task].prefix+e)));let n,o={padding:!0,truncation:!0};n=this instanceof yn&&"_build_translation_inputs"in this.tokenizer?this.tokenizer._build_translation_inputs(e,o,t).input_ids:this.tokenizer(e,o).input_ids;let i=await this.model.generate(n,t),a=this.tokenizer.batch_decode(i,{skip_special_tokens:!0});return null!==this._key&&(a=a.map((e=>null===this._key?e:{[this._key]:e}))),a}}class yn extends xn{constructor(){super(...arguments),(0,n.Z)(this,"_key","translation_text")}}const vn={"text-classification":{tokenizer:ye,pipeline:class extends wn{async _call(e){let{topk:t=1}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},s="multi_label_classification"===this.model.config.problem_type?e=>e.sigmoid().data:e=>(0,a.XA)(e.data),[n,o]=await super._call(e),i=this.model.config.id2label,r=[];for(let l of o.logits){let e=s(l),n=(0,a.em)(e,t).map((function(e){return{label:i[e[0]],score:e[1]}}));1===t?r.push(...n):r.push(n)}return Array.isArray(e)||1===t?r:r[0]}},model:Zs,default:{model:"Xenova/distilbert-base-uncased-finetuned-sst-2-english"},type:"text"},"token-classification":{tokenizer:ye,pipeline:class extends wn{async _call(e){let{ignore_labels:t=["O"]}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},s=Array.isArray(e);s||(e=[e]);let n=this.tokenizer,[o,i]=await super._call(e),r=i.logits,l=this.model.config.id2label,c=[];for(let d=0;d<r.dims[0];++d){let e=o.input_ids[d],s=r[d],i=[];for(let o=0;o<s.dims[0];++o){let r=s[o],c=(0,a.Fp)(r.data)[1],d=l[c];if(t.includes(d))continue;let _=n.decode([e[o].item()],{skip_special_tokens:!0});if(""===_)continue;let u=(0,a.XA)(r.data);i.push({entity:d,score:u[c],index:o,word:_,start:null,end:null})}c.push(i)}return s?c:c[0]}},model:Xs,default:{model:"Xenova/bert-base-multilingual-cased-ner-hrl"},type:"text"},"question-answering":{tokenizer:ye,pipeline:class extends wn{async _call(e,t){let{topk:s=1}=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{},n=this.tokenizer(e,{text_pair:t,padding:!0,truncation:!0}),i=await this.model(n),r=[];for(let l=0;l<i.start_logits.dims[0];++l){let e=n.input_ids[l],t=e.indexOf(this.tokenizer.sep_token_id),c=Array.from((0,a.XA)(i.start_logits[l].data)).map(((e,t)=>[e,t])).filter((e=>e[1]>t)),d=Array.from((0,a.XA)(i.end_logits[l].data)).map(((e,t)=>[e,t])).filter((e=>e[1]>t)),_=(0,o.O7)(c,d).filter((e=>e[0][1]<=e[1][1])).map((e=>[e[0][1],e[1][1],e[0][0]*e[1][0]])).sort(((e,t)=>t[2]-e[2]));for(let n=0;n<Math.min(_.length,s);++n){let[t,s,o]=_[n],i=[...e].slice(t,s+1),a=this.tokenizer.decode(i,{skip_special_tokens:!0});r.push({answer:a,score:o})}}return 1===s?r[0]:r}},model:Ys,default:{model:"Xenova/distilbert-base-cased-distilled-squad"},type:"text"},"fill-mask":{tokenizer:ye,pipeline:class extends wn{async _call(e){let{topk:t=5}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},[s,n]=await super._call(e),o=this.tokenizer,i=[];for(let r=0;r<s.input_ids.dims[0];++r){let e=s.input_ids[r],l=e.indexOf(this.tokenizer.mask_token_id);if(-1===l)throw Error("Mask token (".concat(o.mask_token,") not found in text."));let c=n.logits[r][l],d=(0,a.em)((0,a.XA)(c.data),t);i.push(d.map((t=>{let s=[...e];return s[l]=t[0],{score:t[1],token:t[0],token_str:o.model.vocab[t[0]],sequence:o.decode(s,{skip_special_tokens:!0})}})))}return Array.isArray(e)?i:i[0]}},model:Qs,default:{model:"Xenova/bert-base-uncased"},type:"text"},summarization:{tokenizer:ye,pipeline:class extends xn{constructor(){super(...arguments),(0,n.Z)(this,"_key","summary_text")}},model:Us,default:{model:"Xenova/distilbart-cnn-6-6"},type:"text"},translation:{tokenizer:ye,pipeline:yn,model:Us,default:{model:"Xenova/t5-small"},type:"text"},"text2text-generation":{tokenizer:ye,pipeline:xn,model:Us,default:{model:"Xenova/flan-t5-small"},type:"text"},"text-generation":{tokenizer:ye,pipeline:class extends wn{async _call(e){var t;let s=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},n="string"===typeof e||e instanceof String;n&&(e=[e]);const o=null!==(t=s.add_special_tokens)&&void 0!==t&&t;this.tokenizer.padding_side="left";let i=this.tokenizer(e,{add_special_tokens:o,padding:!0,truncation:!0}),a=i.input_ids,r=i.attention_mask,l=await this.model.generate(a,s,null,{inputs_attention_mask:r});const c=this.tokenizer.batch_decode(l,{skip_special_tokens:!0}),d=Array.from({length:e.length},(e=>[]));for(let _=0;_<c.length;++_){d[Math.floor(_/l.length*e.length)].push({generated_text:c[_]})}return n&&1===d.length?d[0]:d}},model:Ks,default:{model:"Xenova/gpt2"},type:"text"},"zero-shot-classification":{tokenizer:ye,pipeline:class extends wn{constructor(e){var t;super(e),this.label2id=Object.fromEntries(Object.entries(this.model.config.label2id).map((e=>{let[t,s]=e;return[t.toLowerCase(),s]}))),this.entailment_id=this.label2id.entailment,void 0===this.entailment_id&&(console.warn("Could not find 'entailment' in label2id mapping. Using 2 as entailment_id."),this.entailment_id=2),this.contradiction_id=null!==(t=this.label2id.contradiction)&&void 0!==t?t:this.label2id.not_entailment,void 0===this.contradiction_id&&(console.warn("Could not find 'contradiction' in label2id mapping. Using 0 as contradiction_id."),this.contradiction_id=0)}async _call(e,t){let{hypothesis_template:s="This example is {}.",multi_label:n=!1}=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{},o=Array.isArray(e);o||(e=[e]),Array.isArray(t)||(t=[t]);let i=t.map((e=>s.replace("{}",e))),r=n||1===t.length,l=[];for(let c of e){let e,s=[];for(let t of i){let e=this.tokenizer(c,{text_pair:t,padding:!0,truncation:!0}),n=await this.model(e);r?s.push([n.logits.data[this.contradiction_id],n.logits.data[this.entailment_id]]):s.push(n.logits.data[this.entailment_id])}e=r?s.map((e=>(0,a.XA)(e)[1])):(0,a.XA)(s);let n=e.map(((e,t)=>[e,t])).sort(((e,t)=>t[0]-e[0]));l.push({sequence:c,labels:n.map((e=>t[e[1]])),scores:n.map((e=>e[0]))})}return o?l:l[0]}},model:Zs,default:{model:"Xenova/distilbert-base-uncased-mnli"},type:"text"},"audio-classification":{pipeline:class extends wn{constructor(e){super(e)}async _preprocess(e,t){return(0,o.HD)(e)&&(e=await(0,gn.do)(e,t)),e}async _call(e){let{topk:t=5}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},s=!Array.isArray(e);s&&(e=[e]);const n=this.model.config.id2label,o=this.processor.feature_extractor.config.sampling_rate;let i=[];for(let r of e){r=await this._preprocess(r,o);const e=await this.processor(r),s=(await this.model(e)).logits[0];let l=(0,a.em)((0,a.XA)(s.data),t).map((function(e){return{label:n[e[0]],score:e[1]}}));1===t?i.push(...l):i.push(l)}return s&&1!==t?i[0]:i}},model:nn,processor:mn.Z,default:{model:"Xenova/wav2vec2-base-superb-ks"},type:"audio"},"zero-shot-audio-classification":{tokenizer:ye,pipeline:class extends wn{constructor(e){super(e)}async _preprocess(e,t){return(0,o.HD)(e)&&(e=await(0,gn.do)(e,t)),e}async _call(e,t){let{hypothesis_template:s="This is a sound of {}."}=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};const n=!Array.isArray(e);n&&(e=[e]);const o=t.map((e=>s.replace("{}",e))),i=this.tokenizer(o,{padding:!0,truncation:!0}),r=this.processor.feature_extractor.config.sampling_rate,l=[];for(let c of e){c=await this._preprocess(c,r);const e=await this.processor(c),s=await this.model({...i,...e}),n=(0,a.XA)(s.logits_per_audio.data);l.push([...n].map(((e,s)=>({score:e,label:t[s]}))))}return n?l[0]:l}},model:Rs,processor:mn.Z,default:{model:"Xenova/clap-htsat-unfused"},type:"multimodal"},"automatic-speech-recognition":{tokenizer:ye,pipeline:class extends wn{constructor(e){super(e)}async _preprocess(e,t){return(0,o.HD)(e)&&(e=await(0,gn.do)(e,t)),e}async _call(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};switch(this.model.config.model_type){case"whisper":return this._call_whisper(e,t);case"wav2vec2":case"hubert":return this._call_wav2vec2(e,t);default:throw new Error("AutomaticSpeechRecognitionPipeline does not support model type '".concat(this.model.config.model_type,"'."))}}async _call_wav2vec2(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};t.language&&console.warn('`language` parameter is not yet supported for `wav2vec2` models, defaulting to "English".'),t.task&&console.warn('`task` parameter is not yet supported for `wav2vec2` models, defaulting to "transcribe".');let s=!Array.isArray(e);s&&(e=[e]);const n=this.processor.feature_extractor.config.sampling_rate;let o=[];for(let i of e){i=await this._preprocess(i,n);const e=await this.processor(i),t=(await this.model(e)).logits[0],s=[];for(let n of t)s.push((0,a.Fp)(n.data)[1]);const r=this.tokenizer.decode(s);o.push({text:r})}return s?o[0]:o}async _call_whisper(e){var t,s,n,i,r;let l=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},c=null!==(t=l.return_timestamps)&&void 0!==t&&t,d=null!==(s=l.chunk_length_s)&&void 0!==s?s:0,_=null!==(n=l.stride_length_s)&&void 0!==n?n:null,u=null!==(i=l.chunk_callback)&&void 0!==i?i:null,h=null!==(r=l.force_full_sequences)&&void 0!==r&&r;"word"===c&&(l.return_token_timestamps=!0);let p=(0,o.Sw)(l,"language",null),m=(0,o.Sw)(l,"task",null);if(p||m||c){if(l.forced_decoder_ids)throw new Error("Cannot specify `language`/`task`/`return_timestamps` and `forced_decoder_ids` at the same time.");let e=this.tokenizer.get_decoder_prompt_ids({language:p,task:m,no_timestamps:!c});e.length>0&&(l.forced_decoder_ids=e)}let g=!Array.isArray(e);g&&(e=[e]);const f=this.processor.feature_extractor.config.sampling_rate,k=this.processor.feature_extractor.config.chunk_length/this.model.config.max_source_positions,w=this.processor.feature_extractor.config.hop_length;let x=[];for(let o of e){o=await this._preprocess(o,f);let e=[];if(d>0){if(null===_)_=d/6;else if(d<=_)throw Error("`chunk_length_s` must be larger than `stride_length_s`.");const t=f*d,s=f*_,n=t-2*s;let i=0;for(;i<o.length;){let a=o.subarray(i,i+t),r=await this.processor(a),l=0===i,c=i+n>=o.length;e.push({stride:[a.length,l?0:s,c?0:s],input_features:r.input_features,is_last:c}),i+=n}}else e=[{stride:[o.length,0,0],input_features:(await this.processor(o)).input_features,is_last:!0}];for(let n of e){l.num_frames=Math.floor(n.stride[0]/w);let e=await this.model.generate(n.input_features,l);"word"===c?(n.tokens=e.sequences[0],n.token_timestamps=e.token_timestamps.tolist()[0].map((e=>(0,a.NM)(e,2)))):n.tokens=e[0],n.stride=n.stride.map((e=>e/f)),null!==u&&u(n)}let[t,s]=this.tokenizer._decode_asr(e,{time_precision:k,return_timestamps:c,force_full_sequences:h});x.push({text:t,...s})}return g?x[0]:x}},model:[Ws,sn],processor:mn.Z,default:{model:"Xenova/whisper-tiny.en"},type:"multimodal"},"text-to-audio":{tokenizer:ye,pipeline:class extends wn{constructor(e){var t;super(e),(0,n.Z)(this,"DEFAULT_VOCODER_ID","Xenova/speecht5_hifigan"),this.vocoder=null!==(t=e.vocoder)&&void 0!==t?t:null}async _call(e){let{speaker_embeddings:t=null}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if(this.vocoder||(console.log("No vocoder specified, using default HifiGan vocoder."),this.vocoder=await Rs.from_pretrained(this.DEFAULT_VOCODER_ID,{quantized:!1})),("string"===typeof t||t instanceof URL)&&(t=new Float32Array(await(await fetch(t)).arrayBuffer())),t instanceof Float32Array)t=new r.es("float32",t,[1,t.length]);else if(!(t instanceof r.es))throw new Error("Speaker embeddings must be a `Tensor`, `Float32Array`, `string`, or `URL`.");const{input_ids:s}=this.tokenizer(e,{padding:!0,truncation:!0}),{waveform:n}=await this.model.generate_speech(s,t,{vocoder:this.vocoder}),o=this.processor.feature_extractor.config.sampling_rate;return{audio:n.data,sampling_rate:o}}},model:[Vs],processor:mn.Z,default:{model:"Xenova/speecht5_tts"},type:"text"},"image-to-text":{tokenizer:ye,pipeline:class extends wn{constructor(e){super(e)}async _call(e){let t=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},s=Array.isArray(e);e=await kn(e);let{pixel_values:n}=await this.processor(e),o=[];for(let i of n){i.dims=[1,...i.dims];let e=await this.model.generate(i,t),s=this.tokenizer.batch_decode(e,{skip_special_tokens:!0}).map((e=>({generated_text:e.trim()})));o.push(s)}return s?o:o[0]}},model:Hs,processor:mn.Z,default:{model:"Xenova/vit-gpt2-image-captioning"},type:"multimodal"},"image-classification":{pipeline:class extends wn{constructor(e){super(e)}async _call(e){let{topk:t=1}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},s=Array.isArray(e);e=await kn(e);let{pixel_values:n}=await this.processor(e),o=await this.model({pixel_values:n}),i=this.model.config.id2label,r=[];for(let l of o.logits){let e=(0,a.em)((0,a.XA)(l.data),t).map((function(e){return{label:i[e[0]],score:e[1]}}));1===t?r.push(...e):r.push(e)}return s||1===t?r:r[0]}},model:Js,processor:mn.Z,default:{model:"Xenova/vit-base-patch16-224"},type:"multimodal"},"image-segmentation":{pipeline:class extends wn{constructor(e){super(e),this.subtasks_mapping={panoptic:"post_process_panoptic_segmentation",instance:"post_process_instance_segmentation",semantic:"post_process_semantic_segmentation"}}async _call(e){let{threshold:t=.5,mask_threshold:s=.5,overlap_mask_area_threshold:n=.8,label_ids_to_fuse:o=null,target_sizes:i=null,subtask:a=null}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{};if(Array.isArray(e)&&1!==e.length)throw Error("Image segmentation pipeline currently only supports a batch size of 1.");let r=(e=await kn(e)).map((e=>[e.height,e.width])),{pixel_values:l,pixel_mask:c}=await this.processor(e),d=await this.model({pixel_values:l,pixel_mask:c}),_=null;if(null!==a)_=this.subtasks_mapping[a];else for(let[h,p]of Object.entries(this.subtasks_mapping))if(p in this.processor.feature_extractor){_=this.processor.feature_extractor[p].bind(this.processor.feature_extractor),a=h;break}let u=[];if("panoptic"!==a&&"instance"!==a)throw"semantic"===a?Error("semantic segmentation not yet supported."):Error("Subtask ".concat(a," not supported."));{let e=_(d,t,s,n,o,null!==i&&void 0!==i?i:r)[0],a=e.segmentation,l=this.model.config.id2label;for(let t of e.segments_info){let e=new Uint8ClampedArray(a.data.length);for(let n=0;n<a.data.length;++n)a.data[n]===t.id&&(e[n]=255);let s=new fn.O(e,a.dims[1],a.dims[0],1);u.push({score:t.score,label:l[t.label_id],mask:s})}}return u}},model:$s,processor:mn.Z,default:{model:"Xenova/detr-resnet-50-panoptic"},type:"multimodal"},"zero-shot-image-classification":{tokenizer:ye,pipeline:class extends wn{constructor(e){super(e)}async _call(e,t){let{hypothesis_template:s="This is a photo of {}"}=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};const n=Array.isArray(e);e=await kn(e);const o=t.map((e=>s.replace("{}",e))),i=this.tokenizer(o,{padding:!0,truncation:!0}),{pixel_values:r}=await this.processor(e),l=await this.model({...i,pixel_values:r}),c=[];for(const d of l.logits_per_image){const e=[...(0,a.XA)(d.data)].map(((e,s)=>({score:e,label:t[s]})));e.sort(((e,t)=>t.score-e.score)),c.push(e)}return n?c:c[0]}},model:Rs,processor:mn.Z,default:{model:"Xenova/clip-vit-base-patch32"},type:"multimodal"},"object-detection":{pipeline:class extends wn{constructor(e){super(e)}async _call(e){let{threshold:t=.9,percentage:s=!1}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},n=Array.isArray(e);if(n&&1!==e.length)throw Error("Object detection pipeline currently only supports a batch size of 1.");e=await kn(e);let i=s?null:e.map((e=>[e.height,e.width])),{pixel_values:a,pixel_mask:r}=await this.processor(e),l=await this.model({pixel_values:a,pixel_mask:r}),c=this.processor.feature_extractor.post_process_object_detection(l,t,i),d=this.model.config.id2label;const _=c.map((e=>e.boxes.map(((t,n)=>({score:e.scores[n],label:d[e.classes[n]],box:(0,o.bV)(t,!s)})))));return n?_:_[0]}},model:en,processor:mn.Z,default:{model:"Xenova/detr-resnet-50"},type:"multimodal"},"zero-shot-object-detection":{tokenizer:ye,pipeline:class extends wn{constructor(e){super(e)}async _call(e,t){let{threshold:s=.1,topk:n=null,percentage:i=!1}=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};const a=Array.isArray(e);e=await kn(e);const r=this.tokenizer(t,{padding:!0,truncation:!0}),l=await this.processor(e),c=[];for(let d=0;d<e.length;++d){const a=e[d],_=i?null:[[a.height,a.width]],u=l.pixel_values[d].unsqueeze_(0),h=await this.model({...r,pixel_values:u}),p=this.processor.feature_extractor.post_process_object_detection(h,s,_,!0)[0];let m=p.boxes.map(((e,s)=>({score:p.scores[s],label:t[p.classes[s]],box:(0,o.bV)(e,!i)}))).sort(((e,t)=>t.score-e.score));null!==n&&(m=m.slice(0,n)),c.push(m)}return a?c:c[0]}},model:tn,processor:mn.Z,default:{model:"Xenova/owlvit-base-patch32"},type:"multimodal"},"document-question-answering":{tokenizer:ye,pipeline:class extends wn{constructor(e){super(e)}async _call(e,t){let s=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};e=(await kn(e))[0];const{pixel_values:n}=await this.processor(e),o="<s_docvqa><s_question>".concat(t,"</s_question><s_answer>"),i=this.tokenizer(o,{add_special_tokens:!1,padding:!0,truncation:!0}).input_ids,a=await this.model.generate(n,{...s,decoder_input_ids:i,max_length:this.model.config.decoder.max_position_embeddings}),r=this.tokenizer.batch_decode(a)[0].match(/<s_answer>(.*?)<\/s_answer>/);let l=null;return r&&r.length>=2&&(l=r[1].trim()),[{answer:l}]}},model:on,processor:mn.Z,default:{model:"Xenova/donut-base-finetuned-docvqa"},type:"multimodal"},"image-to-image":{pipeline:class extends wn{async _call(e){e=await kn(e);let t=await this.processor(e),s=await this.model(t),n=[];for(let o of s.reconstruction){const e=o.squeeze().clamp_(0,1).mul_(255).round_().to("uint8");n.push(fn.O.fromTensor(e))}return n.length>1?n:n[0]}},model:an,processor:mn.Z,default:{model:"Xenova/swin2SR-classical-sr-x2-64"},type:"image"},"depth-estimation":{pipeline:class extends wn{async _call(e){e=await kn(e);const t=await this.processor(e),{predicted_depth:s}=await this.model(t),n=[];for(let o=0;o<e.length;++o){const t=(0,r.sX)(s[o],e[o].size.reverse(),"bilinear",!1),i=t.mul_(255/(0,a.Fp)(t.data)[0]).to("uint8");n.push({predicted_depth:s[o],depth:fn.O.fromTensor(i)})}return n.length>1?n:n[0]}},model:rn,processor:mn.Z,default:{model:"Xenova/dpt-large"},type:"image"},"feature-extraction":{tokenizer:ye,pipeline:class extends wn{async _call(e){var t;let{pooling:s="none",normalize:n=!1}=arguments.length>1&&void 0!==arguments[1]?arguments[1]:{},[o,i]=await super._call(e),a=null!==(t=i.last_hidden_state)&&void 0!==t?t:i.logits;if("none"===s);else if("mean"===s)a=(0,r.v6)(a,o.attention_mask);else{if("cls"!==s)throw Error("Pooling method '".concat(s,"' not supported."));a=a.slice(null,0)}return n&&(a=a.normalize(2,-1)),a}},model:Rs,default:{model:"Xenova/all-MiniLM-L6-v2"},type:"text"}},bn={"sentiment-analysis":"text-classification",ner:"token-classification",vqa:"visual-question-answering",asr:"automatic-speech-recognition","text-to-speech":"text-to-audio",embeddings:"feature-extraction"};async function Mn(e){var t;let s=arguments.length>1&&void 0!==arguments[1]?arguments[1]:null,{quantized:n=!0,progress_callback:i=null,config:a=null,cache_dir:r=null,local_files_only:l=!1,revision:c="main"}=arguments.length>2&&void 0!==arguments[2]?arguments[2]:{};e=null!==(t=bn[e])&&void 0!==t?t:e;let d=vn[e.split("_",1)[0]];if(!d)throw Error("Unsupported pipeline: ".concat(e,". Must be one of [").concat(Object.keys(vn),"]"));s||(s=d.default.model,console.log('No model specified. Using default model: "'.concat(s,'".')));let _={quantized:n,progress_callback:i,config:a,cache_dir:r,local_files_only:l,revision:c};const u=new Map([["tokenizer",d.tokenizer],["model",d.model],["processor",d.processor]]);let h=await async function(e,t,s){const n=Object.create(null),o=[];for(let[i,a]of e.entries()){if(!a)continue;let e;e=Array.isArray(a)?new Promise((async(e,n)=>{let o;for(let r of a)try{return void e(await r.from_pretrained(t,s))}catch(i){o=i}n(o)})):a.from_pretrained(t,s),n[i]=e,o.push(e)}await Promise.all(o);for(let[i,a]of Object.entries(n))n[i]=await a;return n}(u,s,_);return h.task=e,(0,o.T2)(i,{status:"ready",task:e,model:s}),new(0,d.pipeline)(h)}var An=s(9970)}}]);
//# sourceMappingURL=326.693b1c43.chunk.js.map